From 4e1226410da987df7af428b624c6472192164d77 Mon Sep 17 00:00:00 2001
From: James Taylor <jamestaylor@apache.org>
Date: Wed, 16 Apr 2014 18:28:18 -0700
Subject: [PATCH 01/14] Add local index to grammar and metadata

---
 .../apache/phoenix/end2end/index/LocalIndexIT.java |  58 +++++++++++
 phoenix-core/src/main/antlr3/PhoenixSQL.g          |   7 +-
 .../phoenix/compile/CreateIndexCompiler.java       |   5 +
 .../org/apache/phoenix/compile/JoinCompiler.java   |   4 +-
 .../phoenix/coprocessor/MetaDataEndpointImpl.java  |  11 ++-
 .../coprocessor/generated/PTableProtos.java        | 107 +++++++++++++++++++--
 .../apache/phoenix/exception/SQLExceptionCode.java |   4 +-
 .../apache/phoenix/expression/ExpressionType.java  |   2 +
 .../expression/function/SQLIndexTypeFunction.java  |  77 +++++++++++++++
 .../phoenix/jdbc/PhoenixDatabaseMetaData.java      |   8 +-
 .../org/apache/phoenix/jdbc/PhoenixStatement.java  |  10 +-
 .../apache/phoenix/parse/CreateIndexStatement.java |  10 +-
 .../org/apache/phoenix/parse/ParseNodeFactory.java |   5 +-
 .../phoenix/query/ConnectionQueryServicesImpl.java |   5 +-
 .../org/apache/phoenix/query/QueryConstants.java   |   2 +
 .../org/apache/phoenix/schema/MetaDataClient.java  |  71 +++++++++-----
 .../java/org/apache/phoenix/schema/PTable.java     |  37 +++++++
 .../java/org/apache/phoenix/schema/PTableImpl.java |  36 ++++---
 phoenix-protocol/src/main/PTable.proto             |   1 +
 19 files changed, 397 insertions(+), 63 deletions(-)
 create mode 100644 phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/expression/function/SQLIndexTypeFunction.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
new file mode 100644
index 0000000..4589259
--- /dev/null
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -0,0 +1,58 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.end2end.index;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.SQLException;
+
+import org.apache.phoenix.jdbc.PhoenixConnection;
+import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
+import org.apache.phoenix.schema.PTableKey;
+import org.junit.Test;
+
+public class LocalIndexIT extends BaseIndexIT {
+    private void createBaseTable(String tableName, Integer saltBuckets) throws SQLException {
+        Connection conn = DriverManager.getConnection(getUrl());
+        String ddl = "CREATE TABLE " + tableName + " (t_id VARCHAR NOT NULL,\n" +
+                "k1 INTEGER NOT NULL,\n" +
+                "k2 INTEGER NOT NULL,\n" +
+                "v1 VARCHAR,\n" +
+                "CONSTRAINT pk PRIMARY KEY (t_id, k1, k2))\n"
+                + (saltBuckets == null ? "" : (",salt_buckets="+saltBuckets));
+        conn.createStatement().execute(ddl);
+        conn.close();
+    }
+    
+    @Test
+    public void testLocalIndexRoundTrip() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null);
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+        conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+        PTable localIndex = conn2.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
+        assertEquals(IndexType.LOCAL, localIndex.getIndexType());
+        assertNotNull(localIndex.getViewIndexId());
+    }
+
+}
diff --git a/phoenix-core/src/main/antlr3/PhoenixSQL.g b/phoenix-core/src/main/antlr3/PhoenixSQL.g
index dbcca4f..5f52da4 100644
--- a/phoenix-core/src/main/antlr3/PhoenixSQL.g
+++ b/phoenix-core/src/main/antlr3/PhoenixSQL.g
@@ -98,7 +98,7 @@ tokens
     VALUE='value';
     FOR='for';
     CACHE='cache';
-    DERIVE='derive';
+    LOCAL='local';
     ANY='any';
     SOME='some';
 }
@@ -144,6 +144,7 @@ import org.apache.phoenix.schema.IllegalDataException;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PIndexState;
 import org.apache.phoenix.schema.PTableType;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.util.SchemaUtil;
 }
 
@@ -381,12 +382,12 @@ create_view_node returns [CreateTableStatement ret]
 
 // Parse a create index statement.
 create_index_node returns [CreateIndexStatement ret]
-    :   CREATE INDEX (IF NOT ex=EXISTS)? i=index_name ON t=from_table_name
+    :   CREATE l=LOCAL? INDEX (IF NOT ex=EXISTS)? i=index_name ON t=from_table_name
         (LPAREN pk=index_pk_constraint RPAREN)
         (INCLUDE (LPAREN icrefs=column_names RPAREN))?
         (p=fam_properties)?
         (SPLIT ON v=value_expression_list)?
-        {ret = factory.createIndex(i, factory.namedTable(null,t), pk, icrefs, v, p, ex!=null, getBindCount()); }
+        {ret = factory.createIndex(i, factory.namedTable(null,t), pk, icrefs, v, p, ex!=null, l==null ? IndexType.getDefault() : IndexType.LOCAL, getBindCount()); }
     ;
 
 // Parse a create sequence statement.
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
index bbd7154..b13ff6d 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
@@ -32,6 +32,7 @@ import org.apache.phoenix.jdbc.PhoenixStatement;
 import org.apache.phoenix.parse.CreateIndexStatement;
 import org.apache.phoenix.parse.ParseNode;
 import org.apache.phoenix.schema.MetaDataClient;
+import org.apache.phoenix.schema.PTable.IndexType;
 
 public class CreateIndexCompiler {
     private final PhoenixStatement statement;
@@ -47,6 +48,10 @@ public class CreateIndexCompiler {
         final StatementContext context = new StatementContext(statement, resolver, scan, new SequenceManager(statement));
         ExpressionCompiler expressionCompiler = new ExpressionCompiler(context);
         List<ParseNode> splitNodes = create.getSplitNodes();
+        if (!splitNodes.isEmpty() && create.getIndexType() == IndexType.LOCAL) {
+            throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SPLIT_LOCAL_INDEX)
+            .build().buildException();
+        }
         final byte[][] splits = new byte[splitNodes.size()][];
         for (int i = 0; i < splits.length; i++) {
             ParseNode node = splitNodes.get(i);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
index c433fc5..bba951e 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
@@ -721,7 +721,7 @@ public class JoinCompiler {
             PTable t = PTableImpl.makePTable(table.getTenantId(), PNameFactory.newName(PROJECTED_TABLE_SCHEMA), table.getName(), PTableType.JOIN,
                         table.getIndexState(), table.getTimeStamp(), table.getSequenceNumber(), table.getPKName(),
                         retainPKColumns ? table.getBucketNum() : null, projectedColumns, table.getParentTableName(),
-                        table.getIndexes(), table.isImmutableRows(), Collections.<PName>emptyList(), null, null, table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                        table.getIndexes(), table.isImmutableRows(), Collections.<PName>emptyList(), null, null, table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
             return new ProjectedPTableWrapper(t, columnNameMap, sourceExpressions);
         }
         
@@ -1272,7 +1272,7 @@ public class JoinCompiler {
             }
             PTable t = PTableImpl.makePTable(left.getTenantId(), left.getSchemaName(),
                     PNameFactory.newName(SchemaUtil.getTableName(left.getName().getString(), right.getName().getString())), left.getType(), left.getIndexState(), left.getTimeStamp(), left.getSequenceNumber(), left.getPKName(), left.getBucketNum(), merged,
-                    left.getParentTableName(), left.getIndexes(), left.isImmutableRows(), Collections.<PName>emptyList(), null, null, PTable.DEFAULT_DISABLE_WAL, left.isMultiTenant(), left.getViewType(), left.getViewIndexId());
+                    left.getParentTableName(), left.getIndexes(), left.isImmutableRows(), Collections.<PName>emptyList(), null, null, PTable.DEFAULT_DISABLE_WAL, left.isMultiTenant(), left.getViewType(), left.getViewIndexId(), left.getIndexType());
 
             ListMultimap<String, String> mergedMap = ArrayListMultimap.<String, String>create();
             mergedMap.putAll(this.getColumnNameMap());
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java
index dcda21e..3f4892b 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataEndpointImpl.java
@@ -31,6 +31,7 @@ import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.DISABLE_WAL_BYTES;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.FAMILY_NAME_INDEX;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IMMUTABLE_ROWS_BYTES;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INDEX_STATE_BYTES;
+import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INDEX_TYPE_BYTES;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IS_VIEW_REFERENCED_BYTES;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.LINK_TYPE_BYTES;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.MULTI_TENANT_BYTES;
@@ -115,6 +116,7 @@ import org.apache.phoenix.schema.PIndexState;
 import org.apache.phoenix.schema.PName;
 import org.apache.phoenix.schema.PNameFactory;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTable.LinkType;
 import org.apache.phoenix.schema.PTable.ViewType;
 import org.apache.phoenix.schema.PTableImpl;
@@ -169,6 +171,7 @@ public class MetaDataEndpointImpl extends MetaDataProtocol implements Coprocesso
     private static final KeyValue MULTI_TENANT_KV = KeyValue.createFirstOnRow(ByteUtil.EMPTY_BYTE_ARRAY, TABLE_FAMILY_BYTES, MULTI_TENANT_BYTES);
     private static final KeyValue VIEW_TYPE_KV = KeyValue.createFirstOnRow(ByteUtil.EMPTY_BYTE_ARRAY, TABLE_FAMILY_BYTES, VIEW_TYPE_BYTES);
     private static final KeyValue VIEW_INDEX_ID_KV = KeyValue.createFirstOnRow(ByteUtil.EMPTY_BYTE_ARRAY, TABLE_FAMILY_BYTES, VIEW_INDEX_ID_BYTES);
+    private static final KeyValue INDEX_TYPE_KV = KeyValue.createFirstOnRow(ByteUtil.EMPTY_BYTE_ARRAY, TABLE_FAMILY_BYTES, INDEX_TYPE_BYTES);
     private static final List<KeyValue> TABLE_KV_COLUMNS = Arrays.<KeyValue>asList(
             TABLE_TYPE_KV,
             TABLE_SEQ_NUM_KV,
@@ -183,7 +186,8 @@ public class MetaDataEndpointImpl extends MetaDataProtocol implements Coprocesso
             DISABLE_WAL_KV,
             MULTI_TENANT_KV,
             VIEW_TYPE_KV,
-            VIEW_INDEX_ID_KV
+            VIEW_INDEX_ID_KV,
+            INDEX_TYPE_KV
             );
     static {
         Collections.sort(TABLE_KV_COLUMNS, KeyValue.COMPARATOR);
@@ -202,6 +206,7 @@ public class MetaDataEndpointImpl extends MetaDataProtocol implements Coprocesso
     private static final int MULTI_TENANT_INDEX = TABLE_KV_COLUMNS.indexOf(MULTI_TENANT_KV);
     private static final int VIEW_TYPE_INDEX = TABLE_KV_COLUMNS.indexOf(VIEW_TYPE_KV);
     private static final int VIEW_INDEX_ID_INDEX = TABLE_KV_COLUMNS.indexOf(VIEW_INDEX_ID_KV);
+    private static final int INDEX_TYPE_INDEX = TABLE_KV_COLUMNS.indexOf(INDEX_TYPE_KV);
     
     // KeyValues for Column
     private static final KeyValue DECIMAL_DIGITS_KV = KeyValue.createFirstOnRow(ByteUtil.EMPTY_BYTE_ARRAY, TABLE_FAMILY_BYTES, DECIMAL_DIGITS_BYTES);
@@ -625,6 +630,8 @@ public class MetaDataEndpointImpl extends MetaDataProtocol implements Coprocesso
         ViewType viewType = viewTypeKv == null ? null : ViewType.fromSerializedValue(viewTypeKv.getValueArray()[viewTypeKv.getValueOffset()]);
         Cell viewIndexIdKv = tableKeyValues[VIEW_INDEX_ID_INDEX];
         Short viewIndexId = viewIndexIdKv == null ? null : (Short)MetaDataUtil.getViewIndexIdDataType().getCodec().decodeShort(viewIndexIdKv.getValueArray(), viewIndexIdKv.getValueOffset(), SortOrder.getDefault());
+        Cell indexTypeKv = tableKeyValues[INDEX_TYPE_INDEX];
+        IndexType indexType = indexTypeKv == null ? null : IndexType.fromSerializedValue(indexTypeKv.getValueArray()[indexTypeKv.getValueOffset()]);
         
         List<PColumn> columns = Lists.newArrayListWithExpectedSize(columnCount);
         List<PTable> indexes = new ArrayList<PTable>();
@@ -657,7 +664,7 @@ public class MetaDataEndpointImpl extends MetaDataProtocol implements Coprocesso
         return PTableImpl.makePTable(tenantId, schemaName, tableName, tableType, indexState, timeStamp, 
             tableSeqNum, pkName, saltBucketNum, columns, tableType == INDEX ? dataTableName : null, 
             indexes, isImmutableRows, physicalTables, defaultFamilyName, viewStatement, disableWAL, 
-            multiTenant, viewType, viewIndexId);
+            multiTenant, viewType, viewIndexId, indexType);
     }
 
     private PTable buildDeletedTable(byte[] key, ImmutableBytesPtr cacheKey, HRegion region,
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/generated/PTableProtos.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/generated/PTableProtos.java
index cb6711a..9cb47b9 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/generated/PTableProtos.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/generated/PTableProtos.java
@@ -2535,6 +2535,16 @@ public final class PTableProtos {
      * <code>optional int32 viewIndexId = 21;</code>
      */
     int getViewIndexId();
+
+    // optional bytes indexType = 22;
+    /**
+     * <code>optional bytes indexType = 22;</code>
+     */
+    boolean hasIndexType();
+    /**
+     * <code>optional bytes indexType = 22;</code>
+     */
+    com.google.protobuf.ByteString getIndexType();
   }
   /**
    * Protobuf type {@code PTable}
@@ -2710,6 +2720,11 @@ public final class PTableProtos {
               viewIndexId_ = input.readInt32();
               break;
             }
+            case 178: {
+              bitField0_ |= 0x00020000;
+              indexType_ = input.readBytes();
+              break;
+            }
           }
         }
       } catch (com.google.protobuf.InvalidProtocolBufferException e) {
@@ -3192,6 +3207,22 @@ public final class PTableProtos {
       return viewIndexId_;
     }
 
+    // optional bytes indexType = 22;
+    public static final int INDEXTYPE_FIELD_NUMBER = 22;
+    private com.google.protobuf.ByteString indexType_;
+    /**
+     * <code>optional bytes indexType = 22;</code>
+     */
+    public boolean hasIndexType() {
+      return ((bitField0_ & 0x00020000) == 0x00020000);
+    }
+    /**
+     * <code>optional bytes indexType = 22;</code>
+     */
+    public com.google.protobuf.ByteString getIndexType() {
+      return indexType_;
+    }
+
     private void initFields() {
       schemaNameBytes_ = com.google.protobuf.ByteString.EMPTY;
       tableNameBytes_ = com.google.protobuf.ByteString.EMPTY;
@@ -3214,6 +3245,7 @@ public final class PTableProtos {
       physicalNames_ = java.util.Collections.emptyList();
       tenantId_ = com.google.protobuf.ByteString.EMPTY;
       viewIndexId_ = 0;
+      indexType_ = com.google.protobuf.ByteString.EMPTY;
     }
     private byte memoizedIsInitialized = -1;
     public final boolean isInitialized() {
@@ -3344,6 +3376,9 @@ public final class PTableProtos {
       if (((bitField0_ & 0x00010000) == 0x00010000)) {
         output.writeInt32(21, viewIndexId_);
       }
+      if (((bitField0_ & 0x00020000) == 0x00020000)) {
+        output.writeBytes(22, indexType_);
+      }
       getUnknownFields().writeTo(output);
     }
 
@@ -3442,6 +3477,10 @@ public final class PTableProtos {
         size += com.google.protobuf.CodedOutputStream
           .computeInt32Size(21, viewIndexId_);
       }
+      if (((bitField0_ & 0x00020000) == 0x00020000)) {
+        size += com.google.protobuf.CodedOutputStream
+          .computeBytesSize(22, indexType_);
+      }
       size += getUnknownFields().getSerializedSize();
       memoizedSerializedSize = size;
       return size;
@@ -3558,6 +3597,11 @@ public final class PTableProtos {
         result = result && (getViewIndexId()
             == other.getViewIndexId());
       }
+      result = result && (hasIndexType() == other.hasIndexType());
+      if (hasIndexType()) {
+        result = result && getIndexType()
+            .equals(other.getIndexType());
+      }
       result = result &&
           getUnknownFields().equals(other.getUnknownFields());
       return result;
@@ -3655,6 +3699,10 @@ public final class PTableProtos {
         hash = (37 * hash) + VIEWINDEXID_FIELD_NUMBER;
         hash = (53 * hash) + getViewIndexId();
       }
+      if (hasIndexType()) {
+        hash = (37 * hash) + INDEXTYPE_FIELD_NUMBER;
+        hash = (53 * hash) + getIndexType().hashCode();
+      }
       hash = (29 * hash) + getUnknownFields().hashCode();
       memoizedHashCode = hash;
       return hash;
@@ -3821,6 +3869,8 @@ public final class PTableProtos {
         bitField0_ = (bitField0_ & ~0x00080000);
         viewIndexId_ = 0;
         bitField0_ = (bitField0_ & ~0x00100000);
+        indexType_ = com.google.protobuf.ByteString.EMPTY;
+        bitField0_ = (bitField0_ & ~0x00200000);
         return this;
       }
 
@@ -3949,6 +3999,10 @@ public final class PTableProtos {
           to_bitField0_ |= 0x00010000;
         }
         result.viewIndexId_ = viewIndexId_;
+        if (((from_bitField0_ & 0x00200000) == 0x00200000)) {
+          to_bitField0_ |= 0x00020000;
+        }
+        result.indexType_ = indexType_;
         result.bitField0_ = to_bitField0_;
         onBuilt();
         return result;
@@ -4106,6 +4160,9 @@ public final class PTableProtos {
         if (other.hasViewIndexId()) {
           setViewIndexId(other.getViewIndexId());
         }
+        if (other.hasIndexType()) {
+          setIndexType(other.getIndexType());
+        }
         this.mergeUnknownFields(other.getUnknownFields());
         return this;
       }
@@ -5608,6 +5665,42 @@ public final class PTableProtos {
         return this;
       }
 
+      // optional bytes indexType = 22;
+      private com.google.protobuf.ByteString indexType_ = com.google.protobuf.ByteString.EMPTY;
+      /**
+       * <code>optional bytes indexType = 22;</code>
+       */
+      public boolean hasIndexType() {
+        return ((bitField0_ & 0x00200000) == 0x00200000);
+      }
+      /**
+       * <code>optional bytes indexType = 22;</code>
+       */
+      public com.google.protobuf.ByteString getIndexType() {
+        return indexType_;
+      }
+      /**
+       * <code>optional bytes indexType = 22;</code>
+       */
+      public Builder setIndexType(com.google.protobuf.ByteString value) {
+        if (value == null) {
+    throw new NullPointerException();
+  }
+  bitField0_ |= 0x00200000;
+        indexType_ = value;
+        onChanged();
+        return this;
+      }
+      /**
+       * <code>optional bytes indexType = 22;</code>
+       */
+      public Builder clearIndexType() {
+        bitField0_ = (bitField0_ & ~0x00200000);
+        indexType_ = getDefaultInstance().getIndexType();
+        onChanged();
+        return this;
+      }
+
       // @@protoc_insertion_point(builder_scope:PTable)
     }
 
@@ -5650,7 +5743,7 @@ public final class PTableProtos {
       " \002(\005\022\021\n\tsortOrder\030\010 \002(\005\022\021\n\tarraySize\030\t \001" +
       "(\005\022\024\n\014viewConstant\030\n \001(\014\022\026\n\016viewReferenc" +
       "ed\030\013 \001(\010\"*\n\013PTableStats\022\013\n\003key\030\001 \002(\t\022\016\n\006" +
-      "values\030\002 \003(\014\"\367\003\n\006PTable\022\027\n\017schemaNameByt" +
+      "values\030\002 \003(\014\"\212\004\n\006PTable\022\027\n\017schemaNameByt" +
       "es\030\001 \002(\014\022\026\n\016tableNameBytes\030\002 \002(\014\022\036\n\ttabl" +
       "eType\030\003 \002(\0162\013.PTableType\022\022\n\nindexState\030\004",
       " \001(\t\022\026\n\016sequenceNumber\030\005 \002(\003\022\021\n\ttimeStam" +
@@ -5662,11 +5755,11 @@ public final class PTableProtos {
       "milyName\030\016 \001(\014\022\022\n\ndisableWAL\030\017 \002(\010\022\023\n\013mu" +
       "ltiTenant\030\020 \002(\010\022\020\n\010viewType\030\021 \001(\014\022\025\n\rvie" +
       "wStatement\030\022 \001(\014\022\025\n\rphysicalNames\030\023 \003(\014\022" +
-      "\020\n\010tenantId\030\024 \001(\014\022\023\n\013viewIndexId\030\025 \001(\005*A",
-      "\n\nPTableType\022\n\n\006SYSTEM\020\000\022\010\n\004USER\020\001\022\010\n\004VI" +
-      "EW\020\002\022\t\n\005INDEX\020\003\022\010\n\004JOIN\020\004B@\n(org.apache." +
-      "phoenix.coprocessor.generatedB\014PTablePro" +
-      "tosH\001\210\001\001\240\001\001"
+      "\020\n\010tenantId\030\024 \001(\014\022\023\n\013viewIndexId\030\025 \001(\005\022\021",
+      "\n\tindexType\030\026 \001(\014*A\n\nPTableType\022\n\n\006SYSTE" +
+      "M\020\000\022\010\n\004USER\020\001\022\010\n\004VIEW\020\002\022\t\n\005INDEX\020\003\022\010\n\004JO" +
+      "IN\020\004B@\n(org.apache.phoenix.coprocessor.g" +
+      "eneratedB\014PTableProtosH\001\210\001\001\240\001\001"
     };
     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
@@ -5690,7 +5783,7 @@ public final class PTableProtos {
           internal_static_PTable_fieldAccessorTable = new
             com.google.protobuf.GeneratedMessage.FieldAccessorTable(
               internal_static_PTable_descriptor,
-              new java.lang.String[] { "SchemaNameBytes", "TableNameBytes", "TableType", "IndexState", "SequenceNumber", "TimeStamp", "PkNameBytes", "BucketNum", "Columns", "Indexes", "IsImmutableRows", "GuidePosts", "DataTableNameBytes", "DefaultFamilyName", "DisableWAL", "MultiTenant", "ViewType", "ViewStatement", "PhysicalNames", "TenantId", "ViewIndexId", });
+              new java.lang.String[] { "SchemaNameBytes", "TableNameBytes", "TableType", "IndexState", "SequenceNumber", "TimeStamp", "PkNameBytes", "BucketNum", "Columns", "Indexes", "IsImmutableRows", "GuidePosts", "DataTableNameBytes", "DefaultFamilyName", "DisableWAL", "MultiTenant", "ViewType", "ViewStatement", "PhysicalNames", "TenantId", "ViewIndexId", "IndexType", });
           return null;
         }
       };
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java b/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
index 39b951d..817d259 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
@@ -211,7 +211,7 @@ public enum SQLExceptionCode {
     INSUFFICIENT_MULTI_TENANT_COLUMNS(1040, "42Y96", "A MULTI_TENANT table must have two or more PK columns with the first column being NOT NULL and of type VARCHAR or CHAR."),
     VIEW_WHERE_IS_CONSTANT(1045, "43A02", "WHERE clause in VIEW should not evaluate to a constant."),
     CANNOT_UPDATE_VIEW_COLUMN(1046, "43A03", "Column updated in VIEW may not differ from value specified in WHERE clause."),
-    TOO_MANY_VIEW_INDEXES(1047, "43A04", "Too many indexes have already been created on the physical table."),
+    TOO_MANY_INDEXES(1047, "43A04", "Too many indexes have already been created on the physical table."),
         
     /** Sequence related */
     SEQUENCE_ALREADY_EXIST(1200, "42Z00", "Sequence already exists.", new Factory() {
@@ -250,6 +250,8 @@ public enum SQLExceptionCode {
     SPLIT_POINT_NOT_CONSTANT(1105, "XCL04", "Split points must be constants."),
     BATCH_EXCEPTION(1106, "XCL05", "Exception while executing batch."),
     EXECUTE_UPDATE_WITH_NON_EMPTY_BATCH(1107, "XCL06", "An executeUpdate is prohibited when the batch is not empty. Use clearBatch to empty the batch first."),
+    CANNOT_SPLIT_LOCAL_INDEX(1108,"XCL07", "Local index may not be pre-split"),
+    CANNOT_SALT_LOCAL_INDEX(1109,"XCL08", "Local index may not be salted"),
     
     /**
      * Implementation defined class. Phoenix internal error. (errorcode 20, sqlstate INT).
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
index 7e4b9b6..102eb86 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
@@ -61,6 +61,7 @@ import org.apache.phoenix.expression.function.RoundDateExpression;
 import org.apache.phoenix.expression.function.RoundDecimalExpression;
 import org.apache.phoenix.expression.function.RoundFunction;
 import org.apache.phoenix.expression.function.RoundTimestampExpression;
+import org.apache.phoenix.expression.function.SQLIndexTypeFunction;
 import org.apache.phoenix.expression.function.SQLTableTypeFunction;
 import org.apache.phoenix.expression.function.SQLViewTypeFunction;
 import org.apache.phoenix.expression.function.SqlTypeNameFunction;
@@ -168,6 +169,7 @@ public enum ExpressionType {
     SQLViewTypeFunction(SQLViewTypeFunction.class),
     ExternalSqlTypeIdFunction(ExternalSqlTypeIdFunction.class),
     ConvertTimezoneFunction(ConvertTimezoneFunction.class),
+    SQLIndexTypeFunction(SQLIndexTypeFunction.class);
     DecodeFunction(DecodeFunction.class),
     TimezoneOffsetFunction(TimezoneOffsetFunction.class),
     EncodeFunction(EncodeFunction.class),
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/function/SQLIndexTypeFunction.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/function/SQLIndexTypeFunction.java
new file mode 100644
index 0000000..617e977
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/function/SQLIndexTypeFunction.java
@@ -0,0 +1,77 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.expression.function;
+
+import java.sql.SQLException;
+import java.util.List;
+
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.phoenix.expression.Expression;
+import org.apache.phoenix.parse.FunctionParseNode.Argument;
+import org.apache.phoenix.parse.FunctionParseNode.BuiltInFunction;
+import org.apache.phoenix.schema.PDataType;
+import org.apache.phoenix.schema.PTable.IndexType;
+import org.apache.phoenix.schema.tuple.Tuple;
+
+
+/**
+ * 
+ * Function used to get the SQL view type name from the serialized view type.
+ * Usage:
+ * SQLViewType('v') will return 'VIEW' based on
+ * {@link java.sql.DatabaseMetaData#getTableTypes()}
+ * 
+ * 
+ * @since 2.2
+ */
+@BuiltInFunction(name=SQLIndexTypeFunction.NAME, args= {
+    @Argument(allowedTypes=PDataType.UNSIGNED_TINYINT)} )
+public class SQLIndexTypeFunction extends ScalarFunction {
+    public static final String NAME = "SQLIndexType";
+
+    public SQLIndexTypeFunction() {
+    }
+    
+    public SQLIndexTypeFunction(List<Expression> children) throws SQLException {
+        super(children);
+    }
+    
+    @Override
+    public boolean evaluate(Tuple tuple, ImmutableBytesWritable ptr) {
+        Expression child = children.get(0);
+        if (!child.evaluate(tuple, ptr)) {
+            return false;
+        }
+        if (ptr.getLength() == 0) {
+            return true;
+        }
+        IndexType viewType = IndexType.fromSerializedValue(ptr.get()[ptr.getOffset()]);
+        ptr.set(viewType.getBytes());
+        return true;
+    }
+
+    @Override
+    public PDataType getDataType() {
+        return PDataType.VARCHAR;
+    }
+    
+    @Override
+    public String getName() {
+        return NAME;
+    }
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixDatabaseMetaData.java b/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixDatabaseMetaData.java
index 5f7e017..67b6913 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixDatabaseMetaData.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixDatabaseMetaData.java
@@ -38,6 +38,7 @@ import org.apache.phoenix.exception.SQLExceptionInfo;
 import org.apache.phoenix.expression.RowKeyColumnExpression;
 import org.apache.phoenix.expression.function.ExternalSqlTypeIdFunction;
 import org.apache.phoenix.expression.function.IndexStateNameFunction;
+import org.apache.phoenix.expression.function.SQLIndexTypeFunction;
 import org.apache.phoenix.expression.function.SQLTableTypeFunction;
 import org.apache.phoenix.expression.function.SQLViewTypeFunction;
 import org.apache.phoenix.expression.function.SqlTypeNameFunction;
@@ -174,6 +175,8 @@ public class PhoenixDatabaseMetaData implements DatabaseMetaData, org.apache.pho
     public static final byte[] MULTI_TENANT_BYTES = Bytes.toBytes(MULTI_TENANT);
     public static final String VIEW_TYPE = "VIEW_TYPE";
     public static final byte[] VIEW_TYPE_BYTES = Bytes.toBytes(VIEW_TYPE);
+    public static final String INDEX_TYPE = "INDEX_TYPE";
+    public static final byte[] INDEX_TYPE_BYTES = Bytes.toBytes(INDEX_TYPE);
     public static final String LINK_TYPE = "LINK_TYPE";
     public static final byte[] LINK_TYPE_BYTES = Bytes.toBytes(LINK_TYPE);
     public static final String ARRAY_SIZE = "ARRAY_SIZE";
@@ -205,7 +208,7 @@ public class PhoenixDatabaseMetaData implements DatabaseMetaData, org.apache.pho
     public static final String KEY_SEQ = "KEY_SEQ";
     public static final byte[] KEY_SEQ_BYTES = Bytes.toBytes(KEY_SEQ);
     public static final String SUPERTABLE_NAME = "SUPERTABLE_NAME";
-    		
+    
     public static final String TYPE_ID = "TYPE_ID";
     
     private final PhoenixConnection connection;
@@ -855,7 +858,8 @@ public class PhoenixDatabaseMetaData implements DatabaseMetaData, org.apache.pho
                 SALT_BUCKETS + "," +
                 MULTI_TENANT + "," +
                 VIEW_STATEMENT + "," +
-                SQLViewTypeFunction.NAME + "(" + VIEW_TYPE + ") AS " + VIEW_TYPE +
+                SQLViewTypeFunction.NAME + "(" + VIEW_TYPE + ") AS " + VIEW_TYPE + "," +
+                SQLIndexTypeFunction.NAME + "(" + INDEX_TYPE + ") AS " + INDEX_TYPE +
                 " from " + SYSTEM_CATALOG + " " + SYSTEM_CATALOG_ALIAS +
                 " where " + COLUMN_NAME + " is null" +
                 " and " + COLUMN_FAMILY + " is null");
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixStatement.java b/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixStatement.java
index d4c677b..eed723a 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixStatement.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/jdbc/PhoenixStatement.java
@@ -102,6 +102,7 @@ import org.apache.phoenix.schema.MetaDataClient;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PDatum;
 import org.apache.phoenix.schema.PIndexState;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.RowKeyValueAccessor;
 import org.apache.phoenix.schema.Sequence;
@@ -473,8 +474,8 @@ public class PhoenixStatement implements Statement, SQLCloseable, org.apache.pho
     private static class ExecutableCreateIndexStatement extends CreateIndexStatement implements CompilableStatement {
 
         public ExecutableCreateIndexStatement(NamedNode indexName, NamedTableNode dataTable, PrimaryKeyConstraint pkConstraint, List<ColumnName> includeColumns, List<ParseNode> splits,
-                ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, int bindCount) {
-            super(indexName, dataTable, pkConstraint, includeColumns, splits, props, ifNotExists, bindCount);
+                ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, IndexType indexType, int bindCount) {
+            super(indexName, dataTable, pkConstraint, includeColumns, splits, props, ifNotExists, indexType, bindCount);
         }
 
         @SuppressWarnings("unchecked")
@@ -754,8 +755,9 @@ public class PhoenixStatement implements Statement, SQLCloseable, org.apache.pho
         }
         
         @Override
-        public CreateIndexStatement createIndex(NamedNode indexName, NamedTableNode dataTable, PrimaryKeyConstraint pkConstraint, List<ColumnName> includeColumns, List<ParseNode> splits, ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, int bindCount) {
-            return new ExecutableCreateIndexStatement(indexName, dataTable, pkConstraint, includeColumns, splits, props, ifNotExists, bindCount);
+        public CreateIndexStatement createIndex(NamedNode indexName, NamedTableNode dataTable, PrimaryKeyConstraint pkConstraint, List<ColumnName> includeColumns, List<ParseNode> splits,
+                ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, IndexType indexType, int bindCount) {
+            return new ExecutableCreateIndexStatement(indexName, dataTable, pkConstraint, includeColumns, splits, props, ifNotExists, indexType, bindCount);
         }
         
         @Override
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/parse/CreateIndexStatement.java b/phoenix-core/src/main/java/org/apache/phoenix/parse/CreateIndexStatement.java
index cc2d971..f688ceb 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/parse/CreateIndexStatement.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/parse/CreateIndexStatement.java
@@ -21,6 +21,7 @@ import java.util.Collections;
 import java.util.List;
 
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.phoenix.schema.PTable.IndexType;
 
 import com.google.common.collect.ListMultimap;
 
@@ -32,10 +33,11 @@ public class CreateIndexStatement extends SingleTableStatement {
     private final List<ParseNode> splitNodes;
     private final ListMultimap<String,Pair<String,Object>> props;
     private final boolean ifNotExists;
+    private final IndexType indexType;
 
     public CreateIndexStatement(NamedNode indexTableName, NamedTableNode dataTable, 
             PrimaryKeyConstraint indexConstraint, List<ColumnName> includeColumns, List<ParseNode> splits,
-            ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, int bindCount) {
+            ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, IndexType indexType, int bindCount) {
         super(dataTable, bindCount);
         this.indexTableName =TableName.create(dataTable.getName().getSchemaName(),indexTableName.getName());
         this.indexConstraint = indexConstraint == null ? PrimaryKeyConstraint.EMPTY : indexConstraint;
@@ -43,6 +45,7 @@ public class CreateIndexStatement extends SingleTableStatement {
         this.splitNodes = splits == null ? Collections.<ParseNode>emptyList() : splits;
         this.props = props;
         this.ifNotExists = ifNotExists;
+        this.indexType = indexType;
     }
 
     public PrimaryKeyConstraint getIndexConstraint() {
@@ -69,4 +72,9 @@ public class CreateIndexStatement extends SingleTableStatement {
         return ifNotExists;
     }
 
+
+    public IndexType getIndexType() {
+        return indexType;
+    }
+
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeFactory.java b/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeFactory.java
index 4b27696..a17c761 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeFactory.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeFactory.java
@@ -41,6 +41,7 @@ import org.apache.phoenix.parse.FunctionParseNode.BuiltInFunctionInfo;
 import org.apache.phoenix.parse.JoinTableNode.JoinType;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PIndexState;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.SortOrder;
 import org.apache.phoenix.schema.TypeMismatchException;
@@ -268,8 +269,8 @@ public class ParseNodeFactory {
         return new CreateTableStatement(tableName, props, columns, pkConstraint, splits, tableType, ifNotExists, baseTableName, tableTypeIdNode, bindCount);
     }
     
-    public CreateIndexStatement createIndex(NamedNode indexName, NamedTableNode dataTable, PrimaryKeyConstraint pkConstraint, List<ColumnName> includeColumns, List<ParseNode> splits, ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, int bindCount) {
-        return new CreateIndexStatement(indexName, dataTable, pkConstraint, includeColumns, splits, props, ifNotExists, bindCount);
+    public CreateIndexStatement createIndex(NamedNode indexName, NamedTableNode dataTable, PrimaryKeyConstraint pkConstraint, List<ColumnName> includeColumns, List<ParseNode> splits, ListMultimap<String,Pair<String,Object>> props, boolean ifNotExists, IndexType indexType, int bindCount) {
+        return new CreateIndexStatement(indexName, dataTable, pkConstraint, includeColumns, splits, props, ifNotExists, indexType, bindCount);
     }
     
     public CreateSequenceStatement createSequence(TableName tableName, ParseNode startsWith, ParseNode incrementBy, ParseNode cacheSize, boolean ifNotExits, int bindCount){
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
index aa6fe17..ea455d3 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
@@ -1066,13 +1066,14 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
         byte[] tableName = physicalTableName != null ? physicalTableName : SchemaUtil.getTableNameAsBytes(schemaBytes, tableBytes);
         if ((tableType == PTableType.VIEW && physicalTableName != null) || (tableType != PTableType.VIEW && physicalTableName == null)) {
             // For views this will ensure that metadata already exists
+            // For tables and indexes, this will create the metadata if it doesn't already exist
             ensureTableCreated(tableName, tableType, tableProps, families, splits, true);
         }
         ImmutableBytesWritable ptr = new ImmutableBytesWritable();
-        if (tableType == PTableType.INDEX && physicalTableName != null) { // Index on view
+        if (tableType == PTableType.INDEX) { // Index on view
             // Physical index table created up front for multi tenant
             // TODO: if viewIndexId is Short.MIN_VALUE, then we don't need to attempt to create it
-            if (!MetaDataUtil.isMultiTenant(m, kvBuilder, ptr)) {
+            if (physicalTableName != null && !MetaDataUtil.isMultiTenant(m, kvBuilder, ptr)) {
                 ensureViewIndexTableCreated(tenantIdBytes.length == 0 ? null : PNameFactory.newName(tenantIdBytes), physicalTableName, MetaDataUtil.getClientTimeStamp(m));
             }
         } else if (tableType == PTableType.TABLE && MetaDataUtil.isMultiTenant(m, kvBuilder, ptr)) { // Create view index table up front for multi tenant tables
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/query/QueryConstants.java b/phoenix-core/src/main/java/org/apache/phoenix/query/QueryConstants.java
index db411ab..bf80630 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/query/QueryConstants.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/query/QueryConstants.java
@@ -36,6 +36,7 @@ import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.DISABLE_WAL;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IMMUTABLE_ROWS;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INCREMENT_BY;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INDEX_STATE;
+import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INDEX_TYPE;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IS_AUTOINCREMENT;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IS_NULLABLE;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IS_VIEW_REFERENCED;
@@ -207,6 +208,7 @@ public interface QueryConstants {
             SCOPE_TABLE + " VARCHAR," +
             SOURCE_DATA_TYPE + " SMALLINT," +
             IS_AUTOINCREMENT + " VARCHAR," +
+            INDEX_TYPE + " UNSIGNED_TINYINT," +
             "CONSTRAINT " + SYSTEM_TABLE_PK_NAME + " PRIMARY KEY (" + TENANT_ID + ","
             + TABLE_SCHEM + "," + TABLE_NAME + "," + COLUMN_NAME + "," + COLUMN_FAMILY + "))\n" +
             HConstants.VERSIONS + "=" + MetaDataProtocol.DEFAULT_MAX_META_DATA_VERSIONS + ",\n" +
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index 306ca52..c888007 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -33,6 +33,7 @@ import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.DEFAULT_COLUMN_FAM
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.DISABLE_WAL;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IMMUTABLE_ROWS;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INDEX_STATE;
+import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.INDEX_TYPE;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.IS_VIEW_REFERENCED;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.KEY_SEQ;
 import static org.apache.phoenix.jdbc.PhoenixDatabaseMetaData.LINK_TYPE;
@@ -113,6 +114,7 @@ import org.apache.phoenix.parse.TableName;
 import org.apache.phoenix.query.QueryConstants;
 import org.apache.phoenix.query.QueryServices;
 import org.apache.phoenix.query.QueryServicesOptions;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTable.LinkType;
 import org.apache.phoenix.schema.PTable.ViewType;
 import org.apache.phoenix.util.ByteUtil;
@@ -151,8 +153,9 @@ public class MetaDataClient {
             DISABLE_WAL + "," +
             MULTI_TENANT + "," +
             VIEW_TYPE + "," +
-            VIEW_INDEX_ID +
-            ") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)";
+            VIEW_INDEX_ID + "," +
+            INDEX_TYPE +
+            ") VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)";
     private static final String CREATE_LINK =
             "UPSERT INTO " + SYSTEM_CATALOG_SCHEMA + ".\"" + SYSTEM_CATALOG_TABLE + "\"( " +
             TENANT_ID + "," +
@@ -419,7 +422,7 @@ public class MetaDataClient {
     }
 
     public MutationState createTable(CreateTableStatement statement, byte[][] splits, PTable parent, String viewStatement, ViewType viewType, byte[][] viewColumnConstants, BitSet isViewColumnReferenced) throws SQLException {
-        PTable table = createTableInternal(statement, splits, parent, viewStatement, viewType, viewColumnConstants, isViewColumnReferenced, null);
+        PTable table = createTableInternal(statement, splits, parent, viewStatement, viewType, viewColumnConstants, isViewColumnReferenced, null, null);
         if (table == null || table.getType() == PTableType.VIEW) {
             return new MutationState(0,connection);
         }
@@ -526,8 +529,8 @@ public class MetaDataClient {
         TableRef tableRef = null;
         PTable table = null;
         boolean retry = true;
-        Short viewIndexId = null;
-        boolean allocateViewIndexId = false;
+        Short indexId = null;
+        boolean allocateIndexId = false;
         while (true) {
             try {
                 ColumnResolver resolver = FromCompiler.getResolverForMutation(statement, connection);
@@ -568,8 +571,13 @@ public class MetaDataClient {
                     allPkColumns.add(new Pair<ColumnName, SortOrder>(colName, col.getSortOrder()));
                     columnDefs.add(FACTORY.columnDef(colName, dataType.getSqlTypeName(), col.isNullable(), col.getMaxLength(), col.getScale(), false, SortOrder.getDefault()));
                 }
-                if (dataTable.getType() == PTableType.VIEW && dataTable.getViewType() != ViewType.MAPPED) {
-                    allocateViewIndexId = true;
+                /*
+                 * Allocate an index ID in two circumstances:
+                 * 1) for a local index, as all local indexes will reside in the same HBase table
+                 * 2) for a view on an index.
+                 */
+                if (statement.getIndexType() == IndexType.LOCAL || (dataTable.getType() == PTableType.VIEW && dataTable.getViewType() != ViewType.MAPPED)) {
+                    allocateIndexId = true;
                     // Next add index ID column
                     PDataType dataType = MetaDataUtil.getViewIndexIdDataType();
                     ColumnName colName = ColumnName.caseSensitiveColumnName(MetaDataUtil.getViewIndexIdColumnName());
@@ -626,9 +634,9 @@ public class MetaDataClient {
                     }
                 }
                 
-                // Don't re-allocate viewIndexId on ConcurrentTableMutationException,
+                // Don't re-allocate indexId on ConcurrentTableMutationException,
                 // as there's no need to burn another sequence value.
-                if (allocateViewIndexId && viewIndexId == null) { 
+                if (allocateIndexId && indexId == null) { 
                     Long scn = connection.getSCN();
                     long timestamp = scn == null ? HConstants.LATEST_TIMESTAMP : scn;
                     PName tenantId = connection.getTenantId();
@@ -646,10 +654,10 @@ public class MetaDataClient {
                     }
                     long seqValue = seqValues[0];
                     if (seqValue > Short.MAX_VALUE) {
-                        throw new SQLExceptionInfo.Builder(SQLExceptionCode.TOO_MANY_VIEW_INDEXES)
+                        throw new SQLExceptionInfo.Builder(SQLExceptionCode.TOO_MANY_INDEXES)
                         .setSchemaName(SchemaUtil.getSchemaNameFromFullName(physicalName.getString())).setTableName(SchemaUtil.getTableNameFromFullName(physicalName.getString())).build().buildException();
                     }
-                    viewIndexId = (short) seqValue;
+                    indexId = (short) seqValue;
                 }
                 // Set DEFAULT_COLUMN_FAMILY_NAME of index to match data table
                 // We need this in the props so that the correct column family is created
@@ -657,7 +665,7 @@ public class MetaDataClient {
                     statement.getProps().put("", new Pair<String,Object>(DEFAULT_COLUMN_FAMILY_NAME,dataTable.getDefaultFamilyName().getString()));
                 }
                 CreateTableStatement tableStatement = FACTORY.createTable(indexTableName, statement.getProps(), columnDefs, pk, statement.getSplitNodes(), PTableType.INDEX, statement.ifNotExists(), null, null, statement.getBindCount());
-                table = createTableInternal(tableStatement, splits, dataTable, null, null, null, null, viewIndexId);
+                table = createTableInternal(tableStatement, splits, dataTable, null, null, null, null, indexId, statement.getIndexType());
                 break;
             } catch (ConcurrentTableMutationException e) { // Can happen if parent data table changes while above is in progress
                 if (retry) {
@@ -725,7 +733,7 @@ public class MetaDataClient {
         return null;
     }
     
-    private PTable createTableInternal(CreateTableStatement statement, byte[][] splits, final PTable parent, String viewStatement, ViewType viewType, final byte[][] viewColumnConstants, final BitSet isViewColumnReferenced, Short viewIndexId) throws SQLException {
+    private PTable createTableInternal(CreateTableStatement statement, byte[][] splits, final PTable parent, String viewStatement, ViewType viewType, final byte[][] viewColumnConstants, final BitSet isViewColumnReferenced, Short indexId, IndexType indexType) throws SQLException {
         final PTableType tableType = statement.getTableType();
         boolean wasAutoCommit = connection.getAutoCommit();
         connection.rollback();
@@ -751,10 +759,10 @@ public class MetaDataClient {
                 // TODO: Can we support a multi-tenant index directly on a multi-tenant
                 // table instead of only a view? We don't have anywhere to put the link
                 // from the table to the index, though.
-                if (parent.getType() == PTableType.VIEW && parent.getViewType() != ViewType.MAPPED) {
+                if (indexType == IndexType.LOCAL || (parent.getType() == PTableType.VIEW && parent.getViewType() != ViewType.MAPPED)) {
                     PName physicalName = parent.getPhysicalName();
                     saltBucketNum = parent.getBucketNum();
-                    addSaltColumn = (saltBucketNum != null);
+                    addSaltColumn = (saltBucketNum != null && indexType != IndexType.LOCAL);
                     defaultFamilyName = parent.getDefaultFamilyName() == null ? null : parent.getDefaultFamilyName().getString();
                     // Set physical name of view index table
                     physicalNames = Collections.singletonList(PNameFactory.newName(MetaDataUtil.getViewIndexPhysicalName(physicalName.getBytes())));
@@ -824,10 +832,12 @@ public class MetaDataClient {
             }
             
             // Can't set any of these on views or shared indexes on views
-            if (tableType != PTableType.VIEW && viewIndexId == null) {
+            if (tableType != PTableType.VIEW && indexId == null) {
                 saltBucketNum = (Integer) tableProps.remove(PhoenixDatabaseMetaData.SALT_BUCKETS);
-                if (saltBucketNum != null && (saltBucketNum < 0 || saltBucketNum > SaltingUtil.MAX_BUCKET_NUM)) {
-                    throw new SQLExceptionInfo.Builder(SQLExceptionCode.INVALID_BUCKET_NUM).build().buildException();
+                if (saltBucketNum != null) {
+                    if (saltBucketNum < 0 || saltBucketNum > SaltingUtil.MAX_BUCKET_NUM) {
+                        throw new SQLExceptionInfo.Builder(SQLExceptionCode.INVALID_BUCKET_NUM).build().buildException();
+                    }
                 }
                 // Salt the index table if the data table is salted
                 if (saltBucketNum == null) {
@@ -858,7 +868,11 @@ public class MetaDataClient {
                 disableWAL = disableWALProp;
             }
             // Delay this check as it is supported to have IMMUTABLE_ROWS and SALT_BUCKETS defined on views
-            if ((statement.getTableType() == PTableType.VIEW || viewIndexId != null) && !tableProps.isEmpty()) {
+            if ((statement.getTableType() == PTableType.VIEW || indexId != null) && !tableProps.isEmpty()) {
+                // TODO: do this check in CreateIndexCompiler
+                if (indexType == IndexType.LOCAL && saltBucketNum != null) {
+                    throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SALT_LOCAL_INDEX).build().buildException();
+                }
                 throw new SQLExceptionInfo.Builder(SQLExceptionCode.VIEW_WITH_PROPERTIES).build().buildException();
             }
             if (removedProp) {
@@ -869,7 +883,7 @@ public class MetaDataClient {
             List<PColumn> columns;
             LinkedHashSet<PColumn> pkColumns;    
             
-            if (tenantId != null && (tableType != PTableType.VIEW && viewIndexId == null)) {
+            if (tenantId != null && (tableType != PTableType.VIEW && indexId == null)) {
                 throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_CREATE_TENANT_SPECIFIC_TABLE)
                     .setSchemaName(schemaName).setTableName(tableName).build().buildException();
             }
@@ -1061,9 +1075,9 @@ public class MetaDataClient {
                         null, MetaDataProtocol.MIN_TABLE_TIMESTAMP, PTable.INITIAL_SEQ_NUM,
                         PNameFactory.newName(QueryConstants.SYSTEM_TABLE_PK_NAME), null, columns, null, Collections.<PTable>emptyList(), 
                         isImmutableRows, Collections.<PName>emptyList(),
-                        defaultFamilyName == null ? null : PNameFactory.newName(defaultFamilyName), null, Boolean.TRUE.equals(disableWAL), false, null, viewIndexId);
+                        defaultFamilyName == null ? null : PNameFactory.newName(defaultFamilyName), null, Boolean.TRUE.equals(disableWAL), false, null, indexId, indexType);
                 connection.addTable(table);
-            } else if (tableType == PTableType.INDEX && viewIndexId == null) {
+            } else if (tableType == PTableType.INDEX && indexId == null) {
                 if (tableProps.get(HTableDescriptor.MAX_FILESIZE) == null) {
                     int nIndexRowKeyColumns = isPK ? 1 : pkColumnsNames.size();
                     int nIndexKeyValueColumns = columns.size() - nIndexRowKeyColumns;
@@ -1147,10 +1161,15 @@ public class MetaDataClient {
             } else {
                 tableUpsert.setByte(16, viewType.getSerializedValue());
             }
-            if (viewIndexId == null) {
+            if (indexId == null) {
                 tableUpsert.setNull(17, Types.SMALLINT);
             } else {
-                tableUpsert.setShort(17, viewIndexId);
+                tableUpsert.setShort(17, indexId);
+            }
+            if (indexType == null) {
+                tableUpsert.setNull(18, Types.TINYINT);
+            } else {
+                tableUpsert.setByte(18, indexType.getSerializedValue());
             }
             tableUpsert.execute();
             
@@ -1169,7 +1188,7 @@ public class MetaDataClient {
                     QueryServices.ROW_KEY_ORDER_SALTED_TABLE_ATTRIB, QueryServicesOptions.DEFAULT_ROW_KEY_ORDER_SALTED_TABLE));
             MetaDataMutationResult result = connection.getQueryServices().createTable(
                     tableMetaData, 
-                    viewType == ViewType.MAPPED || viewIndexId != null ? physicalNames.get(0).getBytes() : null,
+                    viewType == ViewType.MAPPED || indexId != null ? physicalNames.get(0).getBytes() : null,
                     tableType, tableProps, familyPropList, splits);
             MutationCode code = result.getMutationCode();
             switch(code) {
@@ -1194,7 +1213,7 @@ public class MetaDataClient {
                         tenantId, PNameFactory.newName(schemaName), PNameFactory.newName(tableName), tableType, indexState, result.getMutationTime(), 
                         PTable.INITIAL_SEQ_NUM, pkName == null ? null : PNameFactory.newName(pkName), saltBucketNum, columns, 
                         dataTableName == null ? null : PNameFactory.newName(dataTableName), Collections.<PTable>emptyList(), isImmutableRows, physicalNames,
-                        defaultFamilyName == null ? null : PNameFactory.newName(defaultFamilyName), viewStatement, Boolean.TRUE.equals(disableWAL), multiTenant, viewType, viewIndexId);
+                        defaultFamilyName == null ? null : PNameFactory.newName(defaultFamilyName), viewStatement, Boolean.TRUE.equals(disableWAL), multiTenant, viewType, indexId, indexType);
                 connection.addTable(table);
                 return table;
             }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/PTable.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/PTable.java
index 74a9373..cbf0dad 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/PTable.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/PTable.java
@@ -80,6 +80,42 @@ public interface PTable {
         }
     }
 
+    public enum IndexType { 
+        GLOBAL((byte)1),
+        LOCAL((byte)2);
+
+        private final byte[] byteValue;
+        private final byte serializedValue;
+        
+        IndexType(byte serializedValue) {
+            this.serializedValue = serializedValue;
+            this.byteValue = Bytes.toBytes(this.name());
+        }
+        
+        public byte[] getBytes() {
+            return byteValue;
+        }
+        
+        public byte getSerializedValue() {
+            return this.serializedValue;
+        }
+        
+        public static IndexType getDefault() {
+            return GLOBAL;
+        }
+        
+        public static IndexType fromToken(String token) {
+            return IndexType.valueOf(token.trim().toUpperCase());
+        }
+        
+        public static IndexType fromSerializedValue(byte serializedValue) {
+            if (serializedValue < 1 || serializedValue > IndexType.values().length) {
+                throw new IllegalArgumentException("Invalid IndexType " + serializedValue);
+            }
+            return IndexType.values()[serializedValue-1];
+        }
+    }
+
     public enum LinkType {
         /**
          * Link from a table to its index table
@@ -279,4 +315,5 @@ public interface PTable {
     PTableKey getKey();
     
     int getEstimatedSize();
+    IndexType getIndexType();
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/PTableImpl.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/PTableImpl.java
index 07d9781..709882a 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/PTableImpl.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/PTableImpl.java
@@ -115,6 +115,7 @@ public class PTableImpl implements PTable {
     private ViewType viewType;
     private Short viewIndexId;
     private int estimatedSize;
+    private IndexType indexType;
     
     public PTableImpl() {
     }
@@ -172,28 +173,28 @@ public class PTableImpl implements PTable {
         return new PTableImpl(
                 table.getTenantId(), table.getSchemaName(), table.getTableName(), table.getType(), table.getIndexState(), timeStamp, 
                 table.getSequenceNumber() + 1, table.getPKName(), table.getBucketNum(), getColumnsToClone(table), table.getParentTableName(), indexes,
-                table.isImmutableRows(), table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                table.isImmutableRows(), table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
     }
 
     public static PTableImpl makePTable(PTable table, List<PColumn> columns) throws SQLException {
         return new PTableImpl(
                 table.getTenantId(), table.getSchemaName(), table.getTableName(), table.getType(), table.getIndexState(), table.getTimeStamp(), 
                 table.getSequenceNumber(), table.getPKName(), table.getBucketNum(), columns, table.getParentTableName(), table.getIndexes(), table.isImmutableRows(),
-                table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
     }
 
     public static PTableImpl makePTable(PTable table, long timeStamp, long sequenceNumber, List<PColumn> columns) throws SQLException {
         return new PTableImpl(
                 table.getTenantId(), table.getSchemaName(), table.getTableName(), table.getType(), table.getIndexState(), timeStamp, 
                 sequenceNumber, table.getPKName(), table.getBucketNum(), columns, table.getParentTableName(), table.getIndexes(), table.isImmutableRows(),
-                table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
     }
 
     public static PTableImpl makePTable(PTable table, long timeStamp, long sequenceNumber, List<PColumn> columns, boolean isImmutableRows) throws SQLException {
         return new PTableImpl(
                 table.getTenantId(), table.getSchemaName(), table.getTableName(), table.getType(), table.getIndexState(), timeStamp, 
                 sequenceNumber, table.getPKName(), table.getBucketNum(), columns, table.getParentTableName(), table.getIndexes(),
-                isImmutableRows, table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                isImmutableRows, table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
     }
 
     public static PTableImpl makePTable(PTable table, PIndexState state) throws SQLException {
@@ -201,21 +202,21 @@ public class PTableImpl implements PTable {
                 table.getTenantId(), table.getSchemaName(), table.getTableName(), table.getType(), state, table.getTimeStamp(), 
                 table.getSequenceNumber(), table.getPKName(), table.getBucketNum(), getColumnsToClone(table), 
                 table.getParentTableName(), table.getIndexes(), table.isImmutableRows(),
-                table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                table.getPhysicalNames(), table.getDefaultFamilyName(), table.getViewStatement(), table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
     }
 
     public static PTableImpl makePTable(PName tenantId, PName schemaName, PName tableName, PTableType type, PIndexState state, long timeStamp, long sequenceNumber,
             PName pkName, Integer bucketNum, List<PColumn> columns, PName dataTableName, List<PTable> indexes, boolean isImmutableRows,
-            List<PName> physicalNames, PName defaultFamilyName, String viewExpression, boolean disableWAL, boolean multiTenant, ViewType viewType, Short viewIndexId) throws SQLException {
+            List<PName> physicalNames, PName defaultFamilyName, String viewExpression, boolean disableWAL, boolean multiTenant, ViewType viewType, Short viewIndexId, IndexType indexType) throws SQLException {
         return new PTableImpl(tenantId, schemaName, tableName, type, state, timeStamp, sequenceNumber, pkName, bucketNum, columns, dataTableName,
-                indexes, isImmutableRows, physicalNames, defaultFamilyName, viewExpression, disableWAL, multiTenant, viewType, viewIndexId);
+                indexes, isImmutableRows, physicalNames, defaultFamilyName, viewExpression, disableWAL, multiTenant, viewType, viewIndexId, indexType);
     }
 
     private PTableImpl(PName tenantId, PName schemaName, PName tableName, PTableType type, PIndexState state, long timeStamp, long sequenceNumber,
             PName pkName, Integer bucketNum, List<PColumn> columns, PName dataTableName, List<PTable> indexes, boolean isImmutableRows,
-            List<PName> physicalNames, PName defaultFamilyName, String viewExpression, boolean disableWAL, boolean multiTenant, ViewType viewType, Short viewIndexId) throws SQLException {
+            List<PName> physicalNames, PName defaultFamilyName, String viewExpression, boolean disableWAL, boolean multiTenant, ViewType viewType, Short viewIndexId, IndexType indexType) throws SQLException {
         init(tenantId, schemaName, tableName, type, state, timeStamp, sequenceNumber, pkName, bucketNum, columns,
-                new PTableStatsImpl(), dataTableName, indexes, isImmutableRows, physicalNames, defaultFamilyName, viewExpression, disableWAL, multiTenant, viewType, viewIndexId);
+                new PTableStatsImpl(), dataTableName, indexes, isImmutableRows, physicalNames, defaultFamilyName, viewExpression, disableWAL, multiTenant, viewType, viewIndexId, indexType);
     }
 
     @Override
@@ -237,7 +238,7 @@ public class PTableImpl implements PTable {
     private void init(PName tenantId, PName schemaName, PName tableName, PTableType type, PIndexState state, long timeStamp, long sequenceNumber,
             PName pkName, Integer bucketNum, List<PColumn> columns, PTableStats stats, PName parentTableName, List<PTable> indexes,
             boolean isImmutableRows, List<PName> physicalNames, PName defaultFamilyName, String viewExpression, boolean disableWAL, boolean multiTenant,
-            ViewType viewType, Short viewIndexId) throws SQLException {
+            ViewType viewType, Short viewIndexId, IndexType indexType) throws SQLException {
         Preconditions.checkNotNull(schemaName);
         Preconditions.checkArgument(tenantId==null || tenantId.getBytes().length > 0); // tenantId should be null or not empty
         int estimatedSize = SizedUtil.OBJECT_SIZE * 2 + 23 * SizedUtil.POINTER_SIZE + 4 * SizedUtil.INT_SIZE + 2 * SizedUtil.LONG_SIZE + 2 * SizedUtil.INT_OBJECT_SIZE +
@@ -264,6 +265,7 @@ public class PTableImpl implements PTable {
         this.multiTenant = multiTenant;
         this.viewType = viewType;
         this.viewIndexId = viewIndexId;
+        this.indexType = indexType;
         List<PColumn> pkColumns;
         PColumn[] allColumns;
 
@@ -802,6 +804,11 @@ public class PTableImpl implements PTable {
         return tenantId;
     }
     
+    @Override
+    public IndexType getIndexType() {
+        return indexType;
+    }
+
     /**
      * Construct a PTable instance from ProtoBuffered PTable instance
      * @param table
@@ -822,6 +829,10 @@ public class PTableImpl implements PTable {
       if(table.hasViewIndexId()){
     	  viewIndexId = (short)table.getViewIndexId();
       }
+      IndexType indexType = IndexType.getDefault();
+      if(table.hasIndexType()){
+          indexType = IndexType.fromSerializedValue(table.getIndexType().toByteArray()[0]);
+      }
       long sequenceNumber = table.getSequenceNumber();
       long timeStamp = table.getTimeStamp();
       PName pkName = null;
@@ -877,7 +888,7 @@ public class PTableImpl implements PTable {
         PTableImpl result = new PTableImpl();
         result.init(tenantId, schemaName, tableName, tableType, indexState, timeStamp, sequenceNumber, pkName,
           (bucketNum == NO_SALTING) ? null : bucketNum, columns, stats, dataTableName,indexes, isImmutableRows, 
-              physicalNames, defaultFamilyName, viewStatement, disableWAL, multiTenant, viewType, viewIndexId);
+              physicalNames, defaultFamilyName, viewStatement, disableWAL, multiTenant, viewType, viewIndexId, indexType);
         return result;
       } catch (SQLException e) {
         throw new RuntimeException(e); // Impossible
@@ -899,6 +910,9 @@ public class PTableImpl implements PTable {
     	if(table.getViewIndexId() != null) {
     	  builder.setViewIndexId(table.getViewIndexId());
     	}
+    	if(table.getIndexType() != null) {
+    	    builder.setIndexType(HBaseZeroCopyByteString.wrap(new byte[]{table.getIndexType().getSerializedValue()}));
+    	}
       }
       builder.setSequenceNumber(table.getSequenceNumber());
       builder.setTimeStamp(table.getTimeStamp());
diff --git a/phoenix-protocol/src/main/PTable.proto b/phoenix-protocol/src/main/PTable.proto
index 96bb52e..20c63e1 100644
--- a/phoenix-protocol/src/main/PTable.proto
+++ b/phoenix-protocol/src/main/PTable.proto
@@ -73,4 +73,5 @@ message PTable {
   repeated bytes physicalNames = 19;
   optional bytes tenantId = 20;
   optional int32 viewIndexId = 21; 
+  optional bytes indexType = 22;
 }
-- 
1.9.4.msysgit.0


From f203f14c7ecde1e3731f85d8f7339241bacc0538 Mon Sep 17 00:00:00 2001
From: James Taylor <jamestaylor@apache.org>
Date: Fri, 18 Apr 2014 16:31:29 -0700
Subject: [PATCH 02/14] PHOENIX-936 Custom load balancer to colocate user table
 regions and index table regions (Rajeshbabu)

---
 .../index/balancer/TestIndexLoadBalancer.java      | 531 ++++++++++++++++
 .../hbase/index/balancer/IndexLoadBalancer.java    | 688 +++++++++++++++++++++
 2 files changed, 1219 insertions(+)
 create mode 100644 phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/hbase/index/balancer/IndexLoadBalancer.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java b/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
new file mode 100644
index 0000000..fb0afa9
--- /dev/null
+++ b/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
@@ -0,0 +1,531 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.hbase.index.balancer;
+
+import static org.junit.Assert.assertTrue;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.MiniHBaseCluster;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.catalog.MetaReader;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.coprocessor.BaseMasterObserver;
+import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.ObserverContext;
+import org.apache.hadoop.hbase.master.HMaster;
+import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.hadoop.hbase.master.RegionStates;
+import org.apache.hadoop.hbase.regionserver.HRegionServer;
+import org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.MockedRegionObserver;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.hbase.util.Threads;
+import org.apache.hadoop.hbase.zookeeper.ZKAssign;
+import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
+import org.apache.phoenix.hbase.index.IndexTestingUtils;
+import org.apache.phoenix.hbase.index.Indexer;
+import org.apache.phoenix.util.ConfigUtil;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category(LargeTests.class)
+public class TestIndexLoadBalancer {
+
+    private static final Log LOG = LogFactory.getLog(TestIndexLoadBalancer.class);
+
+    private static HBaseTestingUtility UTIL = new HBaseTestingUtility();
+
+    private static HBaseAdmin admin = null;
+
+    @BeforeClass
+    public static void setupCluster() throws Exception {
+        final int NUM_RS = 4;
+        Configuration conf = UTIL.getConfiguration();
+        conf.setBoolean(HConstants.REGIONSERVER_INFO_PORT_AUTO, true);
+        conf.set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY, MockedMasterObserver.class.getName());
+        conf.setClass(HConstants.HBASE_MASTER_LOADBALANCER_CLASS, IndexLoadBalancer.class,
+            LoadBalancer.class);
+        IndexTestingUtils.setupConfig(conf);
+        // disable version checking, so we can test against whatever version of HBase happens to be
+        // installed (right now, its generally going to be SNAPSHOT versions).
+        conf.setBoolean(Indexer.CHECK_VERSION_CONF_KEY, false);
+        // set replication required parameter
+        ConfigUtil.setReplicationConfigIfAbsent(conf);
+        UTIL.startMiniCluster(NUM_RS);
+        admin = UTIL.getHBaseAdmin();
+    }
+
+    @AfterClass
+    public static void tearDownAfterClass() throws Exception {
+        if (admin != null) {
+            admin.disableTables(".*");
+            admin.deleteTables(".*");
+            admin.close();
+        }
+        UTIL.shutdownMiniCluster();
+    }
+
+    @Test(timeout = 180000)
+    public void testRoundRobinAssignmentDuringIndexTableCreation() throws Exception {
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        TableName tableName = TableName.valueOf("testRoundRobinAssignmentDuringIndexTableCreation");
+        TableName indexTableName =
+                TableName.valueOf("testRoundRobinAssignmentDuringIndexTableCreation_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+    }
+
+    @Test(timeout = 180000)
+    public void testColocationAfterSplit() throws Exception {
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        // Table names to make use of the
+        TableName tableName = TableName.valueOf("testSplitHooksBeforeAndAfterPONR_1");
+        TableName indexTableName = TableName.valueOf("testSplitHooksBeforeAndAfterPONR_2");
+        HTableDescriptor htd = new HTableDescriptor(tableName);
+        htd.addCoprocessor(MockedRegionObserver.class.getName());
+        htd.addFamily(new HColumnDescriptor("cf"));
+        char c = 'A';
+        byte[][] split = new byte[20][];
+        for (int i = 0; i < 20; i++) {
+            byte[] b = { (byte) c };
+            split[i] = b;
+            c++;
+        }
+        admin.createTable(htd, split);
+        HTableDescriptor iHtd = new HTableDescriptor(indexTableName);
+        iHtd.addFamily(new HColumnDescriptor("cf"));
+        iHtd.setValue(IndexLoadBalancer.PARENT_TABLE_KEY, tableName.toBytes());
+        admin.createTable(iHtd, split);
+
+        // test put with the indexed column
+
+        insertData(tableName);
+        insertData(indexTableName);
+
+        admin.split(tableName.getNameAsString(), "c");
+        List<HRegionInfo> regionsOfUserTable =
+                master.getAssignmentManager().getRegionStates().getRegionsOfTable(tableName);
+
+        while (regionsOfUserTable.size() != 22) {
+            Thread.sleep(100);
+            regionsOfUserTable =
+                    master.getAssignmentManager().getRegionStates().getRegionsOfTable(tableName);
+        }
+
+        List<HRegionInfo> regionsOfIndexTable =
+                master.getAssignmentManager().getRegionStates().getRegionsOfTable(indexTableName);
+
+        while (regionsOfIndexTable.size() != 22) {
+            Thread.sleep(100);
+            regionsOfIndexTable =
+                    master.getAssignmentManager().getRegionStates().getRegionsOfTable(
+                        indexTableName);
+        }
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+    }
+
+    @Test(timeout = 180000)
+    public void testColocationAfterRegionsMerge() throws Exception {
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        RegionStates regionStates = master.getAssignmentManager().getRegionStates();
+        // Table names to make use of the
+        TableName tableName = TableName.valueOf("testColocationAfterRegionsMerge");
+        TableName indexTableName = TableName.valueOf("testColocationAfterRegionsMerge_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        ServerName server = cluster.getRegionServer(0).getServerName();
+        List<HRegionInfo> regionsOfUserTable = regionStates.getRegionsOfTable(tableName);
+        Pair<HRegionInfo, HRegionInfo> regionsToMerge = new Pair<HRegionInfo, HRegionInfo>();
+        byte[] startKey1 = { (byte) 'C' };
+        byte[] startKey2 = { (byte) 'D' };
+        for (HRegionInfo region : regionsOfUserTable) {
+            if (Bytes.compareTo(startKey1, region.getStartKey()) == 0) {
+                regionsToMerge.setFirst(region);
+            } else if (Bytes.compareTo(startKey2, region.getStartKey()) == 0) {
+                regionsToMerge.setSecond(region);
+            }
+        }
+        admin.move(regionsToMerge.getFirst().getEncodedNameAsBytes(), Bytes.toBytes(server
+                .toString()));
+        admin.move(regionsToMerge.getSecond().getEncodedNameAsBytes(), Bytes.toBytes(server
+                .toString()));
+
+        List<HRegionInfo> regionsOfIndexTable = regionStates.getRegionsOfTable(indexTableName);
+        Pair<HRegionInfo, HRegionInfo> indexRegionsToMerge = new Pair<HRegionInfo, HRegionInfo>();
+        for (HRegionInfo region : regionsOfIndexTable) {
+            if (Bytes.compareTo(startKey1, region.getStartKey()) == 0) {
+                indexRegionsToMerge.setFirst(region);
+            } else if (Bytes.compareTo(startKey2, region.getStartKey()) == 0) {
+                indexRegionsToMerge.setSecond(region);
+            }
+        }
+        admin.move(indexRegionsToMerge.getFirst().getEncodedNameAsBytes(), Bytes.toBytes(server
+                .toString()));
+        admin.move(indexRegionsToMerge.getSecond().getEncodedNameAsBytes(), Bytes.toBytes(server
+                .toString()));
+        while (!regionStates.getRegionServerOfRegion(regionsToMerge.getFirst()).equals(server)
+                || !regionStates.getRegionServerOfRegion(regionsToMerge.getSecond()).equals(server)
+                || !regionStates.getRegionServerOfRegion(indexRegionsToMerge.getFirst()).equals(
+                    server)
+                || !regionStates.getRegionServerOfRegion(indexRegionsToMerge.getSecond()).equals(
+                    server)) {
+            Threads.sleep(1000);
+        }
+        admin.mergeRegions(regionsToMerge.getFirst().getEncodedNameAsBytes(), regionsToMerge
+                .getSecond().getEncodedNameAsBytes(), true);
+        admin.mergeRegions(indexRegionsToMerge.getFirst().getEncodedNameAsBytes(),
+            indexRegionsToMerge.getSecond().getEncodedNameAsBytes(), true);
+
+        while (regionsOfUserTable.size() != 20 || regionsOfIndexTable.size() != 20) {
+            Thread.sleep(100);
+            regionsOfUserTable = regionStates.getRegionsOfTable(tableName);
+            regionsOfIndexTable = regionStates.getRegionsOfTable(indexTableName);
+        }
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+    }
+
+    private void insertData(TableName tableName) throws IOException, InterruptedException {
+        HTable table = new HTable(admin.getConfiguration(), tableName);
+        Put p = new Put("a".getBytes());
+        p.add(Bytes.toBytes("cf"), Bytes.toBytes("q1"), Bytes.toBytes("Val"));
+        p.add("cf".getBytes(), "q2".getBytes(), Bytes.toBytes("ValForCF2"));
+        table.put(p);
+
+        Put p1 = new Put("b".getBytes());
+        p1.add(Bytes.toBytes("cf"), Bytes.toBytes("q1"), Bytes.toBytes("Val"));
+        p1.add("cf".getBytes(), "q2".getBytes(), Bytes.toBytes("ValForCF2"));
+        table.put(p1);
+
+        Put p2 = new Put("c".getBytes());
+        p2.add(Bytes.toBytes("cf"), Bytes.toBytes("q1"), Bytes.toBytes("Val"));
+        p2.add("cf".getBytes(), "q2".getBytes(), Bytes.toBytes("ValForCF2"));
+        table.put(p2);
+
+        Put p3 = new Put("c1".getBytes());
+        p3.add(Bytes.toBytes("cf"), Bytes.toBytes("q1"), Bytes.toBytes("Val"));
+        p3.add("cf".getBytes(), "q2".getBytes(), Bytes.toBytes("ValForCF2"));
+        table.put(p3);
+
+        Put p4 = new Put("d".getBytes());
+        p4.add(Bytes.toBytes("cf"), Bytes.toBytes("q1"), Bytes.toBytes("Val"));
+        p4.add("cf".getBytes(), "q2".getBytes(), Bytes.toBytes("ValForCF2"));
+        table.put(p4);
+        admin.flush(tableName.getNameAsString());
+    }
+
+    @Test(timeout = 180000)
+    public void testRandomAssignmentDuringIndexTableEnable() throws Exception {
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        master.getConfiguration().setBoolean("hbase.master.enabletable.roundrobin", false);
+        TableName tableName = TableName.valueOf("testRandomAssignmentDuringIndexTableEnable");
+        TableName indexTableName =
+                TableName.valueOf("testRandomAssignmentDuringIndexTableEnable_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        admin.disableTable(tableName);
+        admin.disableTable(indexTableName);
+        admin.enableTable(tableName);
+        admin.enableTable(indexTableName);
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+
+    }
+
+    @Test(timeout = 180000)
+    public void testBalanceCluster() throws Exception {
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        master.getConfiguration().setBoolean("hbase.master.enabletable.roundrobin", false);
+        master.getConfiguration().setBoolean("hbase.master.startup.retainassign", false);
+        master.getConfiguration().setBoolean("hbase.master.loadbalance.bytable", false);
+
+        TableName tableName = TableName.valueOf("testBalanceCluster");
+        TableName indexTableName = TableName.valueOf("testBalanceCluster_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        HTableDescriptor htd1 = new HTableDescriptor(TableName.valueOf("testBalanceCluster1"));
+        htd1.addFamily(new HColumnDescriptor("fam1"));
+        char c = 'A';
+        byte[][] split1 = new byte[12][];
+        for (int i = 0; i < 12; i++) {
+            byte[] b = { (byte) c };
+            split1[i] = b;
+            c++;
+        }
+        admin.setBalancerRunning(false, false);
+        admin.createTable(htd1, split1);
+        admin.disableTable(tableName);
+        admin.enableTable(tableName);
+        admin.setBalancerRunning(true, false);
+        admin.balancer();
+        boolean isRegionsColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionsColocated);
+    }
+
+    @Test(timeout = 180000)
+    public void testBalanceByTable() throws Exception {
+        ZooKeeperWatcher zkw = UTIL.getZooKeeperWatcher(UTIL);
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        master.getConfiguration().setBoolean("hbase.master.loadbalance.bytable", true);
+        TableName tableName = TableName.valueOf("testBalanceByTable");
+        TableName indexTableName = TableName.valueOf("testBalanceByTable_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        HTableDescriptor htd1 = new HTableDescriptor(TableName.valueOf("testBalanceByTable1"));
+        htd1.addFamily(new HColumnDescriptor("fam1"));
+        char c = 'A';
+        byte[][] split1 = new byte[12][];
+        for (int i = 0; i < 12; i++) {
+            byte[] b = { (byte) c };
+            split1[i] = b;
+            c++;
+        }
+        admin.disableTable(tableName);
+        admin.enableTable(tableName);
+        admin.setBalancerRunning(true, false);
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+        admin.balancer();
+        Thread.sleep(10000);
+        ZKAssign.blockUntilNoRIT(zkw);
+        while (master.getAssignmentManager().getRegionStates().isRegionsInTransition()) {
+            Threads.sleep(1000);
+        }
+        isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+    }
+
+    @Test(timeout = 180000)
+    public void testRoundRobinAssignmentAfterRegionServerDown() throws Exception {
+        ZooKeeperWatcher zkw = UTIL.getZooKeeperWatcher(UTIL);
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        TableName tableName = TableName.valueOf("testRoundRobinAssignmentAfterRegionServerDown");
+        TableName indexTableName =
+                TableName.valueOf("testRoundRobinAssignmentAfterRegionServerDown_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        HRegionServer regionServer = cluster.getRegionServer(1);
+        regionServer.abort("Aborting to test random assignment after region server down");
+        while (master.getServerManager().areDeadServersInProgress()) {
+            Thread.sleep(1000);
+        }
+        ZKAssign.blockUntilNoRIT(zkw);
+        while (master.getAssignmentManager().getRegionStates().isRegionsInTransition()) {
+            Threads.sleep(1000);
+        }
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+
+    }
+
+    @Test(timeout = 180000)
+    public void testRetainAssignmentDuringMasterStartUp() throws Exception {
+        ZooKeeperWatcher zkw = UTIL.getZooKeeperWatcher(UTIL);
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        master.getConfiguration().setBoolean("hbase.master.startup.retainassign", true);
+        TableName tableName = TableName.valueOf("testRetainAssignmentDuringMasterStartUp");
+        TableName indexTableName =
+                TableName.valueOf("testRetainAssignmentDuringMasterStartUp_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        UTIL.shutdownMiniHBaseCluster();
+        UTIL.startMiniHBaseCluster(1, 4);
+        cluster = UTIL.getHBaseCluster();
+        master = cluster.getMaster();
+        if (admin != null) {
+            admin.close();
+            admin = new HBaseAdmin(master.getConfiguration());
+        }
+        ZKAssign.blockUntilNoRIT(zkw);
+        while (master.getAssignmentManager().getRegionStates().isRegionsInTransition()) {
+            Threads.sleep(1000);
+        }
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+
+    }
+
+    @Test(timeout = 300000)
+    public void testRoundRobinAssignmentDuringMasterStartUp() throws Exception {
+        MiniHBaseCluster cluster = UTIL.getHBaseCluster();
+        HMaster master = cluster.getMaster();
+        UTIL.getConfiguration().setBoolean("hbase.master.startup.retainassign", false);
+
+        TableName tableName = TableName.valueOf("testRoundRobinAssignmentDuringMasterStartUp");
+        TableName indexTableName =
+                TableName.valueOf("testRoundRobinAssignmentDuringMasterStartUp_index");
+        createUserAndIndexTable(tableName, indexTableName);
+        UTIL.shutdownMiniHBaseCluster();
+        cluster.waitUntilShutDown();
+        UTIL.startMiniHBaseCluster(1, 4);
+        cluster = UTIL.getHBaseCluster();
+        if (admin != null) {
+            admin.close();
+            admin = new HBaseAdmin(cluster.getMaster().getConfiguration());
+        }
+        master = cluster.getMaster();
+        while (master.getAssignmentManager().getRegionStates().isRegionsInTransition()) {
+            Threads.sleep(1000);
+        }
+        boolean isRegionColocated =
+                checkForColocation(master, tableName.getNameAsString(), indexTableName
+                        .getNameAsString());
+        assertTrue("User regions and index regions should colocate.", isRegionColocated);
+    }
+
+    private void createUserAndIndexTable(TableName tableName, TableName indexTableName)
+            throws IOException {
+        HTableDescriptor htd = new HTableDescriptor(tableName);
+        htd.addFamily(new HColumnDescriptor("cf"));
+        char c = 'A';
+        byte[][] split = new byte[20][];
+        for (int i = 0; i < 20; i++) {
+            byte[] b = { (byte) c };
+            split[i] = b;
+            c++;
+        }
+        admin.createTable(htd, split);
+        HTableDescriptor iHtd = new HTableDescriptor(indexTableName);
+        iHtd.addFamily(new HColumnDescriptor("cf"));
+        iHtd.setValue(IndexLoadBalancer.PARENT_TABLE_KEY, tableName.toBytes());
+        admin.createTable(iHtd, split);
+    }
+
+    private List<Pair<byte[], ServerName>> getStartKeysAndLocations(HMaster master, String tableName)
+            throws IOException, InterruptedException {
+
+        List<Pair<HRegionInfo, ServerName>> tableRegionsAndLocations =
+                MetaReader.getTableRegionsAndLocations(master.getCatalogTracker(), TableName
+                        .valueOf(tableName));
+        List<Pair<byte[], ServerName>> startKeyAndLocationPairs =
+                new ArrayList<Pair<byte[], ServerName>>(tableRegionsAndLocations.size());
+        Pair<byte[], ServerName> startKeyAndLocation = null;
+        for (Pair<HRegionInfo, ServerName> regionAndLocation : tableRegionsAndLocations) {
+            startKeyAndLocation =
+                    new Pair<byte[], ServerName>(regionAndLocation.getFirst().getStartKey(),
+                            regionAndLocation.getSecond());
+            startKeyAndLocationPairs.add(startKeyAndLocation);
+        }
+        return startKeyAndLocationPairs;
+
+    }
+
+    public boolean checkForColocation(HMaster master, String tableName, String indexTableName)
+            throws IOException, InterruptedException {
+        List<Pair<byte[], ServerName>> uTableStartKeysAndLocations =
+                getStartKeysAndLocations(master, tableName);
+        List<Pair<byte[], ServerName>> iTableStartKeysAndLocations =
+                getStartKeysAndLocations(master, indexTableName);
+
+        boolean regionsColocated = true;
+        if (uTableStartKeysAndLocations.size() != iTableStartKeysAndLocations.size()) {
+            regionsColocated = false;
+        } else {
+            for (int i = 0; i < uTableStartKeysAndLocations.size(); i++) {
+                Pair<byte[], ServerName> uStartKeyAndLocation = uTableStartKeysAndLocations.get(i);
+                Pair<byte[], ServerName> iStartKeyAndLocation = iTableStartKeysAndLocations.get(i);
+
+                if (Bytes.compareTo(uStartKeyAndLocation.getFirst(), iStartKeyAndLocation
+                        .getFirst()) == 0) {
+                    if (uStartKeyAndLocation.getSecond().equals(iStartKeyAndLocation.getSecond())) {
+                        continue;
+                    }
+                }
+                regionsColocated = false;
+            }
+        }
+        return regionsColocated;
+    }
+
+    public static class MockedMasterObserver extends BaseMasterObserver {
+        IndexLoadBalancer balancer = null;
+
+        @Override
+        public void preMasterInitialization(ObserverContext<MasterCoprocessorEnvironment> ctx)
+                throws IOException {
+            LoadBalancer loadBalancer =
+                    ctx.getEnvironment().getMasterServices().getAssignmentManager().getBalancer();
+            if (loadBalancer instanceof IndexLoadBalancer) {
+                balancer = (IndexLoadBalancer) loadBalancer;
+            }
+            super.preMasterInitialization(ctx);
+        }
+
+        @Override
+        public void preCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+                HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+            TableName userTableName = null;
+            if (balancer != null && desc.getValue(IndexLoadBalancer.PARENT_TABLE_KEY) != null) {
+                userTableName =
+                        TableName.valueOf(desc.getValue(IndexLoadBalancer.PARENT_TABLE_KEY));
+                balancer.addTablesToColocate(userTableName, desc.getTableName());
+            }
+            if (userTableName != null) balancer.populateRegionLocations(userTableName);
+            super.preCreateTableHandler(ctx, desc, regions);
+        }
+
+        @Override
+        public void postDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+                TableName tableName) throws IOException {
+            if (balancer.isTableColocated(tableName)) {
+                balancer.removeTablesFromColocation(tableName);
+            }
+        }
+    }
+
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/balancer/IndexLoadBalancer.java b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/balancer/IndexLoadBalancer.java
new file mode 100644
index 0000000..fc1963b
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/balancer/IndexLoadBalancer.java
@@ -0,0 +1,688 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.hbase.index.balancer;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Random;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.ClusterStatus;
+import org.apache.hadoop.hbase.HBaseIOException;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.hadoop.hbase.master.MasterServices;
+import org.apache.hadoop.hbase.master.RegionPlan;
+import org.apache.hadoop.hbase.master.RegionState;
+import org.apache.hadoop.hbase.master.RegionStates;
+import org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
+import org.apache.hadoop.util.ReflectionUtils;
+
+/**
+ * <p>This class is an extension of the load balancer class. 
+ * It allows to co-locate the regions of the user table and the regions of corresponding
+ * index table if any.</p> 
+ * 
+ * </>roundRobinAssignment, retainAssignment -> index regions will follow the actual table regions. 
+ * randomAssignment, balancerCluster -> either index table or actual table region(s) will follow
+ * each other based on which ever comes first.</p> 
+ * 
+ * <p>In case of master failover there is a chance that the znodes of the index
+ * table and actual table are left behind. Then in that scenario we may get randomAssignment for
+ * either the actual table region first or the index table region first.</p>
+ * 
+ * <p>In case of balancing by table any table can balance first.</p>
+ * 
+ */
+
+public class IndexLoadBalancer implements LoadBalancer {
+
+    private static final Log LOG = LogFactory.getLog(IndexLoadBalancer.class);
+
+    public static final byte[] PARENT_TABLE_KEY = Bytes.toBytes("PARENT_TABLE");
+
+    public static final String INDEX_BALANCER_DELEGATOR = "hbase.index.balancer.delegator.class";
+
+    private LoadBalancer delegator;
+
+    private MasterServices master;
+
+    private Configuration conf;
+
+    private ClusterStatus clusterStatus;
+
+    private static final Random RANDOM = new Random(EnvironmentEdgeManager.currentTimeMillis());
+
+    Map<TableName, TableName> userTableVsIndexTable = new HashMap<TableName, TableName>();
+
+    Map<TableName, TableName> indexTableVsUserTable = new HashMap<TableName, TableName>();
+
+    /**
+     * Maintains colocation information of user regions and corresponding index regions.
+     */
+    private Map<TableName, Map<ImmutableBytesWritable, ServerName>> colocationInfo =
+            new ConcurrentHashMap<TableName, Map<ImmutableBytesWritable, ServerName>>();
+
+    private Set<TableName> balancedTables = new HashSet<TableName>();
+
+    private boolean stopped = false;
+
+    @Override
+    public void initialize() throws HBaseIOException {
+        Class<? extends LoadBalancer> delegatorKlass =
+                conf.getClass(INDEX_BALANCER_DELEGATOR, StochasticLoadBalancer.class,
+                    LoadBalancer.class);
+        this.delegator = ReflectionUtils.newInstance(delegatorKlass, conf);
+        this.delegator.setClusterStatus(clusterStatus);
+        this.delegator.setMasterServices(this.master);
+        this.delegator.initialize();
+        try {
+            populateTablesToColocate(this.master.getTableDescriptors().getAll());
+        } catch (IOException e) {
+            throw new HBaseIOException(e);
+        }
+    }
+
+    @Override
+    public Configuration getConf() {
+        return conf;
+    }
+
+    @Override
+    public void setConf(Configuration configuration) {
+        this.conf = configuration;
+    }
+
+    @Override
+    public void setClusterStatus(ClusterStatus st) {
+        this.clusterStatus = st;
+    }
+
+    public Map<TableName, Map<ImmutableBytesWritable, ServerName>> getColocationInfo() {
+        return colocationInfo;
+    }
+
+    @Override
+    public void setMasterServices(MasterServices masterServices) {
+        this.master = masterServices;
+    }
+
+    @Override
+    public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState)
+            throws HBaseIOException {
+        synchronized (this.colocationInfo) {
+            boolean balanceByTable = conf.getBoolean("hbase.master.loadbalance.bytable", false);
+            List<RegionPlan> regionPlans = null;
+
+            TableName tableName = null;
+            if (balanceByTable) {
+                Map<ImmutableBytesWritable, ServerName> tableKeys = null;
+                for (Entry<ServerName, List<HRegionInfo>> serverVsRegionList : clusterState
+                        .entrySet()) {
+                    ServerName sn = serverVsRegionList.getKey();
+                    List<HRegionInfo> regionInfos = serverVsRegionList.getValue();
+                    if (regionInfos.isEmpty()) {
+                        continue;
+                    }
+                    if (!isTableColocated(regionInfos.get(0).getTable())) {
+                        return this.delegator.balanceCluster(clusterState);
+                    }
+                    // Just get the table name from any one of the values in the regioninfo list
+                    if (tableName == null) {
+                        tableName = regionInfos.get(0).getTable();
+                        tableKeys = this.colocationInfo.get(tableName);
+                    }
+                    // Check and modify the colocation info map based on values of cluster state
+                    // because we
+                    // will
+                    // call balancer only when the cluster is in stable and reliable state.
+                    if (tableKeys != null) {
+                        for (HRegionInfo hri : regionInfos) {
+                            updateServer(tableKeys, sn, hri);
+                        }
+                    }
+                }
+                // If user table is already balanced find the index table plans from the user table
+                // plans
+                // or vice verca.
+                TableName mappedTableName = getMappedTableToColocate(tableName);
+                if (balancedTables.contains(mappedTableName)) {
+                    balancedTables.remove(mappedTableName);
+                    regionPlans = new ArrayList<RegionPlan>();
+                    return prepareRegionPlansForClusterState(clusterState, regionPlans);
+                } else {
+                    balancedTables.add(tableName);
+                    regionPlans = this.delegator.balanceCluster(clusterState);
+                    if (regionPlans == null) {
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug(tableName + " regions already balanced.");
+                        }
+                        return null;
+                    } else {
+                        updateRegionPlans(regionPlans);
+                        return regionPlans;
+                    }
+                }
+
+            } else {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("Seperating user tables and index tables regions of "
+                            + "each region server in the cluster.");
+                }
+                Map<ServerName, List<HRegionInfo>> userClusterState =
+                        new HashMap<ServerName, List<HRegionInfo>>();
+                Map<ServerName, List<HRegionInfo>> indexClusterState =
+                        new HashMap<ServerName, List<HRegionInfo>>();
+                for (Entry<ServerName, List<HRegionInfo>> serverVsRegionList : clusterState
+                        .entrySet()) {
+                    ServerName sn = serverVsRegionList.getKey();
+                    List<HRegionInfo> regionsInfos = serverVsRegionList.getValue();
+                    List<HRegionInfo> idxRegionsToBeMoved = new ArrayList<HRegionInfo>();
+                    List<HRegionInfo> userRegionsToBeMoved = new ArrayList<HRegionInfo>();
+                    for (HRegionInfo hri : regionsInfos) {
+                        if (hri.isMetaRegion()) {
+                            continue;
+                        }
+                        tableName = hri.getTable();
+                        // Check and modify the colocation info map based on values of cluster state
+                        // because we
+                        // will
+                        // call balancer only when the cluster is in stable and reliable state.
+                        if (isTableColocated(tableName)) {
+                            // table name may change every time thats why always need to get table
+                            // entries.
+                            Map<ImmutableBytesWritable, ServerName> tableKeys =
+                                    this.colocationInfo.get(tableName);
+                            if (tableKeys != null) {
+                                updateServer(tableKeys, sn, hri);
+                            }
+                        }
+                        if (indexTableVsUserTable.containsKey(tableName)) {
+                            idxRegionsToBeMoved.add(hri);
+                            continue;
+                        }
+                        userRegionsToBeMoved.add(hri);
+                    }
+                    // there may be dummy entries here if assignments by table is set
+                    userClusterState.put(sn, userRegionsToBeMoved);
+                    indexClusterState.put(sn, idxRegionsToBeMoved);
+                }
+
+                regionPlans = this.delegator.balanceCluster(userClusterState);
+                if (regionPlans == null) {
+                    if (LOG.isDebugEnabled()) {
+                        LOG.debug("User region plan is null.");
+                    }
+                    regionPlans = new ArrayList<RegionPlan>();
+                } else {
+                    updateRegionPlans(regionPlans);
+                }
+                return prepareRegionPlansForClusterState(indexClusterState, regionPlans);
+            }
+        }
+    }
+
+    private void updateServer(Map<ImmutableBytesWritable, ServerName> tableKeys, ServerName sn,
+            HRegionInfo hri) {
+        ImmutableBytesWritable startKey = new ImmutableBytesWritable(hri.getStartKey());
+        ServerName existingServer = tableKeys.get(startKey);
+        if (!sn.equals(existingServer)) {
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("There is a mismatch in the existing server name for the region " + hri
+                        + ".  Replacing the server " + existingServer + " with " + sn + ".");
+            }
+            tableKeys.put(startKey, sn);
+        }
+    }
+
+    /**
+     * Prepare region plans for cluster state
+     * @param clusterState if balancing is table wise then cluster state contains only indexed or
+     *            index table regions, otherwise it contains all index tables regions.
+     * @param regionPlans
+     * @return
+     */
+    private List<RegionPlan> prepareRegionPlansForClusterState(
+            Map<ServerName, List<HRegionInfo>> clusterState, List<RegionPlan> regionPlans) {
+        if (regionPlans == null) regionPlans = new ArrayList<RegionPlan>();
+        ImmutableBytesWritable startKey = new ImmutableBytesWritable();
+        for (Entry<ServerName, List<HRegionInfo>> serverVsRegionList : clusterState.entrySet()) {
+            List<HRegionInfo> regionInfos = serverVsRegionList.getValue();
+            ServerName server = serverVsRegionList.getKey();
+            for (HRegionInfo regionInfo : regionInfos) {
+                if (!isTableColocated(regionInfo.getTable())) continue;
+                TableName mappedTableName = getMappedTableToColocate(regionInfo.getTable());
+                startKey.set(regionInfo.getStartKey());
+                ServerName sn = this.colocationInfo.get(mappedTableName).get(startKey);
+                if (sn.equals(server)) {
+                    continue;
+                } else {
+                    RegionPlan rp = new RegionPlan(regionInfo, server, sn);
+                    if (LOG.isDebugEnabled()) {
+                        LOG.debug("Selected server " + rp.getDestination()
+                                + " as destination for region "
+                                + regionInfo.getRegionNameAsString() + " from colocation info.");
+                    }
+                    regionOnline(regionInfo, rp.getDestination());
+                    regionPlans.add(rp);
+                }
+            }
+        }
+        return regionPlans;
+    }
+
+    private void updateRegionPlans(List<RegionPlan> regionPlans) {
+        for (RegionPlan regionPlan : regionPlans) {
+            HRegionInfo hri = regionPlan.getRegionInfo();
+            if (!isTableColocated(hri.getTable())) continue;
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("Saving region plan of region " + hri.getRegionNameAsString() + '.');
+            }
+            regionOnline(hri, regionPlan.getDestination());
+        }
+    }
+
+    @Override
+    public Map<ServerName, List<HRegionInfo>> roundRobinAssignment(List<HRegionInfo> regions,
+            List<ServerName> servers) throws HBaseIOException {
+        List<HRegionInfo> userRegions = new ArrayList<HRegionInfo>();
+        List<HRegionInfo> indexRegions = new ArrayList<HRegionInfo>();
+        for (HRegionInfo hri : regions) {
+            seperateUserAndIndexRegion(hri, userRegions, indexRegions);
+        }
+        Map<ServerName, List<HRegionInfo>> bulkPlan = null;
+        if (!userRegions.isEmpty()) {
+            bulkPlan = this.delegator.roundRobinAssignment(userRegions, servers);
+            // This should not happen.
+            if (null == bulkPlan) {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("No region plans selected for user regions in roundRobinAssignment.");
+                }
+                return null;
+            }
+            savePlan(bulkPlan);
+        }
+        bulkPlan = prepareIndexRegionsPlan(indexRegions, bulkPlan, servers);
+        return bulkPlan;
+    }
+
+    private void seperateUserAndIndexRegion(HRegionInfo hri, List<HRegionInfo> userRegions,
+            List<HRegionInfo> indexRegions) {
+        if (indexTableVsUserTable.containsKey(hri.getTable())) {
+            indexRegions.add(hri);
+            return;
+        }
+        userRegions.add(hri);
+    }
+
+    private Map<ServerName, List<HRegionInfo>> prepareIndexRegionsPlan(
+            List<HRegionInfo> indexRegions, Map<ServerName, List<HRegionInfo>> bulkPlan,
+            List<ServerName> servers) throws HBaseIOException {
+        if (null != indexRegions && !indexRegions.isEmpty()) {
+            if (null == bulkPlan) {
+                bulkPlan = new ConcurrentHashMap<ServerName, List<HRegionInfo>>();
+            }
+            for (HRegionInfo hri : indexRegions) {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("Preparing region plan for index region "
+                            + hri.getRegionNameAsString() + '.');
+                }
+                ServerName destServer = getDestServerForIdxRegion(hri);
+                List<HRegionInfo> destServerRegions = null;
+                if (destServer == null) destServer = this.randomAssignment(hri, servers);
+                if (destServer != null) {
+                    destServerRegions = bulkPlan.get(destServer);
+                    if (null == destServerRegions) {
+                        destServerRegions = new ArrayList<HRegionInfo>();
+                        bulkPlan.put(destServer, destServerRegions);
+                    }
+                    if (LOG.isDebugEnabled()) {
+                        LOG.debug("Server " + destServer + " selected for region "
+                                + hri.getRegionNameAsString() + '.');
+                    }
+                    destServerRegions.add(hri);
+                    regionOnline(hri, destServer);
+                }
+            }
+        }
+        return bulkPlan;
+    }
+
+    private ServerName getDestServerForIdxRegion(HRegionInfo hri) {
+        // Every time we calculate the table name because in case of master restart the index
+        // regions
+        // may be coming for different index tables.
+        TableName actualTable = getMappedTableToColocate(hri.getTable());
+        ImmutableBytesWritable startkey = new ImmutableBytesWritable(hri.getStartKey());
+        synchronized (this.colocationInfo) {
+
+            Map<ImmutableBytesWritable, ServerName> tableKeys = colocationInfo.get(actualTable);
+            if (null == tableKeys) {
+                // Can this case come
+                return null;
+            }
+            if (tableKeys.containsKey(startkey)) {
+                // put index region location if corresponding user region found in regionLocation
+                // map.
+                ServerName sn = tableKeys.get(startkey);
+                regionOnline(hri, sn);
+                return sn;
+            }
+        }
+        return null;
+    }
+
+    private void savePlan(Map<ServerName, List<HRegionInfo>> bulkPlan) {
+        synchronized (this.colocationInfo) {
+            for (Entry<ServerName, List<HRegionInfo>> e : bulkPlan.entrySet()) {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("Saving user regions' plans for server " + e.getKey() + '.');
+                }
+                for (HRegionInfo hri : e.getValue()) {
+                    if (!isTableColocated(hri.getTable())) continue;
+                    regionOnline(hri, e.getKey());
+                }
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("Saved user regions' plans for server " + e.getKey() + '.');
+                }
+            }
+        }
+    }
+
+    @Override
+    public Map<ServerName, List<HRegionInfo>> retainAssignment(
+            Map<HRegionInfo, ServerName> regions, List<ServerName> servers) throws HBaseIOException {
+        Map<HRegionInfo, ServerName> userRegionsMap =
+                new ConcurrentHashMap<HRegionInfo, ServerName>();
+        List<HRegionInfo> indexRegions = new ArrayList<HRegionInfo>();
+        for (Entry<HRegionInfo, ServerName> e : regions.entrySet()) {
+            seperateUserAndIndexRegion(e, userRegionsMap, indexRegions, servers);
+        }
+        Map<ServerName, List<HRegionInfo>> bulkPlan = null;
+        if (!userRegionsMap.isEmpty()) {
+            bulkPlan = this.delegator.retainAssignment(userRegionsMap, servers);
+            if (bulkPlan == null) {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("Empty region plan for user regions.");
+                }
+                return null;
+            }
+            savePlan(bulkPlan);
+        }
+        bulkPlan = prepareIndexRegionsPlan(indexRegions, bulkPlan, servers);
+        return bulkPlan;
+    }
+
+    private void seperateUserAndIndexRegion(Entry<HRegionInfo, ServerName> e,
+            Map<HRegionInfo, ServerName> userRegionsMap, List<HRegionInfo> indexRegions,
+            List<ServerName> servers) {
+        HRegionInfo hri = e.getKey();
+        if (indexTableVsUserTable.containsKey(hri.getTable())) {
+            indexRegions.add(hri);
+            return;
+        }
+        if (e.getValue() == null) {
+            userRegionsMap.put(hri, servers.get(RANDOM.nextInt(servers.size())));
+        } else {
+            userRegionsMap.put(hri, e.getValue());
+        }
+    }
+
+    @Override
+    public Map<HRegionInfo, ServerName> immediateAssignment(List<HRegionInfo> regions,
+            List<ServerName> servers) throws HBaseIOException {
+        return this.delegator.immediateAssignment(regions, servers);
+    }
+
+    @Override
+    public ServerName randomAssignment(HRegionInfo regionInfo, List<ServerName> servers)
+            throws HBaseIOException {
+        if (!isTableColocated(regionInfo.getTable())) {
+            return this.delegator.randomAssignment(regionInfo, servers);
+        }
+        ServerName sn = getServerNameFromMap(regionInfo, servers);
+        if (sn == null) {
+            if (LOG.isDebugEnabled()) {
+                LOG.debug("No server found for region " + regionInfo.getRegionNameAsString() + '.');
+            }
+            sn = getRandomServer(regionInfo, servers);
+        }
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("Destination server for region " + regionInfo.getRegionNameAsString()
+                    + " is " + ((sn == null) ? "null" : sn.toString()) + '.');
+        }
+        return sn;
+    }
+
+    private ServerName getRandomServer(HRegionInfo regionInfo, List<ServerName> servers)
+            throws HBaseIOException {
+        ServerName sn = null;
+        sn = this.delegator.randomAssignment(regionInfo, servers);
+        if (sn == null) return null;
+        regionOnline(regionInfo, sn);
+        return sn;
+    }
+
+    private ServerName getServerNameFromMap(HRegionInfo regionInfo, List<ServerName> onlineServers) {
+        TableName tableName = regionInfo.getTable();
+        TableName mappedTable = getMappedTableToColocate(regionInfo.getTable());
+        ImmutableBytesWritable startKey = new ImmutableBytesWritable(regionInfo.getStartKey());
+        synchronized (this.colocationInfo) {
+            Map<ImmutableBytesWritable, ServerName> correspondingTableKeys =
+                    this.colocationInfo.get(mappedTable);
+            Map<ImmutableBytesWritable, ServerName> actualTableKeys =
+                    this.colocationInfo.get(tableName);
+
+            if (null != correspondingTableKeys) {
+                if (correspondingTableKeys.containsKey(startKey)) {
+                    ServerName previousServer = null;
+                    if (null != actualTableKeys) {
+                        previousServer = actualTableKeys.get(startKey);
+                    }
+                    ServerName sn = correspondingTableKeys.get(startKey);
+                    if (null != previousServer) {
+                        // if servername of index region and user region are same in colocationInfo
+                        // clean
+                        // previous plans and return null
+                        if (previousServer.equals(sn)) {
+                            correspondingTableKeys.remove(startKey);
+                            actualTableKeys.remove(startKey);
+                            if (LOG.isDebugEnabled()) {
+                                LOG
+                                        .debug("Both user region plan and corresponding index region plan "
+                                                + "in colocation info are same. Hence clearing the plans to select new plan"
+                                                + " for the region "
+                                                + regionInfo.getRegionNameAsString() + ".");
+                            }
+                            return null;
+                        }
+                    }
+                    if (sn != null && onlineServers.contains(sn)) {
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug("Updating the region plan of the region "
+                                    + regionInfo.getRegionNameAsString() + " with server " + sn);
+                        }
+                        regionOnline(regionInfo, sn);
+                        return sn;
+                    } else if (sn != null) {
+                        if (LOG.isDebugEnabled()) {
+                            LOG.debug("The location " + sn + " of region with start key"
+                                    + Bytes.toStringBinary(regionInfo.getStartKey())
+                                    + " is not in online. Selecting other region server.");
+                        }
+                        return null;
+                    }
+                }
+            } else {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("No region plans in colocationInfo for table " + mappedTable);
+                }
+            }
+            return null;
+        }
+    }
+
+    @Override
+    public void regionOnline(HRegionInfo regionInfo, ServerName sn) {
+        TableName tableName = regionInfo.getTable();
+        synchronized (this.colocationInfo) {
+            Map<ImmutableBytesWritable, ServerName> tabkeKeys = this.colocationInfo.get(tableName);
+            if (tabkeKeys == null) {
+                tabkeKeys = new ConcurrentHashMap<ImmutableBytesWritable, ServerName>();
+                this.colocationInfo.put(tableName, tabkeKeys);
+            }
+            tabkeKeys.put(new ImmutableBytesWritable(regionInfo.getStartKey()), sn);
+        }
+    }
+
+    public void clearTableRegionPlans(TableName tableName) {
+        if (LOG.isDebugEnabled()) {
+            LOG.debug("Clearing regions plans from colocationInfo for table " + tableName);
+        }
+        synchronized (this.colocationInfo) {
+            this.colocationInfo.remove(tableName);
+        }
+    }
+
+    @Override
+    public void regionOffline(HRegionInfo regionInfo) {
+        TableName tableName = regionInfo.getTable();
+        synchronized (this.colocationInfo) {
+            Map<ImmutableBytesWritable, ServerName> tableKeys = this.colocationInfo.get(tableName);
+            if (null == tableKeys) {
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("No regions of table " + tableName + " in the colocationInfo.");
+                }
+            } else {
+                tableKeys.remove(new ImmutableBytesWritable(regionInfo.getStartKey()));
+                if (LOG.isDebugEnabled()) {
+                    LOG.debug("The regioninfo " + regionInfo + " removed from the colocationInfo");
+                }
+            }
+        }
+    }
+
+    @Override
+    public boolean isStopped() {
+        return stopped;
+    }
+
+    @Override
+    public void stop(String why) {
+        LOG.info("Load Balancer stop requested: " + why);
+        stopped = true;
+    }
+
+    public void populateTablesToColocate(Map<String, HTableDescriptor> tableDescriptors) {
+        HTableDescriptor desc = null;
+        for (Entry<String, HTableDescriptor> entry : tableDescriptors.entrySet()) {
+            desc = entry.getValue();
+            if (desc.getValue(PARENT_TABLE_KEY) != null) {
+                addTablesToColocate(TableName.valueOf(desc.getValue(PARENT_TABLE_KEY)), desc
+                        .getTableName());
+            }
+        }
+    }
+
+    /**
+     * Add tables whose regions to co-locate.
+     * @param userTable
+     * @param indexTable
+     */
+    public void addTablesToColocate(TableName userTable, TableName indexTable) {
+        if (userTable.equals(indexTable)) {
+            throw new IllegalArgumentException("Tables to colocate should not be same.");
+        } else if (isTableColocated(userTable)) {
+            throw new IllegalArgumentException("User table already colocated with table "
+                    + getMappedTableToColocate(userTable));
+        } else if (isTableColocated(indexTable)) {
+            throw new IllegalArgumentException("Index table is already colocated with table "
+                    + getMappedTableToColocate(indexTable));
+        }
+        userTableVsIndexTable.put(userTable, indexTable);
+        indexTableVsUserTable.put(indexTable, userTable);
+    }
+
+    /**
+     * Removes the specified table and corresponding table from co-location.
+     * @param table
+     */
+    public void removeTablesFromColocation(TableName table) {
+        TableName other = userTableVsIndexTable.remove(table);
+        if (other != null) {
+            indexTableVsUserTable.remove(other);
+        } else {
+            other = indexTableVsUserTable.remove(table);
+            if (other != null) userTableVsIndexTable.remove(table);
+        }
+    }
+
+    /**
+     * Return mapped table to co-locate.
+     * @param tableName
+     * @return index table if the specified table is user table or vice versa.
+     */
+    public TableName getMappedTableToColocate(TableName tableName) {
+        TableName other = userTableVsIndexTable.get(tableName);
+        return other == null ? indexTableVsUserTable.get(tableName) : other;
+    }
+
+    public boolean isTableColocated(TableName table) {
+        return userTableVsIndexTable.containsKey(table) || indexTableVsUserTable.containsKey(table);
+    }
+
+    /**
+     * Populates table's region locations into co-location info from master.
+     * @param table
+     */
+    public void populateRegionLocations(TableName table) {
+        synchronized (this.colocationInfo) {
+            if (!isTableColocated(table)) {
+                throw new IllegalArgumentException("Specified table " + table
+                        + " should be in one of the tables to co-locate.");
+            }
+            RegionStates regionStates = this.master.getAssignmentManager().getRegionStates();
+            List<HRegionInfo> onlineRegions = regionStates.getRegionsOfTable(table);
+            for (HRegionInfo hri : onlineRegions) {
+                regionOnline(hri, regionStates.getRegionServerOfRegion(hri));
+            }
+            Map<String, RegionState> regionsInTransition = regionStates.getRegionsInTransition();
+            for (RegionState regionState : regionsInTransition.values()) {
+                if (table.equals(regionState.getRegion().getTable())
+                        && regionState.getServerName() != null) {
+                    regionOnline(regionState.getRegion(), regionState.getServerName());
+                }
+            }
+        }
+    }
+}
-- 
1.9.4.msysgit.0


From d4cfc5d9bb758895b607838b33a78211bb7cb165 Mon Sep 17 00:00:00 2001
From: James Taylor <jamestaylor@apache.org>
Date: Fri, 25 Apr 2014 21:36:03 -0700
Subject: [PATCH 03/14] PHOENIX-935 create local index table with the same
 split keys of user table (Rajeshbabu)

---
 .../apache/phoenix/end2end/index/LocalIndexIT.java | 62 ++++++++++++++++++-
 .../index/balancer/TestIndexLoadBalancer.java      | 43 +-------------
 .../hbase/index/IndexRegionSplitPolicy.java        | 32 ++++++++++
 .../hbase/index/master/IndexMasterObserver.java    | 69 ++++++++++++++++++++++
 .../phoenix/query/ConnectionQueryServicesImpl.java | 47 ++++++++++++++-
 .../org/apache/phoenix/schema/MetaDataClient.java  | 35 +++++++++--
 .../java/org/apache/phoenix/util/MetaDataUtil.java | 29 +++++++++
 7 files changed, 267 insertions(+), 50 deletions(-)
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/hbase/index/IndexRegionSplitPolicy.java
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/hbase/index/master/IndexMasterObserver.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index 4589259..4c3e8ab 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -19,33 +19,43 @@ package org.apache.phoenix.end2end.index;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.fail;
 
 import java.sql.Connection;
 import java.sql.DriverManager;
 import java.sql.SQLException;
 
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.hadoop.hbase.client.HTable;
+import org.apache.phoenix.hbase.index.IndexRegionSplitPolicy;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableKey;
+import org.apache.phoenix.schema.TableNotFoundException;
+import org.apache.phoenix.util.MetaDataUtil;
+import org.apache.phoenix.util.TestUtil;
 import org.junit.Test;
 
 public class LocalIndexIT extends BaseIndexIT {
-    private void createBaseTable(String tableName, Integer saltBuckets) throws SQLException {
+    private void createBaseTable(String tableName, Integer saltBuckets, String splits) throws SQLException {
         Connection conn = DriverManager.getConnection(getUrl());
         String ddl = "CREATE TABLE " + tableName + " (t_id VARCHAR NOT NULL,\n" +
                 "k1 INTEGER NOT NULL,\n" +
                 "k2 INTEGER NOT NULL,\n" +
                 "v1 VARCHAR,\n" +
                 "CONSTRAINT pk PRIMARY KEY (t_id, k1, k2))\n"
-                + (saltBuckets == null ? "" : (",salt_buckets="+saltBuckets));
+                        + (saltBuckets == null || splits != null ? "" : (",salt_buckets=" + saltBuckets)
+                        + (saltBuckets != null || splits == null ? "" : ",splits=" + splits));
         conn.createStatement().execute(ddl);
         conn.close();
     }
     
     @Test
     public void testLocalIndexRoundTrip() throws Exception {
-        createBaseTable(DATA_TABLE_NAME, null);
+        createBaseTable(DATA_TABLE_NAME, null, null);
         Connection conn1 = DriverManager.getConnection(getUrl());
         Connection conn2 = DriverManager.getConnection(getUrl());
         conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
@@ -54,5 +64,51 @@ public class LocalIndexIT extends BaseIndexIT {
         assertEquals(IndexType.LOCAL, localIndex.getIndexType());
         assertNotNull(localIndex.getViewIndexId());
     }
+    
+    @Test
+    public void testLocalIndexCreationWithSplitsShouldFail() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, null);
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        try {
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)"+" splits={1,2,3}");
+            fail("Local index cannot be pre-split");
+        } catch (SQLException e) { }
+        try {
+            conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+            conn2.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
+            fail("Local index should be created.");
+        } catch (TableNotFoundException e) { }
+    }
+
+    @Test
+    public void testLocalIndexCreationWithSaltingShouldFail() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, null);
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        try {
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)"+" salt_buckets=16");
+            fail("Local index cannot be salted.");
+        } catch (SQLException e) { }
+        try {
+            conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+            conn2.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
+            fail("Local index should not be created.");
+        } catch (TableNotFoundException e) { }
+    }
 
+    @Test
+    public void testLocalIndexTableRegionSplitPolicyAndSplitKeys() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null,"{1,2,3}");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+        conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+        HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        HTableDescriptor htd = admin.getTableDescriptor(TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)));
+        assertEquals(IndexRegionSplitPolicy.class.getName(), htd.getValue(HTableDescriptor.SPLIT_POLICY));
+        HTable userTable = new HTable(admin.getConfiguration(),TableName.valueOf(DATA_TABLE_NAME));
+        HTable indexTable = new HTable(admin.getConfiguration(),TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)));
+        assertEquals("Both user region and index table should have same split keys.", userTable.getStartKeys(), indexTable.getStartKeys());
+    }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java b/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
index fb0afa9..7a5d61c 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
@@ -39,10 +39,7 @@ import org.apache.hadoop.hbase.catalog.MetaReader;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.client.Put;
-import org.apache.hadoop.hbase.coprocessor.BaseMasterObserver;
 import org.apache.hadoop.hbase.coprocessor.CoprocessorHost;
-import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
-import org.apache.hadoop.hbase.coprocessor.ObserverContext;
 import org.apache.hadoop.hbase.master.HMaster;
 import org.apache.hadoop.hbase.master.LoadBalancer;
 import org.apache.hadoop.hbase.master.RegionStates;
@@ -55,6 +52,7 @@ import org.apache.hadoop.hbase.zookeeper.ZKAssign;
 import org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher;
 import org.apache.phoenix.hbase.index.IndexTestingUtils;
 import org.apache.phoenix.hbase.index.Indexer;
+import org.apache.phoenix.hbase.index.master.IndexMasterObserver;
 import org.apache.phoenix.util.ConfigUtil;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
@@ -75,7 +73,7 @@ public class TestIndexLoadBalancer {
         final int NUM_RS = 4;
         Configuration conf = UTIL.getConfiguration();
         conf.setBoolean(HConstants.REGIONSERVER_INFO_PORT_AUTO, true);
-        conf.set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY, MockedMasterObserver.class.getName());
+        conf.set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY, IndexMasterObserver.class.getName());
         conf.setClass(HConstants.HBASE_MASTER_LOADBALANCER_CLASS, IndexLoadBalancer.class,
             LoadBalancer.class);
         IndexTestingUtils.setupConfig(conf);
@@ -491,41 +489,4 @@ public class TestIndexLoadBalancer {
         }
         return regionsColocated;
     }
-
-    public static class MockedMasterObserver extends BaseMasterObserver {
-        IndexLoadBalancer balancer = null;
-
-        @Override
-        public void preMasterInitialization(ObserverContext<MasterCoprocessorEnvironment> ctx)
-                throws IOException {
-            LoadBalancer loadBalancer =
-                    ctx.getEnvironment().getMasterServices().getAssignmentManager().getBalancer();
-            if (loadBalancer instanceof IndexLoadBalancer) {
-                balancer = (IndexLoadBalancer) loadBalancer;
-            }
-            super.preMasterInitialization(ctx);
-        }
-
-        @Override
-        public void preCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
-                HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
-            TableName userTableName = null;
-            if (balancer != null && desc.getValue(IndexLoadBalancer.PARENT_TABLE_KEY) != null) {
-                userTableName =
-                        TableName.valueOf(desc.getValue(IndexLoadBalancer.PARENT_TABLE_KEY));
-                balancer.addTablesToColocate(userTableName, desc.getTableName());
-            }
-            if (userTableName != null) balancer.populateRegionLocations(userTableName);
-            super.preCreateTableHandler(ctx, desc, regions);
-        }
-
-        @Override
-        public void postDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
-                TableName tableName) throws IOException {
-            if (balancer.isTableColocated(tableName)) {
-                balancer.removeTablesFromColocation(tableName);
-            }
-        }
-    }
-
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/IndexRegionSplitPolicy.java b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/IndexRegionSplitPolicy.java
new file mode 100644
index 0000000..0ee27e2
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/IndexRegionSplitPolicy.java
@@ -0,0 +1,32 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.hbase.index;
+
+import org.apache.hadoop.hbase.regionserver.RegionSplitPolicy;
+
+/**
+ * Split policy for index regions to avoid split from external requests.
+ */
+public class IndexRegionSplitPolicy extends RegionSplitPolicy {
+
+    @Override
+    protected boolean shouldSplit() {
+        return false;
+    }
+
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/master/IndexMasterObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/master/IndexMasterObserver.java
new file mode 100644
index 0000000..dfb2c62
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/master/IndexMasterObserver.java
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.hbase.index.master;
+
+import java.io.IOException;
+
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.coprocessor.BaseMasterObserver;
+import org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment;
+import org.apache.hadoop.hbase.coprocessor.ObserverContext;
+import org.apache.hadoop.hbase.master.LoadBalancer;
+import org.apache.phoenix.hbase.index.balancer.IndexLoadBalancer;
+
+/**
+ * Defines of coprocessor hooks(to support secondary indexing) of operations on
+ * {@link org.apache.hadoop.hbase.master.HMaster} process.
+ */
+public class IndexMasterObserver extends BaseMasterObserver {
+    IndexLoadBalancer balancer = null;
+
+    @Override
+    public void preMasterInitialization(ObserverContext<MasterCoprocessorEnvironment> ctx)
+            throws IOException {
+        LoadBalancer loadBalancer =
+                ctx.getEnvironment().getMasterServices().getAssignmentManager().getBalancer();
+        if (loadBalancer instanceof IndexLoadBalancer) {
+            balancer = (IndexLoadBalancer) loadBalancer;
+        }
+        super.preMasterInitialization(ctx);
+    }
+
+    @Override
+    public void preCreateTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+            HTableDescriptor desc, HRegionInfo[] regions) throws IOException {
+        TableName userTableName = null;
+        if (balancer != null && desc.getValue(IndexLoadBalancer.PARENT_TABLE_KEY) != null) {
+            userTableName =
+                    TableName.valueOf(desc.getValue(IndexLoadBalancer.PARENT_TABLE_KEY));
+            balancer.addTablesToColocate(userTableName, desc.getTableName());
+        }
+        if (userTableName != null) balancer.populateRegionLocations(userTableName);
+        super.preCreateTableHandler(ctx, desc, regions);
+    }
+
+    @Override
+    public void postDeleteTableHandler(ObserverContext<MasterCoprocessorEnvironment> ctx,
+            TableName tableName) throws IOException {
+        if (balancer != null && balancer.isTableColocated(tableName)) {
+            balancer.removeTablesFromColocation(tableName);
+        }
+    }
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
index ea455d3..c0c6940 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
@@ -123,6 +123,7 @@ import org.apache.phoenix.exception.PhoenixIOException;
 import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.exception.SQLExceptionInfo;
 import org.apache.phoenix.execute.MutationState;
+import org.apache.phoenix.hbase.index.IndexRegionSplitPolicy;
 import org.apache.phoenix.hbase.index.Indexer;
 import org.apache.phoenix.hbase.index.covered.CoveredColumnsIndexBuilder;
 import org.apache.phoenix.hbase.index.util.KeyValueBuilder;
@@ -803,6 +804,9 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
                 if (isMetaTable) {
                     newDesc.remove(HTableDescriptor.SPLIT_POLICY);
                 }
+                if (newDesc.getValue(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_BYTES) != null && Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(newDesc.getValue(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_BYTES)))) {
+                    newDesc.setValue(HTableDescriptor.SPLIT_POLICY, IndexRegionSplitPolicy.class.getName());
+                }
                 try {
                     if (splits == null) {
                         admin.createTable(newDesc);
@@ -1019,6 +1023,40 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
         }
     }
 
+    private void ensureLocalIndexTableCreated(byte[] physicalTableName, Map<String,Object> tableProps, List<Pair<byte[],Map<String,Object>>> families, byte[][] splits, long timestamp) throws SQLException {
+        PTable table;
+        String parentTableName = Bytes.toString(physicalTableName, MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX_BYTES.length, 
+            physicalTableName.length - MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX_BYTES.length);
+        try {
+            table = latestMetaData.getTable(new PTableKey(PName.EMPTY_NAME, parentTableName));
+            if (table.getTimeStamp() >= timestamp) { // Table in cache is newer than client timestamp which shouldn't be the case
+                throw new TableNotFoundException(table.getSchemaName().getString(), table.getTableName().getString());
+            }
+        } catch (TableNotFoundException e) {
+            byte[] schemaName = Bytes.toBytes(SchemaUtil.getSchemaNameFromFullName(parentTableName));
+            byte[] tableName = Bytes.toBytes(SchemaUtil.getTableNameFromFullName(parentTableName));
+            MetaDataMutationResult result = this.getTable(null, schemaName, tableName, HConstants.LATEST_TIMESTAMP, timestamp);
+            table = result.getTable();
+            if (table == null) {
+                throw e;
+            }
+        }
+        ensureLocalIndexTableCreated(physicalTableName, tableProps, families, splits);
+    }
+
+    private void ensureLocalIndexTableCreated(byte[] physicalTableName, Map<String, Object> tableProps, List<Pair<byte[], Map<String, Object>>> families, byte[][] splits) throws SQLException, TableAlreadyExistsException {
+        tableProps.put(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_NAME, TRUE_BYTES_AS_STRING);
+        HTableDescriptor desc = ensureTableCreated(physicalTableName, PTableType.TABLE, tableProps, families, splits, false);
+        if (desc != null) {
+            if (!Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(desc.getValue(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_BYTES)))) {
+                String fullTableName = Bytes.toString(physicalTableName);
+                throw new TableAlreadyExistsException(
+                        "Unable to create shared physical table for local indexes.",
+                        SchemaUtil.getSchemaNameFromFullName(fullTableName),
+                        SchemaUtil.getTableNameFromFullName(fullTableName));
+            }
+        }
+    }
 
     private boolean ensureViewIndexTableDropped(byte[] physicalTableName, long timestamp) throws SQLException {
         byte[] physicalIndexName = MetaDataUtil.getViewIndexPhysicalName(physicalTableName);
@@ -1064,6 +1102,8 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
         byte[] schemaBytes = rowKeyMetadata[PhoenixDatabaseMetaData.SCHEMA_NAME_INDEX];
         byte[] tableBytes = rowKeyMetadata[PhoenixDatabaseMetaData.TABLE_NAME_INDEX];
         byte[] tableName = physicalTableName != null ? physicalTableName : SchemaUtil.getTableNameAsBytes(schemaBytes, tableBytes);
+        boolean localIndexTable = Boolean.TRUE.equals(tableProps.remove(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_NAME));
+
         if ((tableType == PTableType.VIEW && physicalTableName != null) || (tableType != PTableType.VIEW && physicalTableName == null)) {
             // For views this will ensure that metadata already exists
             // For tables and indexes, this will create the metadata if it doesn't already exist
@@ -1074,7 +1114,11 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
             // Physical index table created up front for multi tenant
             // TODO: if viewIndexId is Short.MIN_VALUE, then we don't need to attempt to create it
             if (physicalTableName != null && !MetaDataUtil.isMultiTenant(m, kvBuilder, ptr)) {
-                ensureViewIndexTableCreated(tenantIdBytes.length == 0 ? null : PNameFactory.newName(tenantIdBytes), physicalTableName, MetaDataUtil.getClientTimeStamp(m));
+                if (localIndexTable) {
+                    ensureLocalIndexTableCreated(tableName, tableProps, families, splits, MetaDataUtil.getClientTimeStamp(m));
+                } else {
+                    ensureViewIndexTableCreated(tenantIdBytes.length == 0 ? null : PNameFactory.newName(tenantIdBytes), physicalTableName, MetaDataUtil.getClientTimeStamp(m));
+                }
             }
         } else if (tableType == PTableType.TABLE && MetaDataUtil.isMultiTenant(m, kvBuilder, ptr)) { // Create view index table up front for multi tenant tables
             ptr.set(QueryConstants.DEFAULT_COLUMN_FAMILY_BYTES);
@@ -1096,6 +1140,7 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
                 familiesPlusDefault.add(new Pair<byte[],Map<String,Object>>(defaultCF,Collections.<String,Object>emptyMap()));
             }
             ensureViewIndexTableCreated(tableName, tableProps, familiesPlusDefault, MetaDataUtil.isSalted(m, kvBuilder, ptr) ? splits : null, MetaDataUtil.getClientTimeStamp(m));
+            ensureLocalIndexTableCreated(MetaDataUtil.getLocalIndexPhysicalName(tableName), tableProps, families, splits);
         }
         
         byte[] tableKey = SchemaUtil.getTableKey(tenantIdBytes, schemaBytes, tableBytes);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index c888007..266118c 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -77,6 +77,7 @@ import java.util.Set;
 
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Mutation;
@@ -764,8 +765,13 @@ public class MetaDataClient {
                     saltBucketNum = parent.getBucketNum();
                     addSaltColumn = (saltBucketNum != null && indexType != IndexType.LOCAL);
                     defaultFamilyName = parent.getDefaultFamilyName() == null ? null : parent.getDefaultFamilyName().getString();
-                    // Set physical name of view index table
-                    physicalNames = Collections.singletonList(PNameFactory.newName(MetaDataUtil.getViewIndexPhysicalName(physicalName.getBytes())));
+                    if (indexType == IndexType.LOCAL) {
+                        // Set physical name of local index table
+                        physicalNames = Collections.singletonList(PNameFactory.newName(MetaDataUtil.getLocalIndexPhysicalName(physicalName.getBytes())));
+                    } else {
+                        // Set physical name of view index table
+                        physicalNames = Collections.singletonList(PNameFactory.newName(MetaDataUtil.getViewIndexPhysicalName(physicalName.getBytes())));
+                    }
                 }
                 
                 multiTenant = parent.isMultiTenant();
@@ -1184,8 +1190,14 @@ public class MetaDataClient {
              */
             Collections.reverse(tableMetaData);
             
-            splits = SchemaUtil.processSplits(splits, pkColumns, saltBucketNum, connection.getQueryServices().getProps().getBoolean(
+            if (parent != null && tableType == PTableType.INDEX && indexType == IndexType.LOCAL) {
+                tableProps.put(MetaDataUtil.PARENT_TABLE_KEY, parent.getPhysicalName().getString());
+                tableProps.put(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_NAME, Boolean.TRUE);
+                splits = getSplitKeys(connection.getQueryServices().getAllTableRegions(parent.getPhysicalName().getBytes()));
+            } else {
+                splits = SchemaUtil.processSplits(splits, pkColumns, saltBucketNum, connection.getQueryServices().getProps().getBoolean(
                     QueryServices.ROW_KEY_ORDER_SALTED_TABLE_ATTRIB, QueryServicesOptions.DEFAULT_ROW_KEY_ORDER_SALTED_TABLE));
+            }
             MetaDataMutationResult result = connection.getQueryServices().createTable(
                     tableMetaData, 
                     viewType == ViewType.MAPPED || indexId != null ? physicalNames.get(0).getBytes() : null,
@@ -1221,7 +1233,20 @@ public class MetaDataClient {
             connection.setAutoCommit(wasAutoCommit);
         }
     }
-    
+
+    private byte[][] getSplitKeys(List<HRegionLocation> allTableRegions) {
+        if(allTableRegions.size() == 1) return null;
+        byte[][] splitKeys = new byte[allTableRegions.size()-1][];
+        int i = 0;
+        for (HRegionLocation region : allTableRegions) {
+            if (region.getRegionInfo().getStartKey().length != 0) {
+                splitKeys[i] = region.getRegionInfo().getStartKey();
+                i++;
+            }
+        }
+        return splitKeys;
+    }
+
     private static boolean hasColumnWithSameNameAndFamily(Collection<PColumn> columns, PColumn column) {
         for (PColumn currColumn : columns) {
            if (Objects.equal(currColumn.getFamilyName(), column.getFamilyName()) &&
@@ -1331,7 +1356,7 @@ public class MetaDataClient {
                         // PName name, PTableType type, long timeStamp, long sequenceNumber, List<PColumn> columns
                         List<TableRef> tableRefs = Lists.newArrayListWithExpectedSize(2 + table.getIndexes().size());
                         // All multi-tenant tables have a view index table, so no need to check in that case
-                        if (tableType == PTableType.TABLE && (table.isMultiTenant() || MetaDataUtil.hasViewIndexTable(connection, table.getPhysicalName()))) {
+                        if (tableType == PTableType.TABLE && (table.isMultiTenant() || MetaDataUtil.hasViewIndexTable(connection, table.getPhysicalName()) || MetaDataUtil.hasLocalIndexTable(connection, table.getPhysicalName()))) {
                             MetaDataUtil.deleteViewIndexSequences(connection, table.getPhysicalName());
                             // TODO: consider removing this, as the DROP INDEX done for each DROP VIEW command
                             // would have deleted all the rows already
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
index 1af644f..8a3a190 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
@@ -48,9 +48,13 @@ import org.apache.phoenix.schema.TableNotFoundException;
 public class MetaDataUtil {
     public static final String VIEW_INDEX_TABLE_PREFIX = "_IDX_";
     public static final byte[] VIEW_INDEX_TABLE_PREFIX_BYTES = Bytes.toBytes(VIEW_INDEX_TABLE_PREFIX);
+    public static final String LOCAL_INDEX_TABLE_PREFIX = "_LOCAL_IDX_";
+    public static final byte[] LOCAL_INDEX_TABLE_PREFIX_BYTES = Bytes.toBytes(LOCAL_INDEX_TABLE_PREFIX);
     public static final String VIEW_INDEX_SEQUENCE_PREFIX = "_SEQ_";
     public static final byte[] VIEW_INDEX_SEQUENCE_PREFIX_BYTES = Bytes.toBytes(VIEW_INDEX_SEQUENCE_PREFIX);
     public static final String VIEW_INDEX_ID_COLUMN_NAME = "_INDEX_ID";
+    public static final String PARENT_TABLE_KEY = "PARENT_TABLE";
+    public static final byte[] PARENT_TABLE_KEY_BYTES = Bytes.toBytes("PARENT_TABLE");
     
     public static boolean areClientAndServerCompatible(long version) {
         // As of 3.0, we allow a client and server to differ for the minor version.
@@ -246,6 +250,18 @@ public class MetaDataUtil {
         return schemaName;
     }
 
+    public static byte[] getLocalIndexPhysicalName(byte[] physicalTableName) {
+        return ByteUtil.concat(LOCAL_INDEX_TABLE_PREFIX_BYTES, physicalTableName);
+    }
+    
+    public static String getLocalIndexTableName(String tableName) {
+        return LOCAL_INDEX_TABLE_PREFIX + tableName;
+    }
+    
+    public static String getLocalIndexSchemaName(String schemaName) {
+        return schemaName;
+    }  
+
     public static SequenceKey getViewIndexSequenceKey(String tenantId, PName physicalName) {
         // Create global sequence of the form: <prefixed base table name><tenant id>
         // rather than tenant-specific sequence, as it makes it much easier
@@ -274,6 +290,16 @@ public class MetaDataUtil {
         }
     }
     
+    public static boolean hasLocalIndexTable(PhoenixConnection connection, PName name) throws SQLException {
+        byte[] physicalIndexName = MetaDataUtil.getLocalIndexPhysicalName(name.getBytes());
+        try {
+            HTableDescriptor desc = connection.getQueryServices().getTableDescriptor(physicalIndexName);
+            return desc != null && Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(desc.getValue(IS_LOCAL_INDEX_TABLE_PROP_BYTES)));
+        } catch (TableNotFoundException e) {
+            return false;
+        }
+    }
+
     public static void deleteViewIndexSequences(PhoenixConnection connection, PName name) throws SQLException {
         SequenceKey key = getViewIndexSequenceKey(null, name);
         connection.createStatement().executeUpdate("DELETE FROM " + PhoenixDatabaseMetaData.SEQUENCE_TABLE_NAME + 
@@ -284,4 +310,7 @@ public class MetaDataUtil {
 
     public static final String IS_VIEW_INDEX_TABLE_PROP_NAME = "IS_VIEW_INDEX_TABLE";
     public static final byte[] IS_VIEW_INDEX_TABLE_PROP_BYTES = Bytes.toBytes(IS_VIEW_INDEX_TABLE_PROP_NAME);
+
+    public static final String IS_LOCAL_INDEX_TABLE_PROP_NAME = "IS_LOCAL_INDEX_TABLE";
+    public static final byte[] IS_LOCAL_INDEX_TABLE_PROP_BYTES = Bytes.toBytes(IS_LOCAL_INDEX_TABLE_PROP_NAME);
 }
-- 
1.9.4.msysgit.0


From 78a6fe59332d6e765f211b752427a17f22537bf8 Mon Sep 17 00:00:00 2001
From: James Taylor <jamestaylor@apache.org>
Date: Sun, 27 Apr 2014 22:26:27 -0700
Subject: [PATCH 04/14] PHOENIX-935 create local index table with the same
 split keys of user table (Rajeshbabu)

---
 .../apache/phoenix/end2end/index/LocalIndexIT.java | 39 ++++++++++
 .../apache/phoenix/end2end/index/ViewIndexIT.java  | 85 ++++++++++++++++++++++
 .../phoenix/compile/CreateIndexCompiler.java       | 20 ++++-
 .../phoenix/query/ConnectionQueryServicesImpl.java | 36 ++++++++-
 .../org/apache/phoenix/schema/MetaDataClient.java  | 15 ++--
 .../java/org/apache/phoenix/util/MetaDataUtil.java | 22 +++++-
 6 files changed, 205 insertions(+), 12 deletions(-)
 create mode 100644 phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index 4c3e8ab..59bbc7f 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -18,12 +18,16 @@
 package org.apache.phoenix.end2end.index;
 
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import java.sql.Connection;
 import java.sql.DriverManager;
+import java.sql.ResultSet;
 import java.sql.SQLException;
+import java.util.Map;
 
 import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableName;
@@ -31,15 +35,31 @@ import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.phoenix.hbase.index.IndexRegionSplitPolicy;
 import org.apache.phoenix.jdbc.PhoenixConnection;
+import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
+import org.apache.phoenix.query.QueryServices;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableKey;
 import org.apache.phoenix.schema.TableNotFoundException;
 import org.apache.phoenix.util.MetaDataUtil;
+import org.apache.phoenix.util.ReadOnlyProps;
 import org.apache.phoenix.util.TestUtil;
+import org.junit.BeforeClass;
 import org.junit.Test;
 
+import com.google.common.collect.Maps;
+
 public class LocalIndexIT extends BaseIndexIT {
+
+    @BeforeClass 
+    public static void doSetup() throws Exception {
+        Map<String,String> props = Maps.newHashMapWithExpectedSize(3);
+        // Drop the HBase table metadata for this test
+        props.put(QueryServices.DROP_METADATA_ATTRIB, Boolean.toString(true));
+        // Must update config before starting server
+        startServer(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
+    }
+
     private void createBaseTable(String tableName, Integer saltBuckets, String splits) throws SQLException {
         Connection conn = DriverManager.getConnection(getUrl());
         String ddl = "CREATE TABLE " + tableName + " (t_id VARCHAR NOT NULL,\n" +
@@ -111,4 +131,23 @@ public class LocalIndexIT extends BaseIndexIT {
         HTable indexTable = new HTable(admin.getConfiguration(),TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)));
         assertEquals("Both user region and index table should have same split keys.", userTable.getStartKeys(), indexTable.getStartKeys());
     }
+
+    @Test
+    public void testDropLocalIndexTable() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, null);
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+        conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+        HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        assertTrue("Local index table should be present.", admin.tableExists(TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME))));
+        conn1.createStatement().execute("DROP TABLE "+ DATA_TABLE_NAME);
+        admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        assertFalse("Local index table should be deleted.", admin.tableExists(TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME))));
+        ResultSet rs = conn2.createStatement().executeQuery("SELECT "
+                + PhoenixDatabaseMetaData.SEQUENCE_SCHEMA + ","
+                + PhoenixDatabaseMetaData.SEQUENCE_NAME
+                + " FROM " + PhoenixDatabaseMetaData.SEQUENCE_TABLE_NAME);
+        assertFalse("View index sequences should be deleted.", rs.next());
+    }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java
new file mode 100644
index 0000000..386d0f1
--- /dev/null
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.end2end.index;
+
+import static org.junit.Assert.assertFalse;
+
+import java.sql.Connection;
+import java.sql.DriverManager;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.util.Map;
+
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.HBaseAdmin;
+import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
+import org.apache.phoenix.query.QueryServices;
+import org.apache.phoenix.util.MetaDataUtil;
+import org.apache.phoenix.util.ReadOnlyProps;
+import org.apache.phoenix.util.TestUtil;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import com.google.common.collect.Maps;
+
+public class ViewIndexIT extends BaseIndexIT {
+
+    private String VIEW_NAME = "MY_VIEW";
+
+    @BeforeClass
+    public static void doSetup() throws Exception {
+        Map<String,String> props = Maps.newHashMapWithExpectedSize(3);
+        // Drop the HBase table metadata for this test
+        props.put(QueryServices.DROP_METADATA_ATTRIB, Boolean.toString(true));
+        // Must update config before starting server
+        startServer(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
+    }
+
+    private void createBaseTable(String tableName, Integer saltBuckets, String splits) throws SQLException {
+        Connection conn = DriverManager.getConnection(getUrl());
+        String ddl = "CREATE TABLE " + tableName + " (t_id VARCHAR NOT NULL,\n" +
+                "k1 INTEGER NOT NULL,\n" +
+                "k2 INTEGER NOT NULL,\n" +
+                "v1 VARCHAR,\n" +
+                "CONSTRAINT pk PRIMARY KEY (t_id, k1, k2))\n"
+                        + (saltBuckets == null || splits != null ? "" : (",salt_buckets=" + saltBuckets)
+                        + (saltBuckets != null || splits == null ? "" : ",splits=" + splits));
+        conn.createStatement().execute(ddl);
+        conn.close();
+    }
+
+    @Test
+    public void testDeleteViewIndexSequences() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, null);
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("CREATE VIEW " + VIEW_NAME + " AS SELECT * FROM " + DATA_TABLE_NAME);
+        conn1.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + VIEW_NAME + " (v1)");
+        conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+        HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        conn1.createStatement().execute("DROP VIEW " + VIEW_NAME);
+        conn1.createStatement().execute("DROP TABLE "+ DATA_TABLE_NAME);
+        admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        assertFalse("View index table should be deleted.", admin.tableExists(TableName.valueOf(MetaDataUtil.getViewIndexTableName(DATA_TABLE_NAME))));
+        ResultSet rs = conn2.createStatement().executeQuery("SELECT "
+                + PhoenixDatabaseMetaData.SEQUENCE_SCHEMA + ","
+                + PhoenixDatabaseMetaData.SEQUENCE_NAME
+                + " FROM " + PhoenixDatabaseMetaData.SEQUENCE_TABLE_NAME);
+        assertFalse("View index sequences should be deleted.", rs.next());
+    }
+}
\ No newline at end of file
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
index b13ff6d..3ba5789 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
@@ -23,14 +23,17 @@ import java.util.Collections;
 import java.util.List;
 
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.util.Pair;
 import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.exception.SQLExceptionInfo;
 import org.apache.phoenix.execute.MutationState;
 import org.apache.phoenix.expression.LiteralExpression;
 import org.apache.phoenix.jdbc.PhoenixConnection;
+import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
 import org.apache.phoenix.jdbc.PhoenixStatement;
 import org.apache.phoenix.parse.CreateIndexStatement;
 import org.apache.phoenix.parse.ParseNode;
+import org.apache.phoenix.parse.PropertyName;
 import org.apache.phoenix.schema.MetaDataClient;
 import org.apache.phoenix.schema.PTable.IndexType;
 
@@ -48,9 +51,20 @@ public class CreateIndexCompiler {
         final StatementContext context = new StatementContext(statement, resolver, scan, new SequenceManager(statement));
         ExpressionCompiler expressionCompiler = new ExpressionCompiler(context);
         List<ParseNode> splitNodes = create.getSplitNodes();
-        if (!splitNodes.isEmpty() && create.getIndexType() == IndexType.LOCAL) {
-            throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SPLIT_LOCAL_INDEX)
-            .build().buildException();
+        if (create.getIndexType() == IndexType.LOCAL) {
+            if (!splitNodes.isEmpty()) {
+                throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SPLIT_LOCAL_INDEX)
+                .build().buildException();
+            } 
+            if (create.getProps() != null && create.getProps().get("") != null) {
+                List<Pair<String, Object>> list = create.getProps().get("");
+                for (Pair<String, Object> pair : list) {
+                    if (pair.getFirst().equals(PhoenixDatabaseMetaData.SALT_BUCKETS)) {
+                        throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SALT_LOCAL_INDEX)
+                        .build().buildException();
+                    }
+                }
+            }
         }
         final byte[][] splits = new byte[splitNodes.size()][];
         for (int i = 0; i < splits.length; i++) {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
index c0c6940..5681ea3 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
@@ -1090,7 +1090,40 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
         }
         return wasDeleted;
     }
-    
+
+    private boolean ensureLocalIndexTableDropped(byte[] physicalTableName, long timestamp) throws SQLException {
+        byte[] physicalIndexName = MetaDataUtil.getLocalIndexPhysicalName(physicalTableName);
+        HTableDescriptor desc = null;
+        HBaseAdmin admin = null;
+        boolean wasDeleted = false;
+        try {
+            admin = new HBaseAdmin(config);
+            try {
+                desc = admin.getTableDescriptor(physicalIndexName);
+                if (Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(desc.getValue(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_BYTES)))) {
+                    final ReadOnlyProps props = this.getProps();
+                    final boolean dropMetadata = props.getBoolean(DROP_METADATA_ATTRIB, DEFAULT_DROP_METADATA);
+                    if (dropMetadata) {
+                        admin.disableTable(physicalIndexName);
+                        admin.deleteTable(physicalIndexName);
+                        wasDeleted = true;
+                    }
+                }
+            } catch (org.apache.hadoop.hbase.TableNotFoundException ignore) {
+                // Ignore, as we may never have created a view index table
+            }
+        } catch (IOException e) {
+            throw ServerUtil.parseServerException(e); 
+        } finally {
+            try {
+                if (admin != null) admin.close();
+            } catch (IOException e) {
+                logger.warn("",e);
+            }
+        }
+        return wasDeleted;
+    }
+
     @Override
     public MetaDataMutationResult createTable(final List<Mutation> tableMetaData, byte[] physicalTableName, PTableType tableType,
             Map<String,Object> tableProps, final List<Pair<byte[],Map<String,Object>>> families, byte[][] splits) throws SQLException {
@@ -1236,6 +1269,7 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
                 byte[] physicalTableName = SchemaUtil.getTableNameAsBytes(schemaBytes, tableBytes);
                 long timestamp = MetaDataUtil.getClientTimeStamp(tableMetaData);
                 ensureViewIndexTableDropped(physicalTableName, timestamp);
+                ensureLocalIndexTableDropped(physicalTableName, timestamp);
             }
             break;
         default:
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index 266118c..e0fcc22 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -875,10 +875,6 @@ public class MetaDataClient {
             }
             // Delay this check as it is supported to have IMMUTABLE_ROWS and SALT_BUCKETS defined on views
             if ((statement.getTableType() == PTableType.VIEW || indexId != null) && !tableProps.isEmpty()) {
-                // TODO: do this check in CreateIndexCompiler
-                if (indexType == IndexType.LOCAL && saltBucketNum != null) {
-                    throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SALT_LOCAL_INDEX).build().buildException();
-                }
                 throw new SQLExceptionInfo.Builder(SQLExceptionCode.VIEW_WITH_PROPERTIES).build().buildException();
             }
             if (removedProp) {
@@ -1321,10 +1317,15 @@ public class MetaDataClient {
             List<Mutation> tableMetaData = Lists.newArrayListWithExpectedSize(2);
             Delete tableDelete = new Delete(key, clientTimeStamp);
             tableMetaData.add(tableDelete);
+            boolean hasViewIndexTable = false;
+            boolean hasLocalIndexTable = false;
             if (parentTableName != null) {
                 byte[] linkKey = MetaDataUtil.getParentLinkKey(tenantIdStr, schemaName, parentTableName, tableName);
                 Delete linkDelete = new Delete(linkKey, clientTimeStamp);
                 tableMetaData.add(linkDelete);
+            } else {
+                hasViewIndexTable = MetaDataUtil.hasViewIndexTable(connection, schemaName, tableName);
+                hasLocalIndexTable = MetaDataUtil.hasLocalIndexTable(connection, schemaName, tableName);
             }
 
             MetaDataMutationResult result = connection.getQueryServices().dropTable(tableMetaData, tableType);
@@ -1356,7 +1357,7 @@ public class MetaDataClient {
                         // PName name, PTableType type, long timeStamp, long sequenceNumber, List<PColumn> columns
                         List<TableRef> tableRefs = Lists.newArrayListWithExpectedSize(2 + table.getIndexes().size());
                         // All multi-tenant tables have a view index table, so no need to check in that case
-                        if (tableType == PTableType.TABLE && (table.isMultiTenant() || MetaDataUtil.hasViewIndexTable(connection, table.getPhysicalName()) || MetaDataUtil.hasLocalIndexTable(connection, table.getPhysicalName()))) {
+                        if (tableType == PTableType.TABLE && (table.isMultiTenant() || hasViewIndexTable || hasLocalIndexTable)) {
                             MetaDataUtil.deleteViewIndexSequences(connection, table.getPhysicalName());
                             // TODO: consider removing this, as the DROP INDEX done for each DROP VIEW command
                             // would have deleted all the rows already
@@ -1365,6 +1366,10 @@ public class MetaDataClient {
                                 String viewIndexTableName = MetaDataUtil.getViewIndexTableName(tableName);
                                 PTable viewIndexTable = new PTableImpl(null, viewIndexSchemaName, viewIndexTableName, ts, table.getColumnFamilies());
                                 tableRefs.add(new TableRef(null, viewIndexTable, ts, false));
+                                String localIndexSchemaName = MetaDataUtil.getLocalIndexSchemaName(schemaName);
+                                String localIndexTableName = MetaDataUtil.getLocalIndexTableName(tableName);
+                                PTable localIndexTable = new PTableImpl(null, localIndexSchemaName, localIndexTableName, ts, table.getColumnFamilies());
+                                tableRefs.add(new TableRef(null, localIndexTable, ts, false));
                             }
                         }
                         if (!dropMetaData) {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
index 8a3a190..4842626 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
@@ -281,7 +281,15 @@ public class MetaDataUtil {
     }
 
     public static boolean hasViewIndexTable(PhoenixConnection connection, PName name) throws SQLException {
-        byte[] physicalIndexName = MetaDataUtil.getViewIndexPhysicalName(name.getBytes());
+        return hasViewIndexTable(connection, name.getBytes());
+    }
+    
+    public static boolean hasViewIndexTable(PhoenixConnection connection, String schemaName, String tableName) throws SQLException {
+        return hasViewIndexTable(connection, SchemaUtil.getTableNameAsBytes(schemaName, tableName));
+    }
+    
+    public static boolean hasViewIndexTable(PhoenixConnection connection, byte[] physicalTableName) throws SQLException {
+        byte[] physicalIndexName = MetaDataUtil.getViewIndexPhysicalName(physicalTableName);
         try {
             HTableDescriptor desc = connection.getQueryServices().getTableDescriptor(physicalIndexName);
             return desc != null && Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(desc.getValue(IS_VIEW_INDEX_TABLE_PROP_BYTES)));
@@ -289,9 +297,17 @@ public class MetaDataUtil {
             return false;
         }
     }
-    
+
     public static boolean hasLocalIndexTable(PhoenixConnection connection, PName name) throws SQLException {
-        byte[] physicalIndexName = MetaDataUtil.getLocalIndexPhysicalName(name.getBytes());
+        return hasLocalIndexTable(connection, name.getBytes());
+    }
+
+    public static boolean hasLocalIndexTable(PhoenixConnection connection, String schemaName, String tableName) throws SQLException {
+        return hasLocalIndexTable(connection, SchemaUtil.getTableNameAsBytes(schemaName, tableName));
+    }
+
+    public static boolean hasLocalIndexTable(PhoenixConnection connection, byte[] physicalTableName) throws SQLException {
+        byte[] physicalIndexName = MetaDataUtil.getLocalIndexPhysicalName(physicalTableName);
         try {
             HTableDescriptor desc = connection.getQueryServices().getTableDescriptor(physicalIndexName);
             return desc != null && Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(desc.getValue(IS_LOCAL_INDEX_TABLE_PROP_BYTES)));
-- 
1.9.4.msysgit.0


From 90bdf2683adb3f9e389c10693ebd1c5bff68afeb Mon Sep 17 00:00:00 2001
From: James Taylor <jamestaylor@apache.org>
Date: Mon, 28 Apr 2014 09:31:31 -0700
Subject: [PATCH 05/14] PHOENIX-955 Skip region start key at beginning of local
 index rows (JamesTaylor)

---
 .../index/balancer/TestIndexLoadBalancer.java      |  4 ---
 .../coprocessor/BaseScannerRegionObserver.java     |  1 +
 .../GroupedAggregateRegionObserver.java            | 18 ++++++++--
 .../phoenix/coprocessor/ScanRegionObserver.java    | 19 ++++++++--
 .../org/apache/phoenix/execute/BasicQueryPlan.java |  6 ++++
 .../phoenix/expression/RowKeyColumnExpression.java | 12 ++++++-
 .../visitor/RowKeyExpressionVisitor.java           | 36 +++++++++++++++++++
 .../org/apache/phoenix/filter/SkipScanFilter.java  |  7 +++-
 .../org/apache/phoenix/index/IndexMaintainer.java  | 40 ++++++++++++----------
 .../apache/phoenix/index/PhoenixIndexCodec.java    |  4 +--
 .../java/org/apache/phoenix/util/IndexUtil.java    | 24 ++++++++++++-
 .../java/org/apache/phoenix/util/ScanUtil.java     | 35 +++++++++++++++++++
 .../apache/phoenix/index/IndexMaintainerTest.java  |  2 +-
 13 files changed, 174 insertions(+), 34 deletions(-)
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/expression/visitor/RowKeyExpressionVisitor.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java b/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
index 7a5d61c..9020e73 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/hbase/index/balancer/TestIndexLoadBalancer.java
@@ -23,8 +23,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.HColumnDescriptor;
@@ -62,8 +60,6 @@ import org.junit.experimental.categories.Category;
 @Category(LargeTests.class)
 public class TestIndexLoadBalancer {
 
-    private static final Log LOG = LogFactory.getLog(TestIndexLoadBalancer.class);
-
     private static HBaseTestingUtility UTIL = new HBaseTestingUtility();
 
     private static HBaseAdmin admin = null;
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
index b94bf0a..6f0124a 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
@@ -44,6 +44,7 @@ abstract public class BaseScannerRegionObserver extends BaseRegionObserver {
     public static final String EMPTY_CF = "_EmptyCF";
     public static final String SPECIFIC_ARRAY_INDEX = "_SpecificArrayIndex";
     public static final String GROUP_BY_LIMIT = "_GroupByLimit";
+    public static final String LOCAL_INDEX = "_LocalIndex";
 
     /**
      * Used by logger to identify coprocessor
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java
index a74745d..84adaa9 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java
@@ -64,6 +64,7 @@ import org.apache.phoenix.query.QueryConstants;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.SortOrder;
 import org.apache.phoenix.schema.tuple.MultiKeyValueTuple;
+import org.apache.phoenix.util.IndexUtil;
 import org.apache.phoenix.util.KeyValueUtil;
 import org.apache.phoenix.util.ScanUtil;
 import org.apache.phoenix.util.SizedUtil;
@@ -105,7 +106,17 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
             }
             keyOrdered = true;
         }
-        List<Expression> expressions = deserializeGroupByExpressions(expressionBytes);
+        int offset = 0;
+        if (ScanUtil.isLocalIndex(scan)) {
+            /*
+             * For local indexes, we need to set an offset on row key expressions to skip
+             * the region start key.
+             */
+            offset = c.getEnvironment().getRegion().getStartKey().length;
+            ScanUtil.setRowKeyOffset(scan, offset);
+        }
+        
+        List<Expression> expressions = deserializeGroupByExpressions(expressionBytes, offset);
         ServerAggregators aggregators =
                 ServerAggregators.deserialize(scan
                         .getAttribute(BaseScannerRegionObserver.AGGREGATORS), c
@@ -165,7 +176,7 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
 
     }
 
-    private List<Expression> deserializeGroupByExpressions(byte[] expressionBytes)
+    private List<Expression> deserializeGroupByExpressions(byte[] expressionBytes, int offset)
             throws IOException {
         List<Expression> expressions = new ArrayList<Expression>(3);
         ByteArrayInputStream stream = new ByteArrayInputStream(expressionBytes);
@@ -177,6 +188,9 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
                     Expression expression =
                             ExpressionType.values()[expressionOrdinal].newInstance();
                     expression.readFields(input);
+                    if (offset != 0) {
+                        IndexUtil.setRowKeyExpressionOffset(expression, offset);
+                    }
                     expressions.add(expression);
                 } catch (EOFException e) {
                     break;
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
index 51d9033..acb9464 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
@@ -25,8 +25,8 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.Type;
@@ -57,6 +57,7 @@ import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.ValueBitSet;
 import org.apache.phoenix.schema.tuple.MultiKeyValueTuple;
 import org.apache.phoenix.schema.tuple.Tuple;
+import org.apache.phoenix.util.IndexUtil;
 import org.apache.phoenix.util.ScanUtil;
 import org.apache.phoenix.util.ServerUtil;
 
@@ -100,7 +101,7 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
         }
     }
     
-    public static OrderedResultIterator deserializeFromScan(Scan scan, RegionScanner s) {
+    public static OrderedResultIterator deserializeFromScan(Scan scan, RegionScanner s, int offset) {
         byte[] topN = scan.getAttribute(BaseScannerRegionObserver.TOPN);
         if (topN == null) {
             return null;
@@ -116,6 +117,9 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
             for (int i = 0; i < size; i++) {
                 OrderByExpression orderByExpression = new OrderByExpression();
                 orderByExpression.readFields(input);
+                if (offset != 0) {
+                    IndexUtil.setRowKeyExpressionOffset(orderByExpression.getExpression(), offset);
+                }
                 orderByExpressions.add(orderByExpression);
             }
             ResultIterator inner = new RegionScannerResultIterator(s);
@@ -176,6 +180,15 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
         if (isScanQuery == null || Bytes.compareTo(PDataType.FALSE_BYTES, isScanQuery) == 0) {
             return s;
         }
+        int offset = 0;
+        if (ScanUtil.isLocalIndex(scan)) {
+            /*
+             * For local indexes, we need to set an offset on row key expressions to skip
+             * the region start key.
+             */
+            offset = c.getEnvironment().getRegion().getStartKey().length;
+            ScanUtil.setRowKeyOffset(scan, offset);
+        }
         
         final TupleProjector p = TupleProjector.deserializeProjectorFromScan(scan);
         final HashJoinInfo j = HashJoinInfo.deserializeHashJoinFromScan(scan);
@@ -186,7 +199,7 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
             innerScanner = new HashJoinRegionScanner(s, p, j, tenantId, c.getEnvironment());
         }
         
-        final OrderedResultIterator iterator = deserializeFromScan(scan,innerScanner);
+        final OrderedResultIterator iterator = deserializeFromScan(scan,innerScanner, offset);
         List<KeyValueColumnExpression> arrayKVRefs = new ArrayList<KeyValueColumnExpression>();
         Expression[] arrayFuncRefs = deserializeArrayPostionalExpressionInfoFromScan(
                 scan, innerScanner, arrayKVRefs);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java b/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java
index d0c97bd..17a1824 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java
@@ -36,6 +36,7 @@ import org.apache.phoenix.iterate.ParallelIterators.ParallelIteratorFactory;
 import org.apache.phoenix.iterate.ResultIterator;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.parse.FilterableStatement;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.TableRef;
 import org.apache.phoenix.util.SQLCloseable;
 import org.apache.phoenix.util.SQLCloseables;
@@ -131,6 +132,8 @@ public abstract class BasicQueryPlan implements QueryPlan {
             return ResultIterator.EMPTY_ITERATOR;
         }
         
+        // Set miscellaneous scan attributes. This is the last chance to set them before we
+        // clone the scan for each parallelized chunk.
         Scan scan = context.getScan();
         // Set producer on scan so HBase server does round robin processing
         //setProducer(scan);
@@ -150,6 +153,9 @@ public abstract class BasicQueryPlan implements QueryPlan {
         }
         ScanUtil.setTimeRange(scan, scn);
         ScanUtil.setTenantId(scan, connection.getTenantId() == null ? null : connection.getTenantId().getBytes());
+        if (context.getCurrentTable().getTable().getIndexType() == IndexType.LOCAL) {
+            ScanUtil.setLocalIndex(scan);
+        }
         ResultIterator iterator = newIterator();
         return dependencies.isEmpty() ? 
                 iterator : new DelegateResultIterator(iterator) {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/RowKeyColumnExpression.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/RowKeyColumnExpression.java
index 6761733..7913ab8 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/RowKeyColumnExpression.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/RowKeyColumnExpression.java
@@ -41,6 +41,7 @@ public class RowKeyColumnExpression  extends ColumnExpression {
     private PDataType fromType;
     private RowKeyValueAccessor accessor;
     protected final String name;
+    private int offset;
     
     public RowKeyColumnExpression() {
         name = null; // Only on client
@@ -65,6 +66,15 @@ public class RowKeyColumnExpression  extends ColumnExpression {
         this(datum, accessor, fromType, datum.toString());
     }
     
+    /**
+     * Used to set an offset to be skipped from the start of a the row key. Used by
+     * local indexing to skip the region start key bytes.
+     * @param offset the number of bytes to offset accesses to row key columns
+     */
+    public void setOffset(int offset) {
+        this.offset = offset;
+    }
+    
     public int getPosition() {
         return accessor.getIndex();
     }
@@ -94,7 +104,7 @@ public class RowKeyColumnExpression  extends ColumnExpression {
     @Override
     public boolean evaluate(Tuple tuple, ImmutableBytesWritable ptr) {
         tuple.getKey(ptr);
-        int offset = accessor.getOffset(ptr.get(), ptr.getOffset());
+        int offset = accessor.getOffset(ptr.get(), ptr.getOffset() + this.offset);
         // Null is represented in the last expression of a multi-part key 
         // by the bytes not being present.
         int maxOffset = ptr.getOffset() + ptr.getLength();
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/visitor/RowKeyExpressionVisitor.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/visitor/RowKeyExpressionVisitor.java
new file mode 100644
index 0000000..a3e8fb0
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/visitor/RowKeyExpressionVisitor.java
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.expression.visitor;
+
+import org.apache.phoenix.expression.RowKeyColumnExpression;
+
+
+
+
+/**
+ * 
+ * Implementation of ExpressionVisitor where only a RowKeyColumnExpression (i.e.
+ * a reference to a column that makes up the row key) is being visited,
+ *
+ * 
+ * @since 0.1
+ */
+public abstract class RowKeyExpressionVisitor extends TraverseAllExpressionVisitor<Void> {
+    @Override
+    abstract public Void visit(RowKeyColumnExpression node);
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java b/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
index b627226..b451ad6 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
@@ -72,6 +72,7 @@ public class SkipScanFilter extends FilterBase implements Writable {
     private byte[] endKey; 
     private int endKeyLength;
     private boolean isDone;
+    private int offset;
 
     private final ImmutableBytesWritable ptr = new ImmutableBytesWritable();
 
@@ -85,6 +86,10 @@ public class SkipScanFilter extends FilterBase implements Writable {
     public SkipScanFilter(List<List<KeyRange>> slots, RowKeySchema schema) {
         init(slots, schema);
     }
+    
+    public void setOffset(int offset) {
+        this.offset = offset;
+    }
 
     private void init(List<List<KeyRange>> slots, RowKeySchema schema) {
         for (List<KeyRange> ranges : slots) {
@@ -113,7 +118,7 @@ public class SkipScanFilter extends FilterBase implements Writable {
 
     @Override
     public ReturnCode filterKeyValue(Cell kv) {
-        return navigate(kv.getRowArray(), kv.getRowOffset(),kv.getRowLength(),Terminate.AFTER);
+        return navigate(kv.getRowArray(), kv.getRowOffset() + offset,kv.getRowLength(),Terminate.AFTER);
     }
 
     @Override
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
index 0c69df3..8251668 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
@@ -32,7 +32,6 @@ import java.util.Map;
 import java.util.Set;
 
 import org.apache.hadoop.hbase.CellUtil;
-import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.Type;
 import org.apache.hadoop.hbase.client.Delete;
@@ -52,6 +51,7 @@ import org.apache.phoenix.schema.PColumnFamily;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PIndexState;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.RowKeySchema;
 import org.apache.phoenix.schema.SaltingUtil;
@@ -211,6 +211,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
     private ImmutableBytesPtr emptyKeyValueCFPtr;
     private int nDataCFs;
     private boolean indexWALDisabled;
+    private boolean isLocalIndex;
 
     // Transient state
     private final boolean isDataTableSalted;
@@ -270,13 +271,19 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         this.emptyKeyValueCFPtr = SchemaUtil.getEmptyColumnFamilyPtr(index);
         this.nDataCFs = dataTable.getColumnFamilies().size();
         this.indexWALDisabled = indexWALDisabled;
+        this.isLocalIndex = index.getIndexType() == IndexType.LOCAL;
     }
 
-    public byte[] buildRowKey(ValueGetter valueGetter, ImmutableBytesWritable rowKeyPtr)  {
+    public byte[] buildRowKey(ValueGetter valueGetter, ImmutableBytesWritable rowKeyPtr, byte[] regionStartKey)  {
         ImmutableBytesWritable ptr = new ImmutableBytesWritable();
-        TrustedByteArrayOutputStream stream = new TrustedByteArrayOutputStream(estimatedIndexRowKeyBytes);
+        boolean prependRegionStartKey = isLocalIndex && regionStartKey != null;
+        TrustedByteArrayOutputStream stream = new TrustedByteArrayOutputStream(estimatedIndexRowKeyBytes + (prependRegionStartKey ? regionStartKey.length : 0));
         DataOutput output = new DataOutputStream(stream);
         try {
+            // For local indexes, we must prepend the row key with the start region key
+            if (prependRegionStartKey) {
+                output.write(regionStartKey);
+            }
             if (nIndexSaltBuckets > 0) {
                 output.write(0); // will be set at end to index salt byte
             }
@@ -385,11 +392,11 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         }
     }
 
-    public Put buildUpdateMutation(KeyValueBuilder kvBuilder, ValueGetter valueGetter, ImmutableBytesWritable dataRowKeyPtr, long ts) throws IOException {
+    public Put buildUpdateMutation(KeyValueBuilder kvBuilder, ValueGetter valueGetter, ImmutableBytesWritable dataRowKeyPtr, long ts, byte[] regionStartKey) throws IOException {
         Put put = null;
         // New row being inserted: add the empty key value
         if (valueGetter.getLatestValue(dataEmptyKeyValueRef) == null) {
-            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr);
+            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr, regionStartKey);
             put = new Put(indexRowKey);
             // add the keyvalue for the empty row
             put.add(kvBuilder.buildPut(new ImmutableBytesPtr(indexRowKey),
@@ -401,7 +408,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         for (ColumnReference ref : this.getCoverededColumns()) {
             ImmutableBytesPtr cq = this.indexQualifiers.get(i++);
             ImmutableBytesPtr value = valueGetter.getLatestValue(ref);
-            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr);
+            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr, regionStartKey);
             ImmutableBytesPtr rowKey = new ImmutableBytesPtr(indexRowKey);
             if (value != null) {
                 if (put == null) {
@@ -415,14 +422,6 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         return put;
     }
 
-    public Put buildUpdateMutation(KeyValueBuilder kvBuilder, ValueGetter valueGetter, ImmutableBytesWritable dataRowKeyPtr) throws IOException {
-        return buildUpdateMutation(kvBuilder, valueGetter, dataRowKeyPtr, HConstants.LATEST_TIMESTAMP);
-    }
-    
-    public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ValueGetter valueGetter, ImmutableBytesWritable dataRowKeyPtr, Collection<KeyValue> pendingUpdates) throws IOException {
-        return buildDeleteMutation(kvBuilder, valueGetter, dataRowKeyPtr, pendingUpdates, HConstants.LATEST_TIMESTAMP);
-    }
-    
     public boolean isRowDeleted(Collection<KeyValue> pendingUpdates) {
         int nDeleteCF = 0;
         for (KeyValue kv : pendingUpdates) {
@@ -476,12 +475,12 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
      * since we can build the corresponding index row key.
      */
     public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ImmutableBytesWritable dataRowKeyPtr, long ts) throws IOException {
-        return buildDeleteMutation(kvBuilder, null, dataRowKeyPtr, Collections.<KeyValue>emptyList(), ts);
+        return buildDeleteMutation(kvBuilder, null, dataRowKeyPtr, Collections.<KeyValue>emptyList(), ts, null);
     }
     
     @SuppressWarnings("deprecation")
-    public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ValueGetter oldState, ImmutableBytesWritable dataRowKeyPtr, Collection<KeyValue> pendingUpdates, long ts) throws IOException {
-        byte[] indexRowKey = this.buildRowKey(oldState, dataRowKeyPtr);
+    public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ValueGetter oldState, ImmutableBytesWritable dataRowKeyPtr, Collection<KeyValue> pendingUpdates, long ts, byte[] regionStartKey) throws IOException {
+        byte[] indexRowKey = this.buildRowKey(oldState, dataRowKeyPtr, regionStartKey);
         // Delete the entire row if any of the indexed columns changed
         if (oldState == null || isRowDeleted(pendingUpdates) || hasIndexedColumnChanged(oldState, pendingUpdates)) { // Deleting the entire row
             Delete delete = new Delete(indexRowKey, ts);
@@ -560,7 +559,9 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
             PDataType type = PDataType.values()[WritableUtils.readVInt(input)];
             indexedColumnTypes.add(type);
         }
-        int nCoveredColumns = WritableUtils.readVInt(input);
+        int encodedCoveredolumnsAndLocalIndex = WritableUtils.readVInt(input);
+        isLocalIndex = encodedCoveredolumnsAndLocalIndex < 0;
+        int nCoveredColumns = Math.abs(encodedCoveredolumnsAndLocalIndex) - 1;
         coveredColumns = Sets.newLinkedHashSetWithExpectedSize(nCoveredColumns);
         for (int i = 0; i < nCoveredColumns; i++) {
             byte[] cf = Bytes.readByteArray(input);
@@ -599,7 +600,8 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
             PDataType type = indexedColumnTypes.get(i);
             WritableUtils.writeVInt(output, type.ordinal());
         }
-        WritableUtils.writeVInt(output, coveredColumns.size());
+        // Encode coveredColumns.size() and whether or not this is a local index
+        WritableUtils.writeVInt(output, (coveredColumns.size() + 1) * (isLocalIndex ? -1 : 1));
         for (ColumnReference ref : coveredColumns) {
             Bytes.writeByteArray(output, ref.getFamily());
             Bytes.writeByteArray(output, ref.getQualifier());
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java b/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java
index 800daf1..119c7cf 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java
@@ -132,7 +132,7 @@ public class PhoenixIndexCodec extends BaseIndexCodec {
             // get the values from the scanner so we can actually use them
             ValueGetter valueGetter = IndexManagementUtil.createGetterFromScanner(scanner, dataRowKey);
             ptr.set(dataRowKey);
-            Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, state.getCurrentTimestamp());
+            Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, state.getCurrentTimestamp(), env.getRegion().getStartKey());
             indexUpdate.setTable(maintainer.getIndexTableName());
             indexUpdate.setUpdate(put);
             //make sure we close the scanner when we are done
@@ -162,7 +162,7 @@ public class PhoenixIndexCodec extends BaseIndexCodec {
             ptr.set(dataRowKey);
             Delete delete =
                 maintainer.buildDeleteMutation(kvBuilder, valueGetter, ptr,
-                  state.getPendingUpdate(), state.getCurrentTimestamp());
+                  state.getPendingUpdate(), state.getCurrentTimestamp(), env.getRegion().getStartKey());
             scanner.close();
             indexUpdate.setUpdate(delete);
             indexUpdates.add(indexUpdate);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java
index e650fba..8b583f2 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java
@@ -29,6 +29,9 @@ import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.exception.SQLExceptionInfo;
+import org.apache.phoenix.expression.Expression;
+import org.apache.phoenix.expression.RowKeyColumnExpression;
+import org.apache.phoenix.expression.visitor.RowKeyExpressionVisitor;
 import org.apache.phoenix.hbase.index.ValueGetter;
 import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
 import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
@@ -182,7 +185,7 @@ public class IndexUtil {
                         }
                         
                     };
-                    indexMutations.add(maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts));
+                    indexMutations.add(maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts, null));
                 } else {
                     // We can only generate the correct Delete if we have no KV columns in our index.
                     // Perhaps it'd be best to ignore Delete mutations all together here, as this
@@ -211,4 +214,23 @@ public class IndexUtil {
         }
         return false;
     }
+    
+    /**
+     * Traverse the expression tree and set the offset of every RowKeyColumnExpression
+     * to the offset provided. This is used for local indexing on the server-side to
+     * skip over the region start key that prefixes index rows.
+     * @param rootExpression the root expression from which to begin traversal
+     * @param offset the offset to set on each RowKeyColumnExpression
+     */
+    public static void setRowKeyExpressionOffset(Expression rootExpression, final int offset) {
+        rootExpression.accept(new RowKeyExpressionVisitor() {
+
+            @Override
+            public Void visit(RowKeyColumnExpression node) {
+                node.setOffset(offset);
+                return null;
+            }
+            
+        });
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
index be781c5..2e78022 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
@@ -34,7 +34,9 @@ import org.apache.hadoop.hbase.filter.FilterList;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.phoenix.compile.ScanRanges;
+import org.apache.phoenix.coprocessor.BaseScannerRegionObserver;
 import org.apache.phoenix.coprocessor.MetaDataProtocol;
+import org.apache.phoenix.filter.BooleanExpressionFilter;
 import org.apache.phoenix.filter.SkipScanFilter;
 import org.apache.phoenix.query.KeyRange;
 import org.apache.phoenix.query.KeyRange.Bound;
@@ -62,6 +64,14 @@ public class ScanUtil {
         scan.setAttribute(PhoenixRuntime.TENANT_ID_ATTRIB, tenantId);
     }
 
+    public static void setLocalIndex(Scan scan) {
+        scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX, PDataType.TRUE_BYTES);
+    }
+
+    public static boolean isLocalIndex(Scan scan) {
+        return scan.getAttribute(BaseScannerRegionObserver.LOCAL_INDEX) != null;
+    }
+
     // Use getTenantId and pass in column name to match against
     // in as PSchema attribute. If column name matches in 
     // KeyExpressions, set on scan as attribute
@@ -414,4 +424,29 @@ public class ScanUtil {
         byte[] reversed = scan.getAttribute(REVERSED_ATTR);
         return (PDataType.TRUE_BYTES.equals(reversed));
     }
+    
+    private static void setRowKeyOffset(Filter filter, int offset) {
+        if (filter instanceof BooleanExpressionFilter) {
+            BooleanExpressionFilter boolFilter = (BooleanExpressionFilter)filter;
+            IndexUtil.setRowKeyExpressionOffset(boolFilter.getExpression(), offset);
+        } else if (filter instanceof SkipScanFilter) {
+            SkipScanFilter skipScanFilter = (SkipScanFilter)filter;
+            skipScanFilter.setOffset(offset);
+        }
+    }
+
+    public static void setRowKeyOffset(Scan scan, int offset) {
+        Filter filter = scan.getFilter();
+        if (filter == null) {
+            return;
+        }
+        if (filter instanceof FilterList) {
+            FilterList filterList = (FilterList)filter;
+            for (Filter childFilter : filterList.getFilters()) {
+                setRowKeyOffset(childFilter, offset);
+            }
+        } else {
+            setRowKeyOffset(filter, offset);
+        }
+    }
 }
\ No newline at end of file
diff --git a/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java b/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java
index 428d9d2..ba78e52 100644
--- a/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java
+++ b/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java
@@ -133,7 +133,7 @@ public class IndexMaintainerTest  extends BaseConnectionlessQueryTest {
             ImmutableBytesWritable indexKeyPtr = new ImmutableBytesWritable(indexMutation.getRow());
             
             ptr.set(rowKeyPtr.get(), rowKeyPtr.getOffset(), rowKeyPtr.getLength());
-            byte[] mutablelndexRowKey = im1.buildRowKey(valueGetter, ptr);
+            byte[] mutablelndexRowKey = im1.buildRowKey(valueGetter, ptr, null);
             byte[] immutableIndexRowKey = indexKeyPtr.copyBytes();
             assertArrayEquals(immutableIndexRowKey, mutablelndexRowKey);
             
-- 
1.9.4.msysgit.0


From 626b0dc1317d90e37d7ebaa24f55be1b777000fb Mon Sep 17 00:00:00 2001
From: James Taylor <jtaylor@salesforce.com>
Date: Wed, 7 May 2014 23:23:37 -0700
Subject: [PATCH 06/14] PHOENIX-937 Handle puts on local index table
 (Rajeshbabu)

---
 .../apache/phoenix/end2end/index/LocalIndexIT.java | 80 ++++++++++++++++++++--
 .../coprocessor/BaseScannerRegionObserver.java     |  1 +
 .../UngroupedAggregateRegionObserver.java          | 65 +++++++++++++++++-
 .../org/apache/phoenix/index/IndexMaintainer.java  |  4 +-
 .../org/apache/phoenix/schema/MetaDataClient.java  | 54 +++++++++++++--
 5 files changed, 191 insertions(+), 13 deletions(-)

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index 59bbc7f..6bc1b90 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -33,6 +33,10 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.ResultScanner;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.util.Pair;
 import org.apache.phoenix.hbase.index.IndexRegionSplitPolicy;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
@@ -67,8 +71,8 @@ public class LocalIndexIT extends BaseIndexIT {
                 "k2 INTEGER NOT NULL,\n" +
                 "v1 VARCHAR,\n" +
                 "CONSTRAINT pk PRIMARY KEY (t_id, k1, k2))\n"
-                        + (saltBuckets == null || splits != null ? "" : (",salt_buckets=" + saltBuckets)
-                        + (saltBuckets != null || splits == null ? "" : ",splits=" + splits));
+                        + (saltBuckets != null && splits == null ? (",salt_buckets=" + saltBuckets) : "" 
+                        + (saltBuckets == null && splits != null ? (" split on " + splits) : ""));
         conn.createStatement().execute(ddl);
         conn.close();
     }
@@ -91,13 +95,13 @@ public class LocalIndexIT extends BaseIndexIT {
         Connection conn1 = DriverManager.getConnection(getUrl());
         Connection conn2 = DriverManager.getConnection(getUrl());
         try {
-            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)"+" splits={1,2,3}");
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)"+" split on (1,2,3)");
             fail("Local index cannot be pre-split");
         } catch (SQLException e) { }
         try {
             conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
             conn2.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
-            fail("Local index should be created.");
+            fail("Local index should not be created.");
         } catch (TableNotFoundException e) { }
     }
 
@@ -119,7 +123,7 @@ public class LocalIndexIT extends BaseIndexIT {
 
     @Test
     public void testLocalIndexTableRegionSplitPolicyAndSplitKeys() throws Exception {
-        createBaseTable(DATA_TABLE_NAME, null,"{1,2,3}");
+        createBaseTable(DATA_TABLE_NAME, null,"('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
         Connection conn2 = DriverManager.getConnection(getUrl());
         conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
@@ -150,4 +154,70 @@ public class LocalIndexIT extends BaseIndexIT {
                 + " FROM " + PhoenixDatabaseMetaData.SEQUENCE_TABLE_NAME);
         assertFalse("View index sequences should be deleted.", rs.next());
     }
+    
+    @Test
+    public void testPutsToLocalIndexTable() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,'c')");
+        conn1.commit();
+        ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
+        assertTrue(rs.next());
+        assertEquals(4, rs.getInt(1));
+        HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        HTable indexTable = new HTable(admin.getConfiguration() ,TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)));
+        Pair<byte[][], byte[][]> startEndKeys = indexTable.getStartEndKeys();
+        byte[][] startKeys = startEndKeys.getFirst();
+        byte[][] endKeys = startEndKeys.getSecond();
+        for (int i = 0; i < startKeys.length; i++) {
+            Scan s = new Scan();
+            s.setStartRow(startKeys[i]);
+            s.setStopRow(endKeys[i]);
+            ResultScanner scanner = indexTable.getScanner(s);
+            int count = 0;
+            for(Result r:scanner){
+                count++;
+            }
+            scanner.close();
+            assertEquals(1, count);
+        }
+        indexTable.close();
+    }
+    
+    @Test
+    public void testBuildIndexWhenUserTableAlreadyHasData() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,'c')");
+        conn1.commit();
+        conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+        ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
+        assertTrue(rs.next());
+        assertEquals(4, rs.getInt(1));
+        HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+        HTable indexTable = new HTable(admin.getConfiguration() ,TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)));
+        Pair<byte[][], byte[][]> startEndKeys = indexTable.getStartEndKeys();
+        byte[][] startKeys = startEndKeys.getFirst();
+        byte[][] endKeys = startEndKeys.getSecond();
+        for (int i = 0; i < startKeys.length; i++) {
+            Scan s = new Scan();
+            s.setStartRow(startKeys[i]);
+            s.setStopRow(endKeys[i]);
+            ResultScanner scanner = indexTable.getScanner(s);
+            int count = 0;
+            for(Result r:scanner){
+                count++;
+            }
+            scanner.close();
+            assertEquals(1, count);
+        }
+        indexTable.close();
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
index 6f0124a..9e42f64 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
@@ -45,6 +45,7 @@ abstract public class BaseScannerRegionObserver extends BaseRegionObserver {
     public static final String SPECIFIC_ARRAY_INDEX = "_SpecificArrayIndex";
     public static final String GROUP_BY_LIMIT = "_GroupByLimit";
     public static final String LOCAL_INDEX = "_LocalIndex";
+    public static final String LOCAL_INDEX_BUILD = "_LocalIndexBuild";
 
     /**
      * Used by logger to identify coprocessor
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
index 995889d..1a26ab5 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
@@ -21,6 +21,7 @@ import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;
 import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;
 import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;
 import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;
+import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_BYTES_PTR;
 import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;
 
 import java.io.ByteArrayInputStream;
@@ -31,6 +32,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
@@ -38,6 +40,7 @@ import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CoprocessorEnvironment;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
@@ -56,8 +59,12 @@ import org.apache.phoenix.expression.ExpressionType;
 import org.apache.phoenix.expression.aggregator.Aggregator;
 import org.apache.phoenix.expression.aggregator.Aggregators;
 import org.apache.phoenix.expression.aggregator.ServerAggregators;
+import org.apache.phoenix.hbase.index.ValueGetter;
 import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;
+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
+import org.apache.phoenix.hbase.index.util.IndexManagementUtil;
 import org.apache.phoenix.hbase.index.util.KeyValueBuilder;
+import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.index.PhoenixIndexCodec;
 import org.apache.phoenix.join.HashJoinInfo;
 import org.apache.phoenix.join.TupleProjector;
@@ -73,6 +80,7 @@ import org.apache.phoenix.schema.SortOrder;
 import org.apache.phoenix.schema.tuple.MultiKeyValueTuple;
 import org.apache.phoenix.util.ByteUtil;
 import org.apache.phoenix.util.KeyValueUtil;
+import org.apache.phoenix.util.MetaDataUtil;
 import org.apache.phoenix.util.ScanUtil;
 import org.apache.phoenix.util.SchemaUtil;
 import org.slf4j.Logger;
@@ -130,6 +138,10 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
         if (isUngroupedAgg == null) {
             return s;
         }
+
+        byte[] localIndexBytes = scan.getAttribute(LOCAL_INDEX_BUILD);
+        List<IndexMaintainer> indexMaintainers = localIndexBytes == null ? null : IndexMaintainer.deserialize(localIndexBytes);
+        List<Mutation> indexMutations = localIndexBytes == null ? Collections.<Mutation>emptyList() : Lists.<Mutation>newArrayListWithExpectedSize(1024);
         
         final TupleProjector p = TupleProjector.deserializeProjectorFromScan(scan);
         final HashJoinInfo j = HashJoinInfo.deserializeHashJoinFromScan(scan);
@@ -165,6 +177,9 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
             }
             emptyCF = scan.getAttribute(BaseScannerRegionObserver.EMPTY_CF);
         }
+        if(localIndexBytes != null) {
+            ptr = new ImmutableBytesWritable();
+        }
         
         int batchSize = 0;
         long ts = scan.getTimeRange().getMax();
@@ -197,7 +212,26 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
                     rowCount++;
                     result.setKeyValues(results);
                     try {
-                        if (isDelete) {
+                        if (indexMaintainers != null) {
+                            for (IndexMaintainer maintainer : indexMaintainers) {
+                                ImmutableBytesPtr emptyKeyValueFamily = maintainer.getEmptyKeyValueFamily();
+                                Iterator<Cell> iterator = results.iterator();
+                                while (iterator.hasNext()) {
+                                    Cell cell = iterator.next();
+                                    if (Bytes.compareTo(cell.getRowArray(), cell.getFamilyOffset(), cell.getFamilyLength(), emptyKeyValueFamily.get(), emptyKeyValueFamily.getOffset(), emptyKeyValueFamily.getLength()) == 0
+                                            && Bytes.compareTo(cell.getRowArray(), cell.getQualifierOffset(), cell.getQualifierLength(), EMPTY_COLUMN_BYTES_PTR.get(), EMPTY_COLUMN_BYTES_PTR.getOffset(), EMPTY_COLUMN_BYTES_PTR.getLength()) == 0) {
+                                        iterator.remove();
+                                    }
+                                }
+                                if (!results.isEmpty()) {
+                                    result.getKey(ptr);
+                                    ValueGetter valueGetter = IndexManagementUtil.createGetterFromKeyValues(results);
+                                    Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts, c.getEnvironment().getRegion().getStartKey());
+                                    indexMutations.add(put);
+                                }
+                            }
+                            result.setKeyValues(results);
+                        } else if (isDelete) {
                             // FIXME: the version of the Delete constructor without the lock args was introduced
                             // in 0.94.4, thus if we try to use it here we can no longer use the 0.94.2 version
                             // of the client.
@@ -278,6 +312,14 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
                             commitBatch(region, mutations, indexUUID);
                             mutations.clear();
                         }
+                        // Commit in batches based on UPSERT_BATCH_SIZE_ATTRIB in config
+                        if (!indexMutations.isEmpty() && batchSize > 0 && indexMutations.size() % batchSize == 0) {
+                            HRegion indexRegion = getIndexRegion(c.getEnvironment());
+                            // Get indexRegion corresponding to data region
+                            commitBatch(indexRegion, indexMutations, null);
+                            indexMutations.clear();
+                        }
+
                     } catch (ConstraintViolationException e) {
                         // Log and ignore in count
                         logger.error("Failed to create row in " + region.getRegionNameAsString() + " with values " + SchemaUtil.toString(values), e);
@@ -300,6 +342,13 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
             commitBatch(region,mutations, indexUUID);
         }
 
+        if (!indexMutations.isEmpty()) {
+            HRegion indexRegion = getIndexRegion(c.getEnvironment());
+            // Get indexRegion corresponding to data region
+            commitBatch(indexRegion, indexMutations, null);
+            indexMutations.clear();
+        }
+
         final boolean hadAny = hasAny;
         KeyValue keyValue = null;
         if (hadAny) {
@@ -341,7 +390,19 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
         };
         return scanner;
     }
-    
+
+    private HRegion getIndexRegion(RegionCoprocessorEnvironment environment) throws IOException {
+        HRegion userRegion = environment.getRegion();
+        TableName indexTableName = TableName.valueOf(MetaDataUtil.getLocalIndexPhysicalName(userRegion.getTableDesc().getName()));
+        List<HRegion> onlineRegions = environment.getRegionServerServices().getOnlineRegions(indexTableName);
+        for(HRegion indexRegion : onlineRegions) {
+            if (Bytes.compareTo(userRegion.getStartKey(), indexRegion.getStartKey()) == 0) {
+                return indexRegion;
+            }
+        }
+        return null;
+    }
+
     private static PTable deserializeTable(byte[] b) {
         try {
             PTableProtos.PTable ptableProto = PTableProtos.PTable.parseFrom(b);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
index 8251668..c4edf75 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
@@ -520,7 +520,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         return allColumns;
     }
     
-    private ImmutableBytesPtr getEmptyKeyValueFamily() {
+    public ImmutableBytesPtr getEmptyKeyValueFamily() {
         // Since the metadata of an index table will never change,
         // we can infer this based on the family of the first covered column
         // If if there are no covered columns, we know it's our default name
@@ -706,7 +706,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
     }
 
     private int getIndexPkColumnCount() {
-        return dataRowKeySchema.getFieldCount() + indexedColumns.size() - (isDataTableSalted ? 1 : 0) - (isMultiTenant ? 1 : 0) - (viewIndexId == null ? 0 : 1);
+        return dataRowKeySchema.getFieldCount() + indexedColumns.size() - (isDataTableSalted ? 1 : 0) - (isMultiTenant ? 1 : 0) /*+ (viewIndexId == null ? 0 : 1)*/;
     }
     
     private RowKeyMetaData newRowKeyMetaData() {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index e0fcc22..33c1406 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -75,6 +75,7 @@ import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
 
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.HColumnDescriptor;
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.HRegionLocation;
@@ -82,6 +83,8 @@ import org.apache.hadoop.hbase.HTableDescriptor;
 import org.apache.hadoop.hbase.client.Delete;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hbase.util.Pair;
 import org.apache.phoenix.compile.ColumnResolver;
@@ -89,14 +92,20 @@ import org.apache.phoenix.compile.FromCompiler;
 import org.apache.phoenix.compile.MutationPlan;
 import org.apache.phoenix.compile.PostDDLCompiler;
 import org.apache.phoenix.compile.PostIndexDDLCompiler;
+import org.apache.phoenix.compile.QueryPlan;
+import org.apache.phoenix.coprocessor.BaseScannerRegionObserver;
 import org.apache.phoenix.coprocessor.MetaDataProtocol;
 import org.apache.phoenix.coprocessor.MetaDataProtocol.MetaDataMutationResult;
 import org.apache.phoenix.coprocessor.MetaDataProtocol.MutationCode;
 import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.exception.SQLExceptionInfo;
 import org.apache.phoenix.execute.MutationState;
+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
+import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
+import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
+import org.apache.phoenix.jdbc.PhoenixStatement;
 import org.apache.phoenix.parse.AddColumnStatement;
 import org.apache.phoenix.parse.AlterIndexStatement;
 import org.apache.phoenix.parse.ColumnDef;
@@ -489,9 +498,43 @@ public class MetaDataClient {
         connection.rollback();
         try {
             connection.setAutoCommit(true);
-            PostIndexDDLCompiler compiler = new PostIndexDDLCompiler(connection, dataTableRef);
-            MutationPlan plan = compiler.compile(index);
-            MutationState state = connection.getQueryServices().updateData(plan);
+            MutationState state;
+            // For local indexes, we optimize the initial index population by *not* sending Puts over
+            // the wire for the index rows, as we don't need to do that. Instead, we tap into our
+            // region observer to generate the index rows based on the data rows as we scan
+            if (index.getIndexType() == IndexType.LOCAL) {
+                final PhoenixStatement statement = new PhoenixStatement(connection);
+                String query = "SELECT count(*) FROM \"" + dataTableRef.getTable().getName().getString() + "\"";
+                QueryPlan plan = statement.compileQuery(query);
+                // Set attribute on scan that UngroupedAggregateRegionObserver will switch on.
+                // We'll detect that this attribute was set the server-side and write the index
+                // rows per region as a result. The value of the attribute will be our persisted
+                // index maintainers.
+                // Define the LOCAL_INDEX_BUILD as a new static in BaseScannerRegionObserver
+                Scan scan = plan.getContext().getScan();
+                ImmutableBytesWritable ptr = new ImmutableBytesWritable();
+                PTable dataTable = dataTableRef.getTable();
+                dataTable.getIndexMaintainers(ptr);
+                scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX_BUILD, ByteUtil.copyKeyBytesIfNecessary(ptr));
+                // By default, we'd use a FirstKeyOnly filter as nothing else needs to be projected for count(*).
+                // However, in this case, we need to project all of the data columns that contribute to the index.
+                IndexMaintainer indexMaintainer = index.getIndexMaintainer(dataTable);
+                for (ColumnReference columnRef : indexMaintainer.getAllColumns()) {
+                    scan.addColumn(columnRef.getFamily(), columnRef.getQualifier());
+                }
+                Cell kv = plan.iterator().next().getValue(0);
+                ImmutableBytesWritable tmpPtr = new ImmutableBytesWritable(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength());
+                // A single Cell will be returned with the count(*) - we decode that here
+                long rowCount = PDataType.LONG.getCodec().decodeLong(tmpPtr, SortOrder.getDefault());
+                // The contract is to return a MutationState that contains the number of rows modified. In this
+                // case, it's the number of rows in the data table which corresponds to the number of index
+                // rows that were added.
+                state = new MutationState(0, connection, rowCount);
+            } else {
+                PostIndexDDLCompiler compiler = new PostIndexDDLCompiler(connection, dataTableRef);
+                MutationPlan plan = compiler.compile(index);
+                state = connection.getQueryServices().updateData(plan);
+            }
             AlterIndexStatement indexStatement = FACTORY.alterIndex(FACTORY.namedTable(null, 
                     TableName.create(index.getSchemaName().getString(), index.getTableName().getString())),
                     dataTableRef.getTable().getTableName().getString(), false, PIndexState.ACTIVE);
@@ -685,7 +728,10 @@ public class MetaDataClient {
         if (connection.getSCN() != null) {
             return buildIndexAtTimeStamp(table, statement.getTable());
         }
-        
+        if (statement.getIndexType() == IndexType.LOCAL) {
+            ColumnResolver resolver = FromCompiler.getResolverForMutation(statement, connection);
+            tableRef = resolver.getTables().get(0);
+        }
         return buildIndex(table, tableRef);
     }
 
-- 
1.9.4.msysgit.0


From 2a6bb7b19309c2ce94cec732768ee498a185aed4 Mon Sep 17 00:00:00 2001
From: James Taylor <jtaylor@salesforce.com>
Date: Thu, 22 May 2014 20:48:01 -0700
Subject: [PATCH 07/14] PHOENIX-994 Handle scans on local index table in case
 any best fit covering local index available (Rajeshbabu)

---
 .../apache/phoenix/end2end/index/LocalIndexIT.java |  90 ++++++++++++
 .../TrackOrderPreservingExpressionCompiler.java    |   2 +
 .../phoenix/coprocessor/ScanRegionObserver.java    | 151 +++++++++++++++++++--
 .../DefaultParallelIteratorRegionSplitter.java     |  15 +-
 .../apache/phoenix/iterate/ParallelIterators.java  |   8 +-
 .../apache/phoenix/optimize/QueryOptimizer.java    |  20 ++-
 .../org/apache/phoenix/schema/SaltingUtil.java     |  12 ++
 7 files changed, 280 insertions(+), 18 deletions(-)

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index 6bc1b90..16743e4 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -46,6 +46,7 @@ import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableKey;
 import org.apache.phoenix.schema.TableNotFoundException;
 import org.apache.phoenix.util.MetaDataUtil;
+import org.apache.phoenix.util.QueryUtil;
 import org.apache.phoenix.util.ReadOnlyProps;
 import org.apache.phoenix.util.TestUtil;
 import org.junit.BeforeClass;
@@ -220,4 +221,93 @@ public class LocalIndexIT extends BaseIndexIT {
         }
         indexTable.close();
     }
+
+    @Test
+    public void testLocalIndexScan() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        try{
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,'z')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,'c')");
+            conn1.commit();
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+            
+            ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
+            assertTrue(rs.next());
+            
+            HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+            int numRegions = admin.getTableRegions(TableName.valueOf(DATA_TABLE_NAME)).size();
+            
+            String query = "SELECT t_id, k1, k2,V1 FROM " + DATA_TABLE_NAME +" where v1='a'";
+            rs = conn1.createStatement().executeQuery("EXPLAIN "+ query);
+            
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER "
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME) + " [-32768,'a']",
+                        QueryUtil.getExplainPlan(rs));
+            
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals("f", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertTrue(rs.next());
+            assertEquals("j", rs.getString("t_id"));
+            assertEquals(2, rs.getInt("k1"));
+            assertEquals(4, rs.getInt("k2"));
+            
+            query = "SELECT t_id, k1, k2,V1 from " + DATA_TABLE_FULL_NAME + " order by V1,t_id";
+            rs = conn1.createStatement().executeQuery("EXPLAIN " + query);
+            
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY FULL SCAN OVER "
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)+"\n"
+                        + "    SERVER SORTED BY [V1, T_ID]\n" + "CLIENT MERGE SORT",
+                QueryUtil.getExplainPlan(rs));
+            
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals("f", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals("a", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("j", rs.getString("t_id"));
+            assertEquals(2, rs.getInt("k1"));
+            assertEquals(4, rs.getInt("k2"));
+            assertEquals("a", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("q", rs.getString("t_id"));
+            assertEquals(3, rs.getInt("k1"));
+            assertEquals(1, rs.getInt("k2"));
+            assertEquals("c", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("b", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals("z", rs.getString("V1"));
+        } finally {
+            conn1.close();
+        }
+        
+    }
+
+    @Test
+    public void testIndexPlanSelectionIfBothGlobalAndLocalIndexesHasSameColumnsAndOrder() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,'c')");
+        conn1.commit();
+        conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+        conn1.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + "2" + " ON " + DATA_TABLE_NAME + "(v1)");
+        String query = "SELECT t_id, k1, k2,V1 FROM " + DATA_TABLE_NAME +" where v1='a'";
+        ResultSet rs1 = conn1.createStatement().executeQuery("EXPLAIN "+ query);
+        assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER " + INDEX_TABLE_NAME + "2" + " ['a']",QueryUtil.getExplainPlan(rs1));
+        conn1.close();
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java
index ebf117d..615ee6d 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java
@@ -36,6 +36,7 @@ import org.apache.phoenix.parse.SubtractParseNode;
 import org.apache.phoenix.schema.ColumnRef;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.SortOrder;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.util.SchemaUtil;
 
 import com.google.common.collect.Lists;
@@ -69,6 +70,7 @@ public class TrackOrderPreservingExpressionCompiler extends ExpressionCompiler {
         boolean isSharedViewIndex = table.getViewIndexId() != null;
         // TODO: util for this offset, as it's computed in numerous places
         positionOffset = (isSalted ? 1 : 0) + (isMultiTenant ? 1 : 0) + (isSharedViewIndex ? 1 : 0);
+        this.isOrderPreserving &= table.getIndexType() != IndexType.LOCAL;
         entries = Lists.newArrayListWithExpectedSize(expectedEntrySize);
         this.ordering = ordering;
     }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
index acb9464..c581f64 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
@@ -199,11 +199,11 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
             innerScanner = new HashJoinRegionScanner(s, p, j, tenantId, c.getEnvironment());
         }
         
-        final OrderedResultIterator iterator = deserializeFromScan(scan,innerScanner, offset);
         List<KeyValueColumnExpression> arrayKVRefs = new ArrayList<KeyValueColumnExpression>();
         Expression[] arrayFuncRefs = deserializeArrayPostionalExpressionInfoFromScan(
                 scan, innerScanner, arrayKVRefs);
-        innerScanner = getWrappedScanner(c, innerScanner, arrayKVRefs, arrayFuncRefs);
+        innerScanner = getWrappedScanner(c, innerScanner, arrayKVRefs, arrayFuncRefs, offset);
+        final OrderedResultIterator iterator = deserializeFromScan(scan,innerScanner, offset);
         if (iterator == null) {
             return innerScanner;
         }
@@ -291,9 +291,10 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
      * the same from a custom filter.
      * @param arrayFuncRefs 
      * @param arrayKVRefs 
+     * @param offset starting position in the rowkey.
      */
     private RegionScanner getWrappedScanner(final ObserverContext<RegionCoprocessorEnvironment> c, final RegionScanner s, 
-           final List<KeyValueColumnExpression> arrayKVRefs, final Expression[] arrayFuncRefs) {
+           final List<KeyValueColumnExpression> arrayKVRefs, final Expression[] arrayFuncRefs, final int offset) {
         return new RegionScanner() {
 
             @Override
@@ -345,12 +346,16 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
             public boolean nextRaw(List<Cell> result) throws IOException {
                 try {
                     boolean next = s.nextRaw(result);
-                    if(result.size() == 0) {
-                        return next;
-                    } else if((arrayFuncRefs != null && arrayFuncRefs.length == 0) || arrayKVRefs.size() == 0) {
+                    if (result.size() == 0) {
                         return next;
+                    } 
+                    if (arrayFuncRefs != null && arrayFuncRefs.length > 0 && arrayKVRefs.size() > 0) {
+                        replaceArrayIndexElement(arrayKVRefs, arrayFuncRefs, result);
                     }
-                    replaceArrayIndexElement(arrayKVRefs, arrayFuncRefs, result);
+                    if (offset > 0) {
+                        wrapResultUsingOffset(result,offset);
+                    }
+                    // There is a scanattribute set to retrieve the specific array element
                     return next;
                 } catch (Throwable t) {
                     ServerUtil.throwIOException(c.getEnvironment().getRegion().getRegionNameAsString(), t);
@@ -364,11 +369,14 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
                     boolean next = s.nextRaw(result, limit);
                     if (result.size() == 0) {
                         return next;
-                    } else if ((arrayFuncRefs != null && arrayFuncRefs.length == 0) || arrayKVRefs.size() == 0) { 
-                        return next; 
+                    } 
+                    if (arrayFuncRefs != null && arrayFuncRefs.length > 0 && arrayKVRefs.size() > 0) { 
+                        replaceArrayIndexElement(arrayKVRefs, arrayFuncRefs, result);
+                    }
+                    if (offset > 0) {
+                        wrapResultUsingOffset(result,offset);
                     }
                     // There is a scanattribute set to retrieve the specific array element
-                    replaceArrayIndexElement(arrayKVRefs, arrayFuncRefs, result);
                     return next;
                 } catch (Throwable t) {
                     ServerUtil.throwIOException(c.getEnvironment().getRegion().getRegionNameAsString(), t);
@@ -408,7 +416,128 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
                         QueryConstants.ARRAY_VALUE_COLUMN_QUALIFIER.length, HConstants.LATEST_TIMESTAMP,
                         Type.codeToType(rowKv.getTypeByte()), value, 0, value.length));
             }
-            
+
+            private void wrapResultUsingOffset(List<Cell> result, final int offset) {
+                for (int i = 0; i < result.size(); i++) {
+                    final Cell cell = result.get(i);
+                    // TODO: Create DelegateCell class instead
+                    Cell newCell = new Cell() {
+
+                        @Override
+                        public byte[] getRowArray() {
+                            return cell.getRowArray();
+                        }
+
+                        @Override
+                        public int getRowOffset() {
+                            return cell.getRowOffset() + offset;
+                        }
+
+                        @Override
+                        public short getRowLength() {
+                            return (short)(cell.getRowLength() - offset);
+                        }
+
+                        @Override
+                        public byte[] getFamilyArray() {
+                            return cell.getFamilyArray();
+                        }
+
+                        @Override
+                        public int getFamilyOffset() {
+                            return cell.getFamilyOffset();
+                        }
+
+                        @Override
+                        public byte getFamilyLength() {
+                            return cell.getFamilyLength();
+                        }
+
+                        @Override
+                        public byte[] getQualifierArray() {
+                            return cell.getQualifierArray();
+                        }
+
+                        @Override
+                        public int getQualifierOffset() {
+                            return cell.getQualifierOffset();
+                        }
+
+                        @Override
+                        public int getQualifierLength() {
+                            return cell.getQualifierLength();
+                        }
+
+                        @Override
+                        public long getTimestamp() {
+                            return cell.getTimestamp();
+                        }
+
+                        @Override
+                        public byte getTypeByte() {
+                            return cell.getTypeByte();
+                        }
+
+                        @Override
+                        public long getMvccVersion() {
+                            return cell.getMvccVersion();
+                        }
+
+                        @Override
+                        public byte[] getValueArray() {
+                            return cell.getValueArray();
+                        }
+
+                        @Override
+                        public int getValueOffset() {
+                            return cell.getValueOffset();
+                        }
+
+                        @Override
+                        public int getValueLength() {
+                            return cell.getValueLength();
+                        }
+
+                        @Override
+                        public byte[] getTagsArray() {
+                            return cell.getTagsArray();
+                        }
+
+                        @Override
+                        public int getTagsOffset() {
+                            return cell.getTagsOffset();
+                        }
+
+                        @Override
+                        public short getTagsLength() {
+                            return cell.getTagsLength();
+                        }
+
+                        @Override
+                        public byte[] getValue() {
+                            return cell.getValue();
+                        }
+
+                        @Override
+                        public byte[] getFamily() {
+                            return cell.getFamily();
+                        }
+
+                        @Override
+                        public byte[] getQualifier() {
+                            return cell.getQualifier();
+                        }
+
+                        @Override
+                        public byte[] getRow() {
+                            return cell.getRow();
+                        }
+                    };
+                    // Wrap cell in cell that offsets row key
+                    result.set(i, newCell);
+                }
+            }
+
             @Override
             public long getMaxResultSize() {
                 return s.getMaxResultSize();
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
index 7fb99ff..2a22107 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
@@ -41,7 +41,9 @@ import org.apache.phoenix.query.QueryServices;
 import org.apache.phoenix.query.QueryServicesOptions;
 import org.apache.phoenix.query.StatsManager;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.TableRef;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.util.ReadOnlyProps;
 
 
@@ -84,6 +86,10 @@ public class DefaultParallelIteratorRegionSplitter implements ParallelIteratorRe
         Scan scan = context.getScan();
         PTable table = tableRef.getTable();
         List<HRegionLocation> allTableRegions = context.getConnection().getQueryServices().getAllTableRegions(table.getPhysicalName().getBytes());
+
+        if (table.getType().equals(PTableType.INDEX) && table.getIndexType().equals(IndexType.LOCAL)) {
+            return filterRegions(allTableRegions, HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
+        }
         // If we're not salting, then we've already intersected the minMaxRange with the scan range
         // so there's nothing to do here.
         return filterRegions(allTableRegions, scan.getStartRow(), scan.getStopRow());
@@ -140,7 +146,14 @@ public class DefaultParallelIteratorRegionSplitter implements ParallelIteratorRe
         // distributed across regions, using this scheme compensates for regions that
         // have more rows than others, by applying tighter splits and therefore spawning
         // off more scans over the overloaded regions.
-        int splitsPerRegion = regions.size() >= targetConcurrency ? 1 : (regions.size() > targetConcurrency / 2 ? maxConcurrency : targetConcurrency) / regions.size();
+        PTable table = tableRef.getTable();
+        boolean localIndex = table.getType().equals(PTableType.INDEX) && table.getIndexType().equals(IndexType.LOCAL);
+        int splitsPerRegion;
+        if (localIndex) {
+            splitsPerRegion = 1;
+        } else {
+            splitsPerRegion = regions.size() >= targetConcurrency ? 1 : (regions.size() > targetConcurrency / 2 ? maxConcurrency : targetConcurrency) / regions.size();
+        }
         splitsPerRegion = Math.min(splitsPerRegion, maxIntraRegionParallelization);
         // Create a multi-map of ServerName to List<KeyRange> which we'll use to round robin from to ensure
         // that we keep each region server busy for each query.
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java
index 34373bd..8c26fa8 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java
@@ -38,6 +38,7 @@ import org.apache.phoenix.parse.*;
 import org.apache.phoenix.parse.HintNode.Hint;
 import org.apache.phoenix.query.*;
 import org.apache.phoenix.schema.*;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTable.ViewType;
 import org.apache.phoenix.util.*;
 import org.slf4j.Logger;
@@ -223,6 +224,7 @@ public class ParallelIterators extends ExplainTable implements ResultIterators {
         List<PeekingResultIterator> iterators = new ArrayList<PeekingResultIterator>(numSplits);
         List<Pair<byte[],Future<PeekingResultIterator>>> futures = new ArrayList<Pair<byte[],Future<PeekingResultIterator>>>(numSplits);
         final UUID scanId = UUID.randomUUID();
+        final boolean localIndex = this.tableRef.getTable().getType() == PTableType.INDEX && this.tableRef.getTable().getIndexType() == IndexType.LOCAL;
         try {
             ExecutorService executor = services.getExecutor();
             for (KeyRange split : splits) {
@@ -238,7 +240,11 @@ public class ParallelIterators extends ExplainTable implements ResultIterators {
                         minMaxRange = SaltingUtil.addSaltByte(split.getLowerRange(), minMaxRange);
                         split = split.intersect(minMaxRange);
                     }
-                }
+                } else if (localIndex) {
+                    if (splitScan.getStartRow().length != 0 || splitScan.getStopRow().length != 0) {
+                        SaltingUtil.addRegionStartKeyToScanStartAndStopRows(split.getLowerRange(), splitScan);
+                    }
+                } 
                 if (ScanUtil.intersectScanRange(splitScan, split.getLowerRange(), split.getUpperRange(), this.context.getScanRanges().useSkipScanFilter())) {
                     // Delay the swapping of start/stop row until row so we don't muck with the intersect logic
                     ScanUtil.swapStartStopRowIfReversed(splitScan);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java b/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java
index 65c6596..8341813 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java
@@ -44,6 +44,7 @@ import org.apache.phoenix.schema.PColumn;
 import org.apache.phoenix.schema.PDatum;
 import org.apache.phoenix.schema.PIndexState;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableType;
 
 import com.google.common.collect.Lists;
@@ -285,11 +286,11 @@ public class QueryOptimizer {
                 int c = plan2.getContext().getScanRanges().getRanges().size() - plan1.getContext().getScanRanges().getRanges().size();
                 // Account for potential view constants which are always bound
                 if (plan1 == dataPlan) { // plan2 is index plan. Ignore the viewIndexId if present
-                    c += boundRanges - (table2.getViewIndexId() == null ? 0 : 1);
+                    c += boundRanges - (table2.getViewIndexId() == null || table2.getIndexType() == IndexType.LOCAL ? 0 : 1);
                 } else { // plan1 is index plan. Ignore the viewIndexId if present
-                    c -= boundRanges - (table1.getViewIndexId() == null ? 0 : 1);
+                    c -= boundRanges - (table1.getViewIndexId() == null || table1.getIndexType() == IndexType.LOCAL ? 0 : 1);
                 }
-                if (c != 0) return c;
+                if (c != 0 && table1.getIndexType() != IndexType.LOCAL && table2.getIndexType() != IndexType.LOCAL) return c;
                 if (plan1.getGroupBy()!=null && plan2.getGroupBy()!=null) {
                     if (plan1.getGroupBy().isOrderPreserving() != plan2.getGroupBy().isOrderPreserving()) {
                         return plan1.getGroupBy().isOrderPreserving() ? -1 : 1;
@@ -299,11 +300,20 @@ public class QueryOptimizer {
                 c = (table1.getColumns().size() - table1.getPKColumns().size()) - (table2.getColumns().size() - table2.getPKColumns().size());
                 if (c != 0) return c;
                 
+                // If all things are equal, don't choose local index as it forces scan
+                // on every region.
+                if (table1.getIndexType() == IndexType.LOCAL) {
+                    return 1;
+                }
+                if (table2.getIndexType() == IndexType.LOCAL) {
+                    return -1;
+                }
+
                 // All things being equal, just use the table based on the Hint.USE_DATA_OVER_INDEX_TABLE
-                if (plan1.getTableRef().getTable().getType() == PTableType.INDEX) {
+                if (table1.getType() == PTableType.INDEX) {
                     return comparisonOfDataVersusIndexTable;
                 }
-                if (plan2.getTableRef().getTable().getType() == PTableType.INDEX) {
+                if (table2.getType() == PTableType.INDEX) {
                     return -comparisonOfDataVersusIndexTable;
                 }
                 
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java
index 4a27ef5..cfcad0e 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java
@@ -19,6 +19,7 @@ package org.apache.phoenix.schema;
 
 import java.util.List;
 
+import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.phoenix.query.KeyRange;
 import org.apache.phoenix.schema.RowKeySchema.RowKeySchemaBuilder;
@@ -105,4 +106,15 @@ public class SaltingUtil {
         }
         return KeyRange.getKeyRange(lowerRange, upperRange);
     }
+
+    public static void addRegionStartKeyToScanStartAndStopRows(byte[] startKey, Scan scan) {
+        byte[] newStartRow = new byte[scan.getStartRow().length + startKey.length];
+        System.arraycopy(startKey, 0, newStartRow, 0, startKey.length);
+        System.arraycopy(scan.getStartRow(), 0, newStartRow, startKey.length, scan.getStartRow().length);
+        scan.setStartRow(newStartRow);
+        byte[] newStopRow = new byte[scan.getStopRow().length + startKey.length];
+        System.arraycopy(startKey, 0, newStopRow, 0, startKey.length);
+        System.arraycopy(scan.getStopRow(), 0, newStopRow, startKey.length, scan.getStopRow().length);
+        scan.setStopRow(newStopRow);
+    }
 }
-- 
1.9.4.msysgit.0


From 5a5c6ada1db05e82f23d06a154169cb79958e4fa Mon Sep 17 00:00:00 2001
From: James Taylor <jtaylor@salesforce.com>
Date: Tue, 27 May 2014 12:51:56 -0700
Subject: [PATCH 08/14] PHOENIX-1004 'drop index' should delete index data from
 local index table (Rajeshbabu)

---
 .../apache/phoenix/end2end/index/LocalIndexIT.java | 36 ++++++++++++++++++++++
 .../phoenix/compile/IndexStatementRewriter.java    |  6 ++++
 .../org/apache/phoenix/index/IndexMaintainer.java  | 26 +++++++++++-----
 .../org/apache/phoenix/schema/MetaDataClient.java  | 10 +++---
 4 files changed, 67 insertions(+), 11 deletions(-)

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index 16743e4..c4e935d 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -310,4 +310,40 @@ public class LocalIndexIT extends BaseIndexIT {
         assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER " + INDEX_TABLE_NAME + "2" + " ['a']",QueryUtil.getExplainPlan(rs1));
         conn1.close();
     }
+
+    @Test
+    public void testDropLocalIndexShouldDeleteDataFromLocalIndexTable() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        try {
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,'z')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,'c')");
+            conn1.commit();
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+            conn1.createStatement().execute("DROP INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME);
+            HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+            HTable indexTable = new HTable(admin.getConfiguration() ,TableName.valueOf(MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)));
+            Pair<byte[][], byte[][]> startEndKeys = indexTable.getStartEndKeys();
+            byte[][] startKeys = startEndKeys.getFirst();
+            byte[][] endKeys = startEndKeys.getSecond();
+            // No entry should be present in local index table after drop index.
+            for (int i = 0; i < startKeys.length; i++) {
+                Scan s = new Scan();
+                s.setStartRow(startKeys[i]);
+                s.setStopRow(endKeys[i]);
+                ResultScanner scanner = indexTable.getScanner(s);
+                int count = 0;
+                for(Result r:scanner){
+                    count++;
+                }
+                scanner.close();
+                assertEquals(0, count);
+            }
+            indexTable.close();
+        } finally {
+            conn1.close();
+        }
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
index 68a1218..b6acc65 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
@@ -96,6 +96,12 @@ public class IndexStatementRewriter extends ParseNodeRewriter {
 
         String indexColName = IndexUtil.getIndexColumnName(dataCol);
         // Same alias as before, but use the index column name instead of the data column name
+        // TODO: add dataColRef as an alternate ColumnParseNode in the case that the index
+        // ColumnParseNode cannot be resolved. When this occurs, add the dataColRef to a list
+        // on the StatementContext to indicate that a join back to the data table is necessary.
+        // At ExpressionCompiler.resolveColumn(), test if the table is a local index, and only
+        // then use the alternate. How will the values be resolved on the client? Need to have
+        // a special cf? Or append the values to the PK?
         ParseNode indexColNode = new ColumnParseNode(tName, node.isCaseSensitive() ? '"' + indexColName + '"' : indexColName, node.getAlias());
         PDataType indexColType = IndexUtil.getIndexColumnDataType(dataCol);
         PDataType dataColType = dataColRef.getColumn().getDataType();
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
index c4edf75..3583122 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
@@ -138,16 +138,28 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
      * @param ptr bytes pointer to hold returned serialized value
      */
     public static void serialize(PTable dataTable, ImmutableBytesWritable ptr) {
-        Iterator<PTable> indexes = nonDisabledIndexIterator(dataTable.getIndexes().iterator());
-        if (dataTable.isImmutableRows() || !indexes.hasNext()) {
+        List<PTable> indexes = dataTable.getIndexes();
+        serialize(dataTable, ptr, indexes);
+    }
+
+    /**
+     * For client-side to serialize all IndexMaintainers for a given table
+     * @param dataTable data table
+     * @param ptr bytes pointer to hold returned serialized value
+     * @param indexes indexes to serialize
+     */
+    public static void serialize(PTable dataTable, ImmutableBytesWritable ptr,
+            List<PTable> indexes) {
+        Iterator<PTable> indexesItr = nonDisabledIndexIterator(indexes.iterator());
+        if (dataTable.isImmutableRows() || !indexesItr.hasNext()) {
             ptr.set(ByteUtil.EMPTY_BYTE_ARRAY);
             return;
         }
         int nIndexes = 0;
         int estimatedSize = dataTable.getRowKeySchema().getEstimatedByteSize() + 2;
-        while (indexes.hasNext()) {
+        while (indexesItr.hasNext()) {
             nIndexes++;
-            PTable index = indexes.next();
+            PTable index = indexesItr.next();
             estimatedSize += index.getIndexMaintainer(dataTable).getEstimatedByteSize();
         }
         TrustedByteArrayOutputStream stream = new TrustedByteArrayOutputStream(estimatedSize + 1);
@@ -157,9 +169,9 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
             WritableUtils.writeVInt(output, nIndexes * (dataTable.getBucketNum() == null ? 1 : -1));
             // Write out data row key schema once, since it's the same for all index maintainers
             dataTable.getRowKeySchema().write(output);
-            indexes = nonDisabledIndexIterator(dataTable.getIndexes().iterator());
-            while (indexes.hasNext()) {
-                    indexes.next().getIndexMaintainer(dataTable).write(output);
+            indexesItr = nonDisabledIndexIterator(indexes.iterator());
+            while (indexesItr.hasNext()) {
+                    indexesItr.next().getIndexMaintainer(dataTable).write(output);
             }
         } catch (IOException e) {
             throw new RuntimeException(e); // Impossible
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index 33c1406..449c667 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -101,7 +101,6 @@ import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.exception.SQLExceptionInfo;
 import org.apache.phoenix.execute.MutationState;
 import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
-import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
 import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
@@ -514,7 +513,10 @@ public class MetaDataClient {
                 Scan scan = plan.getContext().getScan();
                 ImmutableBytesWritable ptr = new ImmutableBytesWritable();
                 PTable dataTable = dataTableRef.getTable();
-                dataTable.getIndexMaintainers(ptr);
+                List<PTable> indexes = Lists.newArrayListWithExpectedSize(1);
+                // Only build newly created index.
+                indexes.add(index);
+                IndexMaintainer.serialize(dataTable, ptr, indexes);
                 scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX_BUILD, ByteUtil.copyKeyBytesIfNecessary(ptr));
                 // By default, we'd use a FirstKeyOnly filter as nothing else needs to be projected for count(*).
                 // However, in this case, we need to project all of the data columns that contribute to the index.
@@ -1393,11 +1395,11 @@ public class MetaDataClient {
                         connection.removeTable(tenantId, tableName);
                     } catch (TableNotFoundException ignore) { } // Ignore - just means wasn't cached
                     
-                    // TODO: we need to drop the index data when a view is dropped
-                    boolean dropMetaData = connection.getQueryServices().getProps().getBoolean(DROP_METADATA_ATTRIB, DEFAULT_DROP_METADATA);
                     if (result.getTable() != null && tableType != PTableType.VIEW) {
                         connection.setAutoCommit(true);
                         PTable table = result.getTable();
+                        boolean dropMetaData = result.getTable().getViewIndexId() == null && 
+                                connection.getQueryServices().getProps().getBoolean(DROP_METADATA_ATTRIB, DEFAULT_DROP_METADATA);
                         long ts = (scn == null ? result.getMutationTime() : scn);
                         // Create empty table and schema - they're only used to get the name from
                         // PName name, PTableType type, long timeStamp, long sequenceNumber, List<PColumn> columns
-- 
1.9.4.msysgit.0


From 89bd6b4a0996833b1bd17a0886709403feb84c84 Mon Sep 17 00:00:00 2001
From: James Taylor <jtaylor@salesforce.com>
Date: Mon, 9 Jun 2014 11:11:35 -0700
Subject: [PATCH 09/14] PHOENIX-1038 Dynamically add INDEX_TYPE column to
 SYSTEM.CATALOG if not already there

---
 .../main/java/org/apache/phoenix/coprocessor/MetaDataProtocol.java    | 4 +++-
 1 file changed, 3 insertions(+), 1 deletion(-)

diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataProtocol.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataProtocol.java
index 7131993..ac93804 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataProtocol.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/MetaDataProtocol.java
@@ -59,7 +59,9 @@ public abstract class MetaDataProtocol extends MetaDataService {
             VersionUtil.encodeVersion(PHOENIX_MAJOR_VERSION, PHOENIX_MINOR_VERSION, PHOENIX_PATCH_NUMBER);
     
     public static final long MIN_TABLE_TIMESTAMP = 0;
-    public static final long MIN_SYSTEM_TABLE_TIMESTAMP = MIN_TABLE_TIMESTAMP + 1;
+    // Each time a column is added to the SYSTEM.CATALOG, this should be increased.
+    // Adding INDEX_TYPE column for local indexing
+    public static final long MIN_SYSTEM_TABLE_TIMESTAMP = MIN_TABLE_TIMESTAMP + 2;
     public static final int DEFAULT_MAX_META_DATA_VERSIONS = 1000;
 
     // TODO: pare this down to minimum, as we don't need duplicates for both table and column errors, nor should we need
-- 
1.9.4.msysgit.0


From 4eed25562d3cfce2c12ddebec3b3861e82024046 Mon Sep 17 00:00:00 2001
From: Ramkrishna <ramkrishna.s.vasudevan@intel.com>
Date: Fri, 4 Jul 2014 15:49:41 +0530
Subject: [PATCH 10/14] PHOENIX-1015 Support joining back to data table row
 from local index when query condition involves leading columns in local index
 (Rajeshbabu)

---
 .../org/apache/phoenix/end2end/AlterTableIT.java   |  11 +-
 .../end2end/BaseTenantSpecificViewIndexIT.java     |  95 ++++--
 .../org/apache/phoenix/end2end/BaseViewIT.java     |  46 ++-
 .../java/org/apache/phoenix/end2end/DeleteIT.java  |  78 +++--
 .../org/apache/phoenix/end2end/HashJoinIT.java     | 355 +++++++++++++++++++++
 .../java/org/apache/phoenix/end2end/QueryIT.java   |  54 ++--
 .../org/apache/phoenix/end2end/SaltedViewIT.java   |   7 +-
 .../phoenix/end2end/TenantSpecificViewIndexIT.java |  31 +-
 .../end2end/TenantSpecificViewIndexSaltedIT.java   |  10 +
 .../it/java/org/apache/phoenix/end2end/ViewIT.java |   2 +-
 .../phoenix/end2end/index/ImmutableIndexIT.java    |  71 ++++-
 .../apache/phoenix/end2end/index/LocalIndexIT.java | 243 ++++++++++++--
 .../phoenix/end2end/index/MutableIndexIT.java      | 329 ++++++++++++++++---
 .../apache/phoenix/end2end/index/ViewIndexIT.java  |   2 +-
 .../phoenix/compile/CreateIndexCompiler.java       |   4 +-
 .../org/apache/phoenix/compile/DeleteCompiler.java |   3 +-
 .../apache/phoenix/compile/ExpressionCompiler.java |  21 +-
 .../org/apache/phoenix/compile/FromCompiler.java   |   2 +-
 .../phoenix/compile/IndexStatementRewriter.java    |   1 -
 .../org/apache/phoenix/compile/JoinCompiler.java   |  11 +-
 .../apache/phoenix/compile/StatementContext.java   |  38 +++
 .../TrackOrderPreservingExpressionCompiler.java    |   1 -
 .../org/apache/phoenix/compile/WhereCompiler.java  |  11 +
 .../coprocessor/BaseScannerRegionObserver.java     |   3 +
 .../GroupedAggregateRegionObserver.java            |  54 +++-
 .../phoenix/coprocessor/ScanRegionObserver.java    | 175 +++-------
 .../UngroupedAggregateRegionObserver.java          |  56 ++--
 .../org/apache/phoenix/execute/BasicQueryPlan.java | 148 +++++++++
 .../java/org/apache/phoenix/execute/ScanPlan.java  |   3 +-
 .../apache/phoenix/expression/ExpressionType.java  |   2 +-
 .../expression/KeyValueColumnExpression.java       |   7 +
 .../expression/ProjectedColumnExpression.java      |  25 +-
 .../org/apache/phoenix/filter/SkipScanFilter.java  |   2 +-
 .../hbase/index/util/IndexManagementUtil.java      |   4 +-
 .../org/apache/phoenix/index/IndexMaintainer.java  | 317 ++++++++++++++++--
 .../apache/phoenix/index/PhoenixIndexCodec.java    |   4 +-
 .../apache/phoenix/iterate/ParallelIterators.java  |   3 +-
 .../SkipRangeParallelIteratorRegionSplitter.java   |   4 +-
 .../org/apache/phoenix/join/TupleProjector.java    |  16 +-
 .../apache/phoenix/optimize/QueryOptimizer.java    |  20 +-
 .../apache/phoenix/parse/ParseNodeRewriter.java    |   7 +-
 .../phoenix/query/ConnectionQueryServicesImpl.java |   4 +-
 .../java/org/apache/phoenix/schema/ColumnRef.java  |  19 +-
 .../phoenix/schema/LocalIndexDataColumnRef.java    |  55 ++++
 .../org/apache/phoenix/schema/MetaDataClient.java  |  57 +++-
 .../org/apache/phoenix/schema/SaltingUtil.java     |  20 +-
 .../org/apache/phoenix/schema/ValueSchema.java     |  20 +-
 .../java/org/apache/phoenix/util/IndexUtil.java    | 268 +++++++++++++++-
 .../java/org/apache/phoenix/util/MetaDataUtil.java |   8 +
 .../java/org/apache/phoenix/util/ScanUtil.java     |  18 +-
 .../apache/phoenix/index/IndexMaintainerTest.java  |   8 +-
 51 files changed, 2324 insertions(+), 429 deletions(-)
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/schema/LocalIndexDataColumnRef.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/AlterTableIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/AlterTableIT.java
index b822108..cdf405b 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/AlterTableIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/AlterTableIT.java
@@ -54,9 +54,10 @@ public class AlterTableIT extends BaseHBaseManagedTimeIT {
     public static final String SCHEMA_NAME = "";
     public static final String DATA_TABLE_NAME = "T";
     public static final String INDEX_TABLE_NAME = "I";
+    public static final String LOCAL_INDEX_TABLE_NAME = "LI";
     public static final String DATA_TABLE_FULL_NAME = SchemaUtil.getTableName(SCHEMA_NAME, "T");
     public static final String INDEX_TABLE_FULL_NAME = SchemaUtil.getTableName(SCHEMA_NAME, "I");
-
+    public static final String LOCAL_INDEX_TABLE_FULL_NAME = SchemaUtil.getTableName(SCHEMA_NAME, "LI");
 
     @Test
     public void testAlterTableWithVarBinaryKey() throws Exception {
@@ -208,6 +209,9 @@ public class AlterTableIT extends BaseHBaseManagedTimeIT {
     
         conn.createStatement().execute(
           "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        conn.createStatement().execute(
+            "CREATE LOCAL INDEX " + LOCAL_INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -266,9 +270,14 @@ public class AlterTableIT extends BaseHBaseManagedTimeIT {
     
         conn.createStatement().execute(
           "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1) include (v2, v3)");
+        conn.createStatement().execute(
+            "CREATE LOCAL INDEX " + LOCAL_INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1) include (v2, v3)");
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
+        query = "SELECT * FROM " + LOCAL_INDEX_TABLE_FULL_NAME;
+        rs = conn.createStatement().executeQuery(query);
+        assertFalse(rs.next());
     
         // load some data into the table
         stmt = conn.prepareStatement("UPSERT INTO " + DATA_TABLE_FULL_NAME + " VALUES(?,?,?,?)");
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java
index 5878806..f9bb50a 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java
@@ -26,6 +26,8 @@ import java.sql.Connection;
 import java.sql.DriverManager;
 import java.sql.ResultSet;
 import java.sql.SQLException;
+import java.util.Arrays;
+import java.util.List;
 import java.util.Properties;
 import java.util.Set;
 
@@ -33,6 +35,8 @@ import org.apache.hadoop.hbase.util.Pair;
 import org.apache.phoenix.util.PhoenixRuntime;
 import org.apache.phoenix.util.QueryUtil;
 
+import com.google.common.collect.Lists;
+
 public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
     
     public static final String TENANT1_ID = "tenant1";
@@ -41,11 +45,15 @@ public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
     protected Set<Pair<String, String>> tenantViewsToDelete = newHashSet();
     
     protected void testUpdatableView(Integer saltBuckets) throws Exception {
+        testUpdatableView(saltBuckets, false);
+    }
+    
+    protected void testUpdatableView(Integer saltBuckets, boolean localIndex) throws Exception {
         createBaseTable("t", saltBuckets);
         Connection conn = createTenantConnection(TENANT1_ID);
         try {
             createAndPopulateTenantView(conn, TENANT1_ID, "t", "");
-            createAndVerifyIndex(conn, saltBuckets, TENANT1_ID, "");
+            createAndVerifyIndex(conn, saltBuckets, TENANT1_ID, "", localIndex);
             verifyViewData(conn, "");
         } finally {
             try { conn.close();} catch (Exception ignored) {}
@@ -53,6 +61,10 @@ public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
     }
     
     protected void testUpdatableViewsWithSameNameDifferentTenants(Integer saltBuckets) throws Exception {
+        testUpdatableViewsWithSameNameDifferentTenants(saltBuckets, false);
+    }
+
+    protected void testUpdatableViewsWithSameNameDifferentTenants(Integer saltBuckets, boolean localIndex) throws Exception {
         createBaseTable("t", saltBuckets);
         Connection conn1 = createTenantConnection(TENANT1_ID);
         Connection conn2 = createTenantConnection(TENANT2_ID);
@@ -64,8 +76,8 @@ public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
             createAndPopulateTenantView(conn1, TENANT1_ID, "t", prefixForTenant1Data);
             createAndPopulateTenantView(conn2, TENANT2_ID, "t", prefixForTenant2Data);
             
-            createAndVerifyIndex(conn1, saltBuckets, TENANT1_ID, prefixForTenant1Data);
-            createAndVerifyIndex(conn2, saltBuckets, TENANT2_ID, prefixForTenant2Data);
+            createAndVerifyIndex(conn1, saltBuckets, TENANT1_ID, prefixForTenant1Data, localIndex);
+            createAndVerifyIndex(conn2, saltBuckets, TENANT2_ID, prefixForTenant2Data, localIndex);
             
             verifyViewData(conn1, prefixForTenant1Data);
             verifyViewData(conn2, prefixForTenant2Data);
@@ -97,17 +109,26 @@ public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
         conn.commit();
     }
     
-    private void createAndVerifyIndex(Connection conn, Integer saltBuckets, String tenantId, String valuePrefix) throws SQLException {
-        conn.createStatement().execute("CREATE INDEX i ON v(v2)");
+    private void createAndVerifyIndex(Connection conn, Integer saltBuckets, String tenantId, String valuePrefix, boolean localIndex) throws SQLException {
+        if(localIndex){
+            conn.createStatement().execute("CREATE LOCAL INDEX i ON v(v2)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX i ON v(v2)");
+        }
         conn.createStatement().execute("UPSERT INTO v(k2,v1,v2) VALUES (-1, 'blah', 'superblah')"); // sanity check that we can upsert after index is there
         conn.commit();
         ResultSet rs = conn.createStatement().executeQuery("EXPLAIN SELECT k1, k2, v2 FROM v WHERE v2='" + valuePrefix + "v2-1'");
-        
-        String expected = saltBuckets == null ? 
-                "RANGE SCAN OVER _IDX_T ['" + tenantId + "',-32768,'" + valuePrefix + "v2-1']" :
-                "SKIP SCAN ON 3 KEYS OVER _IDX_T [0,'" + tenantId + "',-32768,'" + valuePrefix + "v2-1'] - [2,'" + tenantId + "',-32768,'" + valuePrefix + "v2-1']\n" + 
-                "CLIENT MERGE SORT";
-        assertTrue(QueryUtil.getExplainPlan(rs).contains(expected));
+        if(localIndex){
+            assertEquals(saltBuckets == null ? 
+                    "CLIENT PARALLEL 1-WAY RANGE SCAN OVER _LOCAL_IDX_T ['" + tenantId + "',-32768,'" + valuePrefix + "v2-1']\nCLIENT MERGE SORT" :
+                        "CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T ['" + tenantId + "',-32768,'" + valuePrefix + "v2-1']\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        } else {
+            String expected = saltBuckets == null ? 
+                    "RANGE SCAN OVER _IDX_T ['" + tenantId + "',-32768,'" + valuePrefix + "v2-1']" :
+                    "SKIP SCAN ON 3 KEYS OVER _IDX_T [0,'" + tenantId + "',-32768,'" + valuePrefix + "v2-1'] - [2,'" + tenantId + "',-32768,'" + valuePrefix + "v2-1']\n" + 
+                    "CLIENT MERGE SORT";
+            assertTrue(QueryUtil.getExplainPlan(rs).contains(expected));
+        }
     }
     
     private Connection createTenantConnection(String tenantId) throws SQLException {
@@ -119,26 +140,38 @@ public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
     private void verifyViewData(Connection conn, String valuePrefix) throws SQLException {
         String query = "SELECT k1, k2, v2 FROM v WHERE v2='" + valuePrefix + "v2-1'";
         ResultSet rs = conn.createStatement().executeQuery(query);
-        assertTrue(rs.next());
-        assertEquals(1, rs.getInt(1));
-        assertEquals(1, rs.getInt(2));
-        assertEquals(valuePrefix + "v2-1", rs.getString(3));
-        assertTrue(rs.next());
-        assertEquals(1, rs.getInt(1));
-        assertEquals(3, rs.getInt(2));
-        assertEquals(valuePrefix + "v2-1", rs.getString(3));
-        assertTrue(rs.next());
-        assertEquals(1, rs.getInt(1));
-        assertEquals(5, rs.getInt(2));
-        assertEquals(valuePrefix + "v2-1", rs.getString(3));
-        assertTrue(rs.next());
-        assertEquals(1, rs.getInt(1));
-        assertEquals(7, rs.getInt(2));
-        assertEquals(valuePrefix + "v2-1", rs.getString(3));
-        assertTrue(rs.next());
-        assertEquals(1, rs.getInt(1));
-        assertEquals(9, rs.getInt(2));
-        assertEquals(valuePrefix + "v2-1", rs.getString(3));
+        List<List<Object>> expectedResultsA = Lists.newArrayList(
+            Arrays.<Object>asList(1,1, valuePrefix + "v2-1"),
+            Arrays.<Object>asList(1,3, valuePrefix + "v2-1"),
+            Arrays.<Object>asList(1,5, valuePrefix + "v2-1"),
+            Arrays.<Object>asList(1,7, valuePrefix + "v2-1"),
+            Arrays.<Object>asList(1,9, valuePrefix + "v2-1"));
+        assertValuesEqualsResultSet(rs,expectedResultsA);
         assertFalse(rs.next());
     }
+
+    /**
+     * Asserts that we find the expected values in the result set. We don't know the order, since we don't always
+     * have an order by and we're going through indexes, but we assert that each expected result occurs once as
+     * expected (in any order).
+     */
+    private void assertValuesEqualsResultSet(ResultSet rs, List<List<Object>> expectedResults) throws SQLException {
+        int expectedCount = expectedResults.size();
+        int count = 0;
+        List<List<Object>> actualResults = Lists.newArrayList();
+        List<Object> errorResult = null;
+        while (rs.next() && errorResult == null) {
+            List<Object> result = Lists.newArrayList();
+            for (int i = 0; i < rs.getMetaData().getColumnCount(); i++) {
+                result.add(rs.getObject(i+1));
+            }
+            if (!expectedResults.contains(result)) {
+                errorResult = result;
+            }
+            actualResults.add(result);
+            count++;
+        }
+        assertTrue("Could not find " + errorResult + " in expected results: " + expectedResults + " with actual results: " + actualResults, errorResult == null);
+        assertEquals(count, expectedCount);
+    }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java
index b125ab2..78d86db 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java
@@ -48,9 +48,9 @@ public class BaseViewIT extends BaseHBaseManagedTimeIT {
         setUpTestDriver(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
     }
 
-    protected void testUpdatableViewWithIndex(Integer saltBuckets) throws Exception {
+    protected void testUpdatableViewWithIndex(Integer saltBuckets, boolean localIndex) throws Exception {
         testUpdatableView(saltBuckets);
-        testUpdatableViewIndex(saltBuckets);
+        testUpdatableViewIndex(saltBuckets, localIndex);
     }
 
     protected void testUpdatableView(Integer saltBuckets) throws Exception {
@@ -100,9 +100,18 @@ public class BaseViewIT extends BaseHBaseManagedTimeIT {
     }
 
     protected void testUpdatableViewIndex(Integer saltBuckets) throws Exception {
+        testUpdatableViewIndex(saltBuckets, false);
+    }
+
+    protected void testUpdatableViewIndex(Integer saltBuckets, boolean localIndex) throws Exception {
         ResultSet rs;
         Connection conn = DriverManager.getConnection(getUrl());
-        conn.createStatement().execute("CREATE INDEX i1 on v(k3) include (s)");
+        if (localIndex) {
+            conn.createStatement().execute("CREATE LOCAL INDEX i1 on v(k3)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX i1 on v(k3) include (s)");
+        }
+        conn.createStatement().execute("UPSERT INTO v(k2,S,k3) VALUES(120,'foo',50.0)");
         String query = "SELECT k1, k2, k3, s FROM v WHERE k3 = 51.0";
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -112,12 +121,21 @@ public class BaseViewIT extends BaseHBaseManagedTimeIT {
         assertEquals("bar", rs.getString(4));
         assertFalse(rs.next());
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals(saltBuckets == null
-                ? "CLIENT PARALLEL 1-WAY RANGE SCAN OVER _IDX_T [" + Short.MIN_VALUE + ",51]"
-                : "CLIENT PARALLEL " + saltBuckets + "-WAY SKIP SCAN ON 3 KEYS OVER _IDX_T [0," + Short.MIN_VALUE + ",51] - [2," + Short.MIN_VALUE + ",51]\nCLIENT MERGE SORT",
-            QueryUtil.getExplainPlan(rs));
+        if (localIndex) {
+            assertEquals("CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T [-32768,51]",
+                QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals(saltBuckets == null
+                    ? "CLIENT PARALLEL 1-WAY RANGE SCAN OVER _IDX_T [" + Short.MIN_VALUE + ",51]"
+                            : "CLIENT PARALLEL " + saltBuckets + "-WAY SKIP SCAN ON 3 KEYS OVER _IDX_T [0," + Short.MIN_VALUE + ",51] - [2," + Short.MIN_VALUE + ",51]\nCLIENT MERGE SORT",
+                            QueryUtil.getExplainPlan(rs));
+        }
 
-        conn.createStatement().execute("CREATE INDEX i2 on v(s)");
+        if (localIndex) {
+            conn.createStatement().execute("CREATE LOCAL INDEX i2 on v(s)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX i2 on v(s)");
+        }
         query = "SELECT k1, k2, s FROM v WHERE s = 'foo'";
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -126,10 +144,14 @@ public class BaseViewIT extends BaseHBaseManagedTimeIT {
         assertEquals("foo", rs.getString(3));
         assertFalse(rs.next());
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals(saltBuckets == null
-                ? "CLIENT PARALLEL 1-WAY RANGE SCAN OVER _IDX_T [" + (Short.MIN_VALUE+1) + ",'foo']"
-                : "CLIENT PARALLEL " + saltBuckets + "-WAY SKIP SCAN ON 3 KEYS OVER _IDX_T [0," + (Short.MIN_VALUE+1) + ",'foo'] - [2," + (Short.MIN_VALUE+1) + ",'foo']\nCLIENT MERGE SORT",
-            QueryUtil.getExplainPlan(rs));
+        if (localIndex) {
+            assertEquals("CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T [" + (Short.MIN_VALUE+1) + ",'foo']",QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals(saltBuckets == null
+                    ? "CLIENT PARALLEL 1-WAY RANGE SCAN OVER _IDX_T [" + (Short.MIN_VALUE+1) + ",'foo']"
+                            : "CLIENT PARALLEL " + saltBuckets + "-WAY SKIP SCAN ON 3 KEYS OVER _IDX_T [0," + (Short.MIN_VALUE+1) + ",'foo'] - [2," + (Short.MIN_VALUE+1) + ",'foo']\nCLIENT MERGE SORT",
+                            QueryUtil.getExplainPlan(rs));
+        }
     }
 
 
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/DeleteIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/DeleteIT.java
index 4d41141..337e49b 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/DeleteIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/DeleteIT.java
@@ -32,6 +32,7 @@ import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
 
+import org.apache.phoenix.util.MetaDataUtil;
 import org.apache.phoenix.util.QueryUtil;
 import org.junit.Test;
 import org.junit.experimental.categories.Category;
@@ -147,14 +148,23 @@ public class DeleteIT extends BaseHBaseManagedTimeIT {
             String explainPlan = QueryUtil.getExplainPlan(rs);
             assertEquals(expectedToBeUsed, explainPlan.contains(" SCAN OVER " + indexName));
    }
-    
+
     private void testDeleteRange(boolean autoCommit, boolean createIndex) throws Exception {
+        testDeleteRange(autoCommit, createIndex, false);
+    }
+
+    private void testDeleteRange(boolean autoCommit, boolean createIndex, boolean local) throws Exception {
         Connection conn = DriverManager.getConnection(getUrl());
         initTableValues(conn);
         
         String indexName = "IDX";
         if (createIndex) {
-            conn.createStatement().execute("CREATE INDEX IF NOT EXISTS idx ON IntIntKeyTest(j)");
+            if (local) {
+                conn.createStatement().execute("CREATE LOCAL INDEX IF NOT EXISTS local_idx ON IntIntKeyTest(j)");
+                indexName = MetaDataUtil.getLocalIndexTableName("INTINTKEYTEST");
+            } else {
+                conn.createStatement().execute("CREATE INDEX IF NOT EXISTS idx ON IntIntKeyTest(j)");
+            }
         }
         
         ResultSet rs;
@@ -217,17 +227,32 @@ public class DeleteIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testDeleteRangeNoAutoCommitWithIndex() throws Exception {
-        testDeleteRange(false, true);
+        testDeleteRange(false, true, false);
+    }
+
+    @Test
+    public void testDeleteRangeNoAutoCommitWithLocalIndexIndex() throws Exception {
+        testDeleteRange(false, true, true);
     }
     
     @Test
     public void testDeleteRangeAutoCommitWithIndex() throws Exception {
-        testDeleteRange(true, true);
+        testDeleteRange(true, true, false);
     }
     
     @Test
+    public void testDeleteRangeAutoCommitWithLocalIndex() throws Exception {
+        testDeleteRange(true, true, true);
+    }
+
+    @Test
     public void testDeleteAllFromTableWithIndexAutoCommitSalting() throws SQLException {
-        testDeleteAllFromTableWithIndex(true, true);
+        testDeleteAllFromTableWithIndex(true, true, false);
+    }
+
+    @Test
+    public void testDeleteAllFromTableWithLocalIndexAutoCommitSalting() throws SQLException {
+        testDeleteAllFromTableWithIndex(true, true, true);
     }
     
     @Test
@@ -242,26 +267,40 @@ public class DeleteIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testDeleteAllFromTableWithIndexNoAutoCommitSalted() throws SQLException {
-        testDeleteAllFromTableWithIndex(false, true);
+        testDeleteAllFromTableWithIndex(false, true, false);
     }
     
+    @Test
+    public void testDeleteAllFromTableWithLocalIndexNoAutoCommitSalted() throws SQLException {
+        testDeleteAllFromTableWithIndex(false, true, true);
+    }
+
     private void testDeleteAllFromTableWithIndex(boolean autoCommit, boolean isSalted) throws SQLException {
+        testDeleteAllFromTableWithIndex(autoCommit, isSalted, false);
+    }
+
+    private void testDeleteAllFromTableWithIndex(boolean autoCommit, boolean isSalted, boolean localIndex) throws SQLException {
         Connection con = null;
         try {
             con = DriverManager.getConnection(getUrl());
             con.setAutoCommit(autoCommit);
 
             Statement stm = con.createStatement();
-            stm.execute("CREATE TABLE IF NOT EXISTS web_stats (" +
-            		"HOST CHAR(2) NOT NULL," +
-            		"DOMAIN VARCHAR NOT NULL, " +
-            		"FEATURE VARCHAR NOT NULL, " +
-            		"DATE DATE NOT NULL, \n" + 
-            		"USAGE.CORE BIGINT," +
-            		"USAGE.DB BIGINT," +
-            		"STATS.ACTIVE_VISITOR INTEGER " +
-            		"CONSTRAINT PK PRIMARY KEY (HOST, DOMAIN, FEATURE, DATE))" + (isSalted ? " SALT_BUCKETS=3" : ""));
-            stm.execute("CREATE INDEX web_stats_idx ON web_stats (CORE,DB,ACTIVE_VISITOR)");
+            String s = "CREATE TABLE IF NOT EXISTS web_stats (" +
+                    "HOST CHAR(2) NOT NULL," +
+                    "DOMAIN VARCHAR NOT NULL, " +
+                    "FEATURE VARCHAR NOT NULL, " +
+                    "DATE DATE NOT NULL, \n" + 
+                    "USAGE.CORE BIGINT," +
+                    "USAGE.DB BIGINT," +
+                    "STATS.ACTIVE_VISITOR INTEGER " +
+                    "CONSTRAINT PK PRIMARY KEY (HOST, DOMAIN, FEATURE, DATE))" + (isSalted ? " SALT_BUCKETS=3" : "");
+            stm.execute(s);
+            if (localIndex) {
+                stm.execute("CREATE LOCAL INDEX local_web_stats_idx ON web_stats (CORE,DB,ACTIVE_VISITOR)");
+            } else {
+                stm.execute("CREATE INDEX web_stats_idx ON web_stats (CORE,DB,ACTIVE_VISITOR)");
+            }
             stm.close();
 
             PreparedStatement psInsert = con
@@ -287,8 +326,11 @@ public class DeleteIT extends BaseHBaseManagedTimeIT {
             ResultSet rs = con.createStatement().executeQuery("SELECT /*+ NO_INDEX */ count(*) FROM web_stats");
             assertTrue(rs.next());
             assertEquals(0, rs.getLong(1));
-
-            rs = con.createStatement().executeQuery("SELECT count(*) FROM web_stats_idx");
+            if(localIndex){
+                rs = con.createStatement().executeQuery("SELECT count(*) FROM local_web_stats_idx");
+            } else {
+                rs = con.createStatement().executeQuery("SELECT count(*) FROM web_stats_idx");
+            }
             assertTrue(rs.next());
             assertEquals(0, rs.getLong(1));
 
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java
index 26c9455..ebd018e 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java
@@ -51,6 +51,7 @@ import java.util.Properties;
 import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.query.QueryServices;
 import org.apache.phoenix.schema.TableAlreadyExistsException;
+import org.apache.phoenix.util.MetaDataUtil;
 import org.apache.phoenix.util.QueryUtil;
 import org.apache.phoenix.util.ReadOnlyProps;
 import org.junit.Before;
@@ -777,6 +778,360 @@ public class HashJoinIT extends BaseHBaseManagedTimeIT {
                 "        CLIENT PARALLEL 1-WAY FULL SCAN OVER "+ JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
                 "    JOIN-SCANNER 4 ROW LIMIT",
                 }});
+        testCases.add(new String[][] {
+                {
+                "CREATE LOCAL INDEX \"idx_customer\" ON " + JOIN_CUSTOMER_TABLE_FULL_NAME + " (name)",
+                "CREATE LOCAL INDEX \"idx_item\" ON " + JOIN_ITEM_TABLE_FULL_NAME + " (name) INCLUDE (price, discount1, discount2, \"supplier_id\", description)",
+                "CREATE LOCAL INDEX \"idx_supplier\" ON " + JOIN_SUPPLIER_TABLE_FULL_NAME + " (name)"
+                }, {
+                /* 
+                 * testLeftJoinWithAggregation()
+                 *     SELECT i.name, sum(quantity) FROM joinOrderTable o 
+                 *     LEFT JOIN joinItemTable i ON o.item_id = i.item_id 
+                 *     GROUP BY i.name ORDER BY i.name
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER AGGREGATE INTO DISTINCT ROWS BY [I.0:NAME]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [I.0:NAME]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME +"\n" +
+                "            SERVER FILTER BY FIRST KEY ONLY\n" +
+                "        CLIENT MERGE SORT",
+                /* 
+                 * testLeftJoinWithAggregation()
+                 *     SELECT i.item_id iid, sum(quantity) q FROM joinOrderTable o 
+                 *     LEFT JOIN joinItemTable i ON o.item_id = i.item_id 
+                 *     GROUP BY i.item_id ORDER BY q DESC"
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER AGGREGATE INTO DISTINCT ROWS BY [I.:item_id]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [SUM(O.QUANTITY) DESC]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME +"\n" +
+                "            SERVER FILTER BY FIRST KEY ONLY\n" +
+                "        CLIENT MERGE SORT",          
+                /* 
+                 * testLeftJoinWithAggregation()
+                 *     SELECT i.item_id iid, sum(quantity) q FROM joinItemTable i 
+                 *     LEFT JOIN joinOrderTable o ON o.item_id = i.item_id 
+                 *     GROUP BY i.item_id ORDER BY q DESC NULLS LAST, iid
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER AGGREGATE INTO ORDERED DISTINCT ROWS BY [I.item_id]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [SUM(O.QUANTITY) DESC NULLS LAST, I.item_id]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME,
+                /* 
+                 * testRightJoinWithAggregation()
+                 *     SELECT i.name, sum(quantity) FROM joinOrderTable o 
+                 *     RIGHT JOIN joinItemTable i ON o.item_id = i.item_id 
+                 *     GROUP BY i.name ORDER BY i.name
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME+"\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER AGGREGATE INTO DISTINCT ROWS BY [I.0:NAME]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [I.0:NAME]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME,
+                /*
+                 * testRightJoinWithAggregation()
+                 *     SELECT i.item_id iid, sum(quantity) q FROM joinOrderTable o 
+                 *     RIGHT JOIN joinItemTable i ON o.item_id = i.item_id 
+                 *     GROUP BY i.item_id ORDER BY q DESC NULLS LAST, iid
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER AGGREGATE INTO ORDERED DISTINCT ROWS BY [I.item_id]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [SUM(O.QUANTITY) DESC NULLS LAST, I.item_id]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME,
+                /*
+                 * testJoinWithWildcard()
+                 *     SELECT * FROM joinItemTable LEFT JOIN joinSupplierTable supp 
+                 *     ON joinItemTable.supplier_id = supp.supplier_id 
+                 *     ORDER BY item_id
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_SUPPLIER_TABLE_DISPLAY_NAME,
+                /*
+                 * testJoinPlanWithIndex()
+                 *     SELECT item.item_id, item.name, supp.supplier_id, supp.name 
+                 *     FROM joinItemTable item LEFT JOIN joinSupplierTable supp 
+                 *     ON substr(item.name, 2, 1) = substr(supp.name, 2, 1) 
+                 *         AND (supp.name BETWEEN 'S1' AND 'S5') 
+                 *     WHERE item.name BETWEEN 'T1' AND 'T5'
+                 */
+                "CLIENT PARALLEL 1-WAY RANGE SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME + " [-32768,'T1'] - [-32768,'T5']\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY RANGE SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_SUPPLIER_TABLE_DISPLAY_NAME +" [-32768,'S1'] - [-32768,'S5']\n" +
+                "        CLIENT MERGE SORT",
+                /*
+                 * testJoinPlanWithIndex()
+                 *     SELECT item.item_id, item.name, supp.supplier_id, supp.name 
+                 *     FROM joinItemTable item INNER JOIN joinSupplierTable supp 
+                 *     ON item.supplier_id = supp.supplier_id 
+                 *     WHERE (item.name = 'T1' OR item.name = 'T5') 
+                 *         AND (supp.name = 'S1' OR supp.name = 'S5')
+                 */
+                "CLIENT PARALLEL 1-WAY SKIP SCAN ON 2 KEYS OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME + " [-32768,'T1'] - [-32768,'T5']\n" +
+                "CLIENT MERGE SORT\n" + 
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY SKIP SCAN ON 2 KEYS OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_SUPPLIER_TABLE_DISPLAY_NAME +" [-32768,'S1'] - [-32768,'S5']\n" + 
+                "        CLIENT MERGE SORT",
+                /*
+                 * testJoinWithSkipMergeOptimization()
+                 *     SELECT s.name FROM joinItemTable i 
+                 *     JOIN joinOrderTable o ON o.item_id = i.item_id AND quantity < 5000 
+                 *     JOIN joinSupplierTable s ON i.supplier_id = s.supplier_id
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "CLIENT MERGE SORT\n" + 
+                "    PARALLEL EQUI-JOIN 2 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0 (SKIP MERGE)\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER FILTER BY QUANTITY < 5000\n" +
+                "    BUILD HASH TABLE 1\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_SUPPLIER_TABLE_DISPLAY_NAME + "\n" +
+                "        CLIENT MERGE SORT",
+                /*
+                 * testSelfJoin()
+                 *     SELECT i2.item_id, i1.name FROM joinItemTable i1 
+                 *     JOIN joinItemTable i2 ON i1.item_id = i2.item_id 
+                 *     ORDER BY i1.item_id
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER "+ MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+ JOIN_ITEM_TABLE_DISPLAY_NAME +"\n"  +
+                "            SERVER FILTER BY FIRST KEY ONLY\n" +
+                "        CLIENT MERGE SORT",
+                /*
+                 * testSelfJoin()
+                 *     SELECT i1.name, i2.name FROM joinItemTable i1 
+                 *     JOIN joinItemTable i2 ON i1.item_id = i2.supplier_id 
+                 *     ORDER BY i1.name, i2.name
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+ JOIN_ITEM_TABLE_DISPLAY_NAME +"\n"  +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+ JOIN_ITEM_TABLE_DISPLAY_NAME +"\n" +
+                "        CLIENT MERGE SORT",
+                /*
+                 * testStarJoin()
+                 *     SELECT order_id, c.name, i.name iname, quantity, o.date 
+                 *     FROM joinOrderTable o 
+                 *     JOIN joinCustomerTable c ON o.customer_id = c.customer_id 
+                 *     JOIN joinItemTable i ON o.item_id = i.item_id 
+                 *     ORDER BY order_id
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    PARALLEL EQUI-JOIN 2 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_CUSTOMER_TABLE_DISPLAY_NAME + "\n" +
+                "        CLIENT MERGE SORT\n" +
+                "    BUILD HASH TABLE 1\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER FILTER BY FIRST KEY ONLY\n" +
+                "        CLIENT MERGE SORT",
+                /*
+                 * testStarJoin()
+                 *     SELECT (*NO_STAR_JOIN*) order_id, c.name, i.name iname, quantity, o.date 
+                 *     FROM joinOrderTable o 
+                 *     JOIN joinCustomerTable c ON o.customer_id = c.customer_id 
+                 *     JOIN joinItemTable i ON o.item_id = i.item_id 
+                 *     ORDER BY order_id
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "            PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "            BUILD HASH TABLE 0\n" +
+                "                CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_CUSTOMER_TABLE_DISPLAY_NAME+"\n"+
+                "                CLIENT MERGE SORT",
+                /*
+                 * testSubJoin()
+                 *     SELECT * FROM joinCustomerTable c 
+                 *     INNER JOIN (joinOrderTable o 
+                 *         INNER JOIN (joinSupplierTable s 
+                 *             RIGHT JOIN joinItemTable i ON i.supplier_id = s.supplier_id)
+                 *         ON o.item_id = i.item_id)
+                 *     ON c.customer_id = o.customer_id
+                 *     WHERE c.customer_id <= '0000000005' 
+                 *         AND order_id != '000000000000003' 
+                 *         AND i.name != 'T3' 
+                 *     ORDER BY c.customer_id, i.name
+                 */
+                "CLIENT PARALLEL 1-WAY RANGE SCAN OVER " + JOIN_CUSTOMER_TABLE_DISPLAY_NAME + " [*] - ['0000000005']\n" +
+                "    SERVER SORTED BY [C.customer_id, I.0:NAME]\n"+
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER FILTER BY order_id != '000000000000003'\n" +
+                "            PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "            BUILD HASH TABLE 0\n" +
+                "                CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+ JOIN_ITEM_TABLE_DISPLAY_NAME +"\n" +
+                "                    SERVER FILTER BY NAME != 'T3'\n" +
+                "                CLIENT MERGE SORT\n" +
+                "                    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "                    BUILD HASH TABLE 0\n" +
+                "                        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_SUPPLIER_TABLE_DISPLAY_NAME,
+                /* 
+                 * testJoinWithSubqueryAndAggregation()
+                 *     SELECT i.name, sum(quantity) FROM joinOrderTable o 
+                 *     LEFT JOIN (SELECT name, item_id iid FROM joinItemTable) AS i 
+                 *     ON o.item_id = i.iid 
+                 *     GROUP BY i.name ORDER BY i.name
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER AGGREGATE INTO DISTINCT ROWS BY [I.NAME]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [I.NAME]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME+"\n"+
+                "            SERVER FILTER BY FIRST KEY ONLY\n" +
+                "        CLIENT MERGE SORT",
+                /* 
+                 * testJoinWithSubqueryAndAggregation()
+                 *     SELECT o.iid, sum(o.quantity) q 
+                 *     FROM (SELECT item_id iid, quantity FROM joinOrderTable) AS o 
+                 *     LEFT JOIN (SELECT item_id FROM joinItemTable) AS i 
+                 *     ON o.iid = i.item_id 
+                 *     GROUP BY o.iid ORDER BY q DESC                 
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER AGGREGATE INTO DISTINCT ROWS BY [O.IID]\n" +
+                "CLIENT MERGE SORT\n" +
+                "CLIENT SORTED BY [SUM(O.QUANTITY) DESC]\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0 (SKIP MERGE)\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER FILTER BY FIRST KEY ONLY\n" +
+                "        CLIENT MERGE SORT",
+                /* 
+                 * testJoinWithSubqueryAndAggregation()
+                 *     SELECT i.iid, o.q 
+                 *     FROM (SELECT item_id iid FROM joinItemTable) AS i 
+                 *     LEFT JOIN (SELECT item_id iid, sum(quantity) q FROM joinOrderTable GROUP BY item_id) AS o 
+                 *     ON o.iid = i.iid 
+                 *     ORDER BY o.q DESC NULLS LAST, i.iid
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER AGGREGATE INTO DISTINCT ROWS BY [item_id]\n" +
+                "        CLIENT MERGE SORT",
+                /* 
+                 * testJoinWithSubqueryAndAggregation()
+                 *     SELECT i.iid, o.q 
+                 *     FROM (SELECT item_id iid, sum(quantity) q FROM joinOrderTable GROUP BY item_id) AS o 
+                 *     JOIN (SELECT item_id iid FROM joinItemTable) AS i 
+                 *     ON o.iid = i.iid 
+                 *     ORDER BY o.q DESC, i.iid
+                 */     
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER AGGREGATE INTO DISTINCT ROWS BY [item_id]\n" +
+                "        CLIENT MERGE SORT",
+                /*
+                 * testNestedSubqueries()
+                 *     SELECT * FROM (SELECT customer_id cid, name, phone, address, loc_id, date FROM joinCustomerTable) AS c 
+                 *     INNER JOIN (SELECT o.oid ooid, o.cid ocid, o.iid oiid, o.price * o.quantity, o.date odate, 
+                 *     qi.iiid iiid, qi.iname iname, qi.iprice iprice, qi.idiscount1 idiscount1, qi.idiscount2 idiscount2, qi.isid isid, qi.idescription idescription, 
+                 *     qi.ssid ssid, qi.sname sname, qi.sphone sphone, qi.saddress saddress, qi.sloc_id sloc_id 
+                 *         FROM (SELECT item_id iid, customer_id cid, order_id oid, price, quantity, date FROM joinOrderTable) AS o 
+                 *         INNER JOIN (SELECT i.iid iiid, i.name iname, i.price iprice, i.discount1 idiscount1, i.discount2 idiscount2, i.sid isid, i.description idescription, 
+                 *         s.sid ssid, s.name sname, s.phone sphone, s.address saddress, s.loc_id sloc_id 
+                 *             FROM (SELECT supplier_id sid, name, phone, address, loc_id FROM joinSupplierTable) AS s 
+                 *             RIGHT JOIN (SELECT item_id iid, name, price, discount1, discount2, supplier_id sid, description FROM joinItemTable) AS i 
+                 *             ON i.sid = s.sid) as qi 
+                 *         ON o.iid = qi.iiid) as qo 
+                 *     ON c.cid = qo.ocid 
+                 *     WHERE c.cid <= '0000000005' 
+                 *         AND qo.ooid != '000000000000003' 
+                 *         AND qo.iname != 'T3' 
+                 *     ORDER BY c.cid, qo.iname
+                 */
+                "CLIENT PARALLEL 1-WAY RANGE SCAN OVER " + JOIN_CUSTOMER_TABLE_DISPLAY_NAME + " [*] - ['0000000005']\n" +
+                "    SERVER SORTED BY [C.CID, QO.INAME]\n" +
+                "CLIENT MERGE SORT\n" +
+                "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "            SERVER FILTER BY order_id != '000000000000003'\n" +
+                "            PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "            BUILD HASH TABLE 0\n" +
+                "                CLIENT PARALLEL 1-WAY FULL SCAN OVER " +  MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "                    SERVER FILTER BY NAME != 'T3'\n" +
+                "                CLIENT MERGE SORT\n" +      
+                "                    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
+                "                    BUILD HASH TABLE 0\n" +
+                "                        CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_SUPPLIER_TABLE_DISPLAY_NAME,
+                /*
+                 * testJoinWithLimit()
+                 *     SELECT order_id, i.name, s.name, s.address, quantity 
+                 *     FROM joinSupplierTable s 
+                 *     LEFT JOIN joinItemTable i ON i.supplier_id = s.supplier_id 
+                 *     LEFT JOIN joinOrderTable o ON o.item_id = i.item_id LIMIT 4
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_SUPPLIER_TABLE_DISPLAY_NAME + "\n" +
+                "    SERVER FILTER BY PageFilter 4\n" +
+                "    SERVER 4 ROW LIMIT\n" +
+                "CLIENT 4 ROW LIMIT\n" +
+                "    PARALLEL EQUI-JOIN 2 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER "+ MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "        CLIENT MERGE SORT\n" +      
+                "    BUILD HASH TABLE 1(DELAYED EVALUATION)\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER "+ JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    JOIN-SCANNER 4 ROW LIMIT",
+                /*
+                 * testJoinWithLimit()
+                 *     SELECT order_id, i.name, s.name, s.address, quantity 
+                 *     FROM joinSupplierTable s 
+                 *     JOIN joinItemTable i ON i.supplier_id = s.supplier_id 
+                 *     JOIN joinOrderTable o ON o.item_id = i.item_id LIMIT 4
+                 */
+                "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + JOIN_SUPPLIER_TABLE_DISPLAY_NAME + "\n" +
+                "CLIENT 4 ROW LIMIT\n" +
+                "    PARALLEL EQUI-JOIN 2 HASH TABLES:\n" +
+                "    BUILD HASH TABLE 0\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER "+ MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
+                "        CLIENT MERGE SORT\n" +
+                "    BUILD HASH TABLE 1(DELAYED EVALUATION)\n" +
+                "        CLIENT PARALLEL 1-WAY FULL SCAN OVER "+ JOIN_ORDER_TABLE_DISPLAY_NAME + "\n" +
+                "    JOIN-SCANNER 4 ROW LIMIT",
+                }});
         return testCases;
     }
     
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java
index f453853..61cbd84 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java
@@ -48,12 +48,14 @@ import java.sql.Time;
 import java.sql.Timestamp;
 import java.util.Arrays;
 import java.util.Collection;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import org.apache.hadoop.hbase.TableName;
 import org.apache.hadoop.hbase.client.HBaseAdmin;
 import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
@@ -66,6 +68,7 @@ import org.apache.phoenix.schema.ConstraintViolationException;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.SequenceNotFoundException;
 import org.apache.phoenix.util.ByteUtil;
+import org.apache.phoenix.util.MetaDataUtil;
 import org.apache.phoenix.util.PhoenixRuntime;
 import org.apache.phoenix.util.ReadOnlyProps;
 import org.junit.Before;
@@ -143,6 +146,8 @@ public class QueryIT extends BaseClientManagedTimeIT {
                 + "    B_STRING, " + "    A_DATE)" });
         testCases.add(new String[] { "CREATE INDEX " + ATABLE_INDEX_NAME + " ON aTable (a_integer) INCLUDE ("
                 + "    A_STRING, " + "    B_STRING, " + "    A_DATE)" });
+        testCases.add(new String[] { "CREATE LOCAL INDEX " + ATABLE_INDEX_NAME + " ON aTable (a_integer, a_string) INCLUDE ("
+                + "    B_STRING, " + "    A_DATE)" });
         testCases.add(new String[] { "" });
         return testCases;
     }
@@ -190,21 +195,16 @@ public class QueryIT extends BaseClientManagedTimeIT {
             results.add(result);
         }
         for (int j = 0; j < expectedResultsArray.length; j++) {
-                List<List<Object>> expectedResults = expectedResultsArray[j];
-                Set<List<Object>> expectedResultsSet = Sets.newHashSet(expectedResults);
-                int count = 0;
-                boolean brokeEarly = false;
-                for (List<Object> result : results) {
-                    if (!expectedResultsSet.contains(result)) {
-                        brokeEarly = true;
-                        break;
-                    }
-                    count++;
-                }
-                if (!brokeEarly && count == expectedResults.size()) {
-                    return;
+            List<List<Object>> expectedResults = expectedResultsArray[j];
+            Set<List<Object>> expectedResultsSet = Sets.newHashSet(expectedResults);
+            Iterator<List<Object>> iterator = results.iterator();
+            while (iterator.hasNext()) {
+                if (expectedResultsSet.contains(iterator.next())) {
+                    iterator.remove();
                 }
+            }
         }
+        if (results.isEmpty()) return;
         fail("Unable to find " + results + " in " + Arrays.asList(expectedResultsArray));
     }
     
@@ -771,8 +771,8 @@ public class QueryIT extends BaseClientManagedTimeIT {
         String[] answers = new String[]{"00D300000000XHP5bar","a5bar","15bar","5bar","5bar"};
         String[] queries = new String[] { 
         		"SELECT  organization_id || 5 || 'bar' FROM atable limit 1",
-        		"SELECT a_string || 5 || 'bar' FROM atable limit 1",
-        		"SELECT a_integer||5||'bar' FROM atable limit 1",
+        		"SELECT a_string || 5 || 'bar' FROM atable  order by a_string  limit 1",
+        		"SELECT a_integer||5||'bar' FROM atable order by a_integer  limit 1",
         		"SELECT x_decimal||5||'bar' FROM atable limit 1",
         		"SELECT x_long||5||'bar' FROM atable limit 1"
         };
@@ -879,17 +879,19 @@ public class QueryIT extends BaseClientManagedTimeIT {
             
             byte[] tableName = Bytes.toBytes(ATABLE_NAME);
             admin = conn.unwrap(PhoenixConnection.class).getQueryServices().getAdmin();
-            HTable htable = (HTable) conn.unwrap(PhoenixConnection.class).getQueryServices().getTable(tableName);
-            htable.clearRegionCache();
-            int nRegions = htable.getRegionLocations().size();
-            admin.split(tableName, ByteUtil.concat(Bytes.toBytes(tenantId), Bytes.toBytes("00A" + Character.valueOf((char) ('3' + nextRunCount())) + ts))); // vary split point with test run
-            int retryCount = 0;
-            do {
-                Thread.sleep(2000);
-                retryCount++;
-                //htable.clearRegionCache();
-            } while (retryCount < 10 && htable.getRegionLocations().size() == nRegions);
-            assertNotEquals(nRegions, htable.getRegionLocations().size());
+            if (admin.tableExists(TableName.valueOf(MetaDataUtil.getLocalIndexTableName("atable")))) {
+                HTable htable = (HTable) conn.unwrap(PhoenixConnection.class).getQueryServices().getTable(tableName);
+                htable.clearRegionCache();
+                int nRegions = htable.getRegionLocations().size();
+                admin.split(tableName, ByteUtil.concat(Bytes.toBytes(tenantId), Bytes.toBytes("00A" + Character.valueOf((char) ('3' + nextRunCount())) + ts))); // vary split point with test run
+                int retryCount = 0;
+                do {
+                    Thread.sleep(2000);
+                    retryCount++;
+                    //htable.clearRegionCache();
+                } while (retryCount < 10 && htable.getRegionLocations().size() == nRegions);
+                assertNotEquals(nRegions, htable.getRegionLocations().size());
+            }
             
             statement.setString(1, tenantId);
             rs = statement.executeQuery();
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/SaltedViewIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/SaltedViewIT.java
index ea59b85..0db0408 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/SaltedViewIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/SaltedViewIT.java
@@ -32,6 +32,11 @@ public class SaltedViewIT extends BaseViewIT {
      */
     @Test
     public void testSaltedUpdatableViewWithIndex() throws Exception {
-        testUpdatableViewWithIndex(3);
+        testUpdatableViewWithIndex(3, false);
+    }
+
+    @Test
+    public void testSaltedUpdatableViewWithLocalIndex() throws Exception {
+        testUpdatableViewWithIndex(3, true);
     }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexIT.java
index cec7fed..5452d38 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexIT.java
@@ -42,12 +42,31 @@ public class TenantSpecificViewIndexIT extends BaseTenantSpecificViewIndexIT {
     }
 
     @Test
+    public void testUpdatableViewLocalIndex() throws Exception {
+        testUpdatableView(null, true);
+    }
+
+    @Test
     public void testUpdatableViewsWithSameNameDifferentTenants() throws Exception {
         testUpdatableViewsWithSameNameDifferentTenants(null);
     }
 
     @Test
+    public void testUpdatableViewsWithSameNameDifferentTenantsWithLocalIndex() throws Exception {
+        testUpdatableViewsWithSameNameDifferentTenants(null, true);
+    }
+
+    @Test
     public void testMultiCFViewIndex() throws Exception {
+        testMultiCFViewIndex(false);
+    }
+
+    @Test
+    public void testMultiCFViewLocalIndex() throws Exception {
+        testMultiCFViewIndex(true);
+    }
+    
+    private void testMultiCFViewIndex(boolean localIndex) throws Exception {
         Connection conn = DriverManager.getConnection(getUrl());
         String ddl = "CREATE TABLE MT_BASE (PK1 VARCHAR not null, PK2 VARCHAR not null, "
                 + "MYCF1.COL1 varchar,MYCF2.COL2 varchar "
@@ -76,7 +95,11 @@ public class TenantSpecificViewIndexIT extends BaseTenantSpecificViewIndexIT {
         assertFalse(rs.next());
         conn.createStatement().execute("UPSERT INTO acme VALUES ('e','f','g')");
         conn.commit();
-        conn.createStatement().execute("create index idx_acme on acme (COL1)");
+        if(localIndex){
+            conn.createStatement().execute("create local index idx_acme on acme (COL1)");
+        } else {
+            conn.createStatement().execute("create index idx_acme on acme (COL1)");
+        }
         rs = conn.createStatement().executeQuery("select * from acme");
         assertTrue(rs.next());
         assertEquals("b",rs.getString(1));
@@ -96,7 +119,11 @@ public class TenantSpecificViewIndexIT extends BaseTenantSpecificViewIndexIT {
         assertEquals("f",rs.getString(2));
         assertFalse(rs.next());
         rs = conn.createStatement().executeQuery("explain select pk2,col1 from acme where col1='f'");
-        assertEquals("CLIENT PARALLEL 4-WAY RANGE SCAN OVER _IDX_MT_BASE ['a',-32768,'f']",QueryUtil.getExplainPlan(rs));
+        if(localIndex){
+            assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER _LOCAL_IDX_MT_BASE ['a',-32768,'f']\nCLIENT MERGE SORT",QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 4-WAY RANGE SCAN OVER _IDX_MT_BASE ['a',-32768,'f']",QueryUtil.getExplainPlan(rs));
+        }
         
         try {
             // Cannot reference tenant_id column in tenant specific connection
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexSaltedIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexSaltedIT.java
index 388aaef..da500fe 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexSaltedIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/TenantSpecificViewIndexSaltedIT.java
@@ -37,4 +37,14 @@ public class TenantSpecificViewIndexSaltedIT extends BaseTenantSpecificViewIndex
     public void testUpdatableViewsWithSameNameDifferentTenants() throws Exception {
         testUpdatableViewsWithSameNameDifferentTenants(SALT_BUCKETS);
     }
+    
+    @Test
+    public void testUpdatableSaltedViewWithLocalIndex() throws Exception {
+        testUpdatableView(SALT_BUCKETS, true);
+    }
+
+    @Test
+    public void testUpdatableViewsWithSameNameDifferentTenantsWithLocalIndex() throws Exception {
+        testUpdatableViewsWithSameNameDifferentTenants(SALT_BUCKETS, true);
+    }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/ViewIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/ViewIT.java
index b75bb30..1d022e5 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/ViewIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/ViewIT.java
@@ -95,7 +95,7 @@ public class ViewIT extends BaseViewIT {
 
     @Test
     public void testNonSaltedUpdatableViewWithIndex() throws Exception {
-        testUpdatableViewWithIndex(null);
+        testUpdatableViewWithIndex(null, false);
     }
     
     @Test
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
index 9128595..91cdfe9 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
@@ -112,20 +112,40 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testIndexWithNullableFixedWithCols() throws Exception {
+        testIndexWithNullableFixedWithCols(false);
+    }
+
+    @Test
+    public void testIndexWithNullableFixedWithColsWithLocalIndex() throws Exception {
+        testIndexWithNullableFixedWithCols(true);
+    }
+
+    private void testIndexWithNullableFixedWithCols(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         ensureTableCreated(getUrl(), INDEX_DATA_TABLE);
         populateTestTable();
-        String ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
-                + " (char_col1 ASC, int_col1 ASC)"
-                + " INCLUDE (long_col1, long_col2)";
+        String ddl = null;
+        if(localIndex){
+            ddl = "CREATE LOCAL INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+                    + " (char_col1 ASC, int_col1 ASC)"
+                    + " INCLUDE (long_col1, long_col2)";
+        } else {
+            ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+                    + " (char_col1 ASC, int_col1 ASC)"
+                    + " INCLUDE (long_col1, long_col2)";
+        }
         PreparedStatement stmt = conn.prepareStatement(ddl);
         stmt.execute();
         
         String query = "SELECT char_col1, int_col1 from " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE;
         ResultSet rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER INDEX_TEST.IDX", QueryUtil.getExplainPlan(rs));
+        if (localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_INDEX_TEST.INDEX_DATA_TABLE\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER INDEX_TEST.IDX", QueryUtil.getExplainPlan(rs));
+        }
         
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -190,14 +210,31 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testDeleteFromAllPKColumnIndex() throws Exception {
+        testDeleteFromAllPKColumnIndex(false);
+    }
+
+    @Test
+    public void testDeleteFromAllPKColumnLocalIndex() throws Exception {
+        testDeleteFromAllPKColumnIndex(true);
+    }
+
+    private void testDeleteFromAllPKColumnIndex(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         ensureTableCreated(getUrl(), INDEX_DATA_TABLE);
         populateTestTable();
-        String ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
-                + " (long_pk, varchar_pk)"
-                + " INCLUDE (long_col1, long_col2)";
+        String ddl = null;
+        if (localIndex) {
+            ddl = "CREATE LOCAL INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+                    + " (long_pk, varchar_pk)"
+                    + " INCLUDE (long_col1, long_col2)";
+            
+        } else {
+            ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+                    + " (long_pk, varchar_pk)"
+                    + " INCLUDE (long_col1, long_col2)";
+        }
         PreparedStatement stmt = conn.prepareStatement(ddl);
         stmt.execute();
         
@@ -242,16 +279,30 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
         conn.createStatement().execute("DROP INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE);
     }
     
-    
     @Test
     public void testDropIfImmutableKeyValueColumn() throws Exception {
+        testDropIfImmutableKeyValueColumn(false);
+    }
+    
+    @Test
+    public void testDropIfImmutableKeyValueColumnWithLocalIndex() throws Exception {
+        testDropIfImmutableKeyValueColumn(true);
+    }
+
+    private void testDropIfImmutableKeyValueColumn(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         ensureTableCreated(getUrl(), INDEX_DATA_TABLE);
         populateTestTable();
-        String ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
-                + " (long_col1)";
+        String ddl = null;
+        if(localIndex) {
+            ddl = "CREATE LOCAL INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+                    + " (long_col1)";
+        } else {
+            ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+                    + " (long_col1)";
+        }
         PreparedStatement stmt = conn.prepareStatement(ddl);
         stmt.execute();
         
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index c4e935d..1842e5d 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -37,9 +37,12 @@ import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.ResultScanner;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.util.Pair;
+import org.apache.phoenix.compile.QueryPlan;
+import org.apache.phoenix.coprocessor.BaseScannerRegionObserver;
 import org.apache.phoenix.hbase.index.IndexRegionSplitPolicy;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.jdbc.PhoenixDatabaseMetaData;
+import org.apache.phoenix.jdbc.PhoenixPreparedStatement;
 import org.apache.phoenix.query.QueryServices;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.PTable.IndexType;
@@ -62,7 +65,7 @@ public class LocalIndexIT extends BaseIndexIT {
         // Drop the HBase table metadata for this test
         props.put(QueryServices.DROP_METADATA_ATTRIB, Boolean.toString(true));
         // Must update config before starting server
-        startServer(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
+        setUpTestDriver(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
     }
 
     private void createBaseTable(String tableName, Integer saltBuckets, String splits) throws SQLException {
@@ -70,6 +73,7 @@ public class LocalIndexIT extends BaseIndexIT {
         String ddl = "CREATE TABLE " + tableName + " (t_id VARCHAR NOT NULL,\n" +
                 "k1 INTEGER NOT NULL,\n" +
                 "k2 INTEGER NOT NULL,\n" +
+                "k3 INTEGER,\n" +
                 "v1 VARCHAR,\n" +
                 "CONSTRAINT pk PRIMARY KEY (t_id, k1, k2))\n"
                         + (saltBuckets != null && splits == null ? (",salt_buckets=" + saltBuckets) : "" 
@@ -84,8 +88,8 @@ public class LocalIndexIT extends BaseIndexIT {
         Connection conn1 = DriverManager.getConnection(getUrl());
         Connection conn2 = DriverManager.getConnection(getUrl());
         conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
-        conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
-        PTable localIndex = conn2.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
+        conn1.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+        PTable localIndex = conn1.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
         assertEquals(IndexType.LOCAL, localIndex.getIndexType());
         assertNotNull(localIndex.getViewIndexId());
     }
@@ -161,10 +165,10 @@ public class LocalIndexIT extends BaseIndexIT {
         createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
         conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,'z')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,'z')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,'a')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,'c')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,4,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,3,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,2,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,1,'c')");
         conn1.commit();
         ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
         assertTrue(rs.next());
@@ -193,10 +197,10 @@ public class LocalIndexIT extends BaseIndexIT {
     public void testBuildIndexWhenUserTableAlreadyHasData() throws Exception {
         createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,'z')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,'z')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,'a')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,'c')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,4,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,3,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,2,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,1,'c')");
         conn1.commit();
         conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
         ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
@@ -227,10 +231,12 @@ public class LocalIndexIT extends BaseIndexIT {
         createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
         try{
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,'z')");
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,'a')");
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,'a')");
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,'c')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('a',1,2,5,'y')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,4,'z')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,3,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('e',1,2,3,'b')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,2,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,1,'c')");
             conn1.commit();
             conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
             
@@ -245,7 +251,7 @@ public class LocalIndexIT extends BaseIndexIT {
             
             assertEquals(
                 "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER "
-                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME) + " [-32768,'a']",
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME) + " [-32768,'a']\nCLIENT MERGE SORT",
                         QueryUtil.getExplainPlan(rs));
             
             rs = conn1.createStatement().executeQuery(query);
@@ -257,14 +263,36 @@ public class LocalIndexIT extends BaseIndexIT {
             assertEquals("j", rs.getString("t_id"));
             assertEquals(2, rs.getInt("k1"));
             assertEquals(4, rs.getInt("k2"));
+            assertFalse(rs.next());
+            query = "SELECT t_id, k1, k2,V1, k3 FROM " + DATA_TABLE_NAME +" where v1<='z' order by k3";
+            rs = conn1.createStatement().executeQuery("EXPLAIN "+ query);
+            
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER " + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME) + " [-32768,*] - [-32768,'z']\n" + 
+                "    SERVER SORTED BY [K3]\n" +
+                "CLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+ 
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals(1, rs.getInt("k3"));
+            assertTrue(rs.next());
+            assertEquals(2, rs.getInt("k3"));
+            assertTrue(rs.next());
+            assertEquals(3, rs.getInt("k3"));
+            assertTrue(rs.next());
+            assertEquals(3, rs.getInt("k3"));
+            assertTrue(rs.next());
+            assertEquals(4, rs.getInt("k3"));
+            assertTrue(rs.next());
+            assertEquals(5, rs.getInt("k3"));
+            assertFalse(rs.next());
             
-            query = "SELECT t_id, k1, k2,V1 from " + DATA_TABLE_FULL_NAME + " order by V1,t_id";
+            query = "SELECT t_id, k1, k2,v1 from " + DATA_TABLE_FULL_NAME + " order by V1,t_id";
             rs = conn1.createStatement().executeQuery("EXPLAIN " + query);
             
             assertEquals(
                 "CLIENT PARALLEL " + numRegions + "-WAY FULL SCAN OVER "
-                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)+"\n"
-                        + "    SERVER SORTED BY [V1, T_ID]\n" + "CLIENT MERGE SORT",
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)+"\nCLIENT MERGE SORT",
                 QueryUtil.getExplainPlan(rs));
             
             rs = conn1.createStatement().executeQuery(query);
@@ -279,11 +307,21 @@ public class LocalIndexIT extends BaseIndexIT {
             assertEquals(4, rs.getInt("k2"));
             assertEquals("a", rs.getString("V1"));
             assertTrue(rs.next());
+            assertEquals("e", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals("b", rs.getString("V1"));
+            assertTrue(rs.next());
             assertEquals("q", rs.getString("t_id"));
             assertEquals(3, rs.getInt("k1"));
             assertEquals(1, rs.getInt("k2"));
             assertEquals("c", rs.getString("V1"));
             assertTrue(rs.next());
+            assertEquals("a", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals("y", rs.getString("V1"));
+            assertTrue(rs.next());
             assertEquals("b", rs.getString("t_id"));
             assertEquals(1, rs.getInt("k1"));
             assertEquals(2, rs.getInt("k2"));
@@ -291,17 +329,144 @@ public class LocalIndexIT extends BaseIndexIT {
         } finally {
             conn1.close();
         }
-        
+    }
+
+    @Test
+    public void testLocalIndexScanJoinColumnsFromDataTable() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        try{
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,4,'z')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,3,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,2,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,1,'c')");
+            conn1.commit();
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+            
+            ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
+            assertTrue(rs.next());
+            
+            HBaseAdmin admin = driver.getConnectionQueryServices(getUrl(), TestUtil.TEST_PROPERTIES).getAdmin();
+            int numRegions = admin.getTableRegions(TableName.valueOf(DATA_TABLE_NAME)).size();
+            
+            String query = "SELECT t_id, k1, k2, k3, V1 FROM " + DATA_TABLE_NAME +" where v1='a'";
+            rs = conn1.createStatement().executeQuery("EXPLAIN "+ query);
+            
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER "
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME) + " [-32768,'a']\nCLIENT MERGE SORT",
+                        QueryUtil.getExplainPlan(rs));
+            
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals("f", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals(3, rs.getInt("k3"));
+            assertTrue(rs.next());
+            assertEquals("j", rs.getString("t_id"));
+            assertEquals(2, rs.getInt("k1"));
+            assertEquals(4, rs.getInt("k2"));
+            assertEquals(2, rs.getInt("k3"));
+            assertFalse(rs.next());
+            
+            query = "SELECT t_id, k1, k2, k3, V1 from " + DATA_TABLE_FULL_NAME + "  where v1<='z' order by V1,t_id";
+            rs = conn1.createStatement().executeQuery("EXPLAIN " + query);
+            
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER "
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)+" [-32768,*] - [-32768,'z']\nCLIENT MERGE SORT",
+                QueryUtil.getExplainPlan(rs));
+            
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals("f", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals(3, rs.getInt("k3"));
+            assertEquals("a", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("j", rs.getString("t_id"));
+            assertEquals(2, rs.getInt("k1"));
+            assertEquals(4, rs.getInt("k2"));
+            assertEquals(2, rs.getInt("k3"));
+            assertEquals("a", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("q", rs.getString("t_id"));
+            assertEquals(3, rs.getInt("k1"));
+            assertEquals(1, rs.getInt("k2"));
+            assertEquals(1, rs.getInt("k3"));
+            assertEquals("c", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("b", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k1"));
+            assertEquals(2, rs.getInt("k2"));
+            assertEquals(4, rs.getInt("k3"));
+            assertEquals("z", rs.getString("V1"));
+            
+            query = "SELECT t_id, V1, k3 from " + DATA_TABLE_FULL_NAME + "  where v1 <='z' group by v1,t_id, k3";
+            rs = conn1.createStatement().executeQuery("EXPLAIN " + query);
+            
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER "
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)+" [-32768,*] - [-32768,'z']\n"
+                        + "    SERVER AGGREGATE INTO DISTINCT ROWS BY [V1, T_ID, K3]\n" + "CLIENT MERGE SORT",
+                QueryUtil.getExplainPlan(rs));
+            
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals("f", rs.getString("t_id"));
+            assertEquals(3, rs.getInt("k3"));
+            assertEquals("a", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("j", rs.getString("t_id"));
+            assertEquals(2, rs.getInt("k3"));
+            assertEquals("a", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("q", rs.getString("t_id"));
+            assertEquals(1, rs.getInt("k3"));
+            assertEquals("c", rs.getString("V1"));
+            assertTrue(rs.next());
+            assertEquals("b", rs.getString("t_id"));
+            assertEquals(4, rs.getInt("k3"));
+            assertEquals("z", rs.getString("V1"));
+            
+            query = "SELECT v1,sum(k3) from " + DATA_TABLE_FULL_NAME + " where v1 <='z'  group by v1 order by v1";
+            PhoenixPreparedStatement statement = conn1.prepareStatement(query).unwrap(PhoenixPreparedStatement.class);
+            QueryPlan plan = statement.compileQuery("EXPLAIN " + query);
+            assertTrue(query, plan.getContext().getScan().getAttribute(BaseScannerRegionObserver.KEY_ORDERED_GROUP_BY_EXPRESSIONS) == null);
+            assertTrue(query, plan.getContext().getScan().getAttribute(BaseScannerRegionObserver.UNORDERED_GROUP_BY_EXPRESSIONS) != null);
+            
+            rs = conn1.createStatement().executeQuery("EXPLAIN " + query);
+            assertEquals(
+                "CLIENT PARALLEL " + numRegions + "-WAY RANGE SCAN OVER "
+                        + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_NAME)+" [-32768,*] - [-32768,'z']\n"
+                        + "    SERVER AGGREGATE INTO ORDERED DISTINCT ROWS BY [V1]\nCLIENT MERGE SORT",
+                QueryUtil.getExplainPlan(rs));
+            
+            rs = conn1.createStatement().executeQuery(query);
+            assertTrue(rs.next());
+            assertEquals("a", rs.getString(1));
+            assertEquals(5, rs.getInt(2));
+            assertTrue(rs.next());
+            assertEquals("c", rs.getString(1));
+            assertEquals(1, rs.getInt(2));
+            assertTrue(rs.next());
+            assertEquals("z", rs.getString(1));
+            assertEquals(4, rs.getInt(2));
+       } finally {
+            conn1.close();
+        }
     }
 
     @Test
     public void testIndexPlanSelectionIfBothGlobalAndLocalIndexesHasSameColumnsAndOrder() throws Exception {
         createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,'z')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,'a')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,'a')");
-        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,'c')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('b',1,2,4,'z')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('f',1,2,3,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('j',2,4,3,'a')");
+        conn1.createStatement().execute("UPSERT INTO "+DATA_TABLE_NAME+" values('q',3,1,1,'c')");
         conn1.commit();
         conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
         conn1.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + "2" + " ON " + DATA_TABLE_NAME + "(v1)");
@@ -316,10 +481,10 @@ public class LocalIndexIT extends BaseIndexIT {
         createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
         try {
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,'z')");
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,'a')");
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,'a')");
-            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,'c')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,4,'z')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,3,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,2,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,1,'c')");
             conn1.commit();
             conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
             conn1.createStatement().execute("DROP INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME);
@@ -346,4 +511,26 @@ public class LocalIndexIT extends BaseIndexIT {
             conn1.close();
         }
     }
+
+    @Test
+    public void testLocalIndexRowsShouldBeDeletedWhenUserTableRowsDeleted() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, "('e','i','o')");
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        try {
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('b',1,2,4,'z')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('f',1,2,3,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('j',2,4,2,'a')");
+            conn1.createStatement().execute("UPSERT INTO " + DATA_TABLE_NAME + " values('q',3,1,1,'c')");
+            conn1.commit();
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+            conn1.createStatement().execute("DELETE FROM " + DATA_TABLE_NAME + " where v1='a'");
+            conn1.commit();
+            conn1 = DriverManager.getConnection(getUrl());
+            ResultSet rs = conn1.createStatement().executeQuery("SELECT COUNT(*) FROM " + INDEX_TABLE_NAME);
+            assertTrue(rs.next());
+            assertEquals(2, rs.getInt(1));
+        } finally {
+            conn1.close();
+        }
+    }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java
index fed948b..7f7d0c6 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java
@@ -38,6 +38,7 @@ import org.apache.phoenix.end2end.BaseHBaseManagedTimeIT;
 import org.apache.phoenix.end2end.HBaseManagedTimeTest;
 import org.apache.phoenix.end2end.Shadower;
 import org.apache.phoenix.query.QueryServices;
+import org.apache.phoenix.util.MetaDataUtil;
 import org.apache.phoenix.util.QueryUtil;
 import org.apache.phoenix.util.ReadOnlyProps;
 import org.apache.phoenix.util.TestUtil;
@@ -59,28 +60,49 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         props.put(QueryServices.MAX_INTRA_REGION_PARALLELIZATION_ATTRIB, Integer.toString(1));
         // Forces server cache to be used
         props.put(QueryServices.INDEX_MUTATE_BATCH_SIZE_THRESHOLD_ATTRIB, Integer.toString(2));
+        props.put(QueryServices.DROP_METADATA_ATTRIB, Boolean.toString(true));
         // Must update config before starting server
         props.put(QueryServices.DROP_METADATA_ATTRIB, Boolean.toString(true));
         setUpTestDriver(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
     }
-    
+
     @Test
     public void testIndexWithNullableFixedWithCols() throws Exception {
+        testIndexWithNullableFixedWithCols(false);
+    }
+    
+    @Test
+    public void testLocalIndexWithNullableFixedWithCols() throws Exception {
+        testIndexWithNullableFixedWithCols(true);
+    }
+
+    private void testIndexWithNullableFixedWithCols(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         try {
             createTestTable();
             populateTestTable();
-            String ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME
-                    + " (char_col1 ASC, int_col1 ASC)"
-                    + " INCLUDE (long_col1, long_col2)";
+            String ddl = null;
+            if(localIndex){
+                ddl = "CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME
+                        + " (char_col1 ASC, int_col1 ASC)"
+                        + " INCLUDE (long_col1, long_col2)";
+            } else {
+                ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME
+                        + " (char_col1 ASC, int_col1 ASC)"
+                        + " INCLUDE (long_col1, long_col2)";
+            }
             PreparedStatement stmt = conn.prepareStatement(ddl);
             stmt.execute();
             
             String query = "SELECT d.char_col1, int_col1 from " + DATA_TABLE_FULL_NAME + " as d";
             ResultSet rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            if (localIndex) {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.getLocalIndexTableName(DATA_TABLE_FULL_NAME)+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            }
             
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
@@ -101,6 +123,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
     @Test
     public void testIndexWithNullableDateCol() throws Exception {
+        testIndexWithNullableDateCol(false);
+    }
+
+    @Test
+    public void testLocalIndexWithNullableDateCol() throws Exception {
+        testIndexWithNullableDateCol(true);
+    }
+
+    private void testIndexWithNullableDateCol(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
@@ -109,13 +140,22 @@ public class MutableIndexIT extends BaseMutableIndexIT {
             
             createTestTable();
             populateTestTable(date);
-            String ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (date_col)";
+            String ddl = null;
+            if (localIndex) {
+                ddl = "CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (date_col)";
+            } else {
+                ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (date_col)";
+            }
             PreparedStatement stmt = conn.prepareStatement(ddl);
             stmt.execute();
             
             String query = "SELECT int_pk from " + DATA_TABLE_FULL_NAME ;
             ResultSet rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            if (localIndex) {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME +"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            }
             
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
@@ -128,7 +168,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
             
             query = "SELECT date_col from " + DATA_TABLE_FULL_NAME + " order by date_col" ;
             rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            if (localIndex) {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME + "\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            }
             
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
@@ -145,21 +189,42 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
     @Test
     public void testCoveredColumnUpdates() throws Exception {
+        testCoveredColumnUpdates(false);
+    }
+
+    @Test
+    public void testCoveredColumnUpdatesWithLocalIndex() throws Exception {
+        testCoveredColumnUpdates(true);
+    }
+
+    private void testCoveredColumnUpdates(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         try {
             createTestTable();
             populateTestTable();
-            String ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME
-                    + " (char_col1 ASC, int_col1 ASC)"
-                    + " INCLUDE (long_col1, long_col2)";
+            String ddl = null;
+            if(localIndex) {
+                ddl = "CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME
+                        + " (char_col1 ASC, int_col1 ASC)"
+                        + " INCLUDE (long_col1, long_col2)";
+            } else {
+                ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME
+                        + " (char_col1 ASC, int_col1 ASC)"
+                        + " INCLUDE (long_col1, long_col2)";
+            }
+
             PreparedStatement stmt = conn.prepareStatement(ddl);
             stmt.execute();
             
             String query = "SELECT char_col1, int_col1, long_col2 from " + DATA_TABLE_FULL_NAME;
             ResultSet rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            if (localIndex) {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME +"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            }
             
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
@@ -228,6 +293,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
     
     @Test
     public void testSelectAllAndAliasWithIndex() throws Exception {
+        testSelectAllAndAliasWithIndex(false);
+    }
+
+    @Test
+    public void testSelectAllAndAliasWithLocalIndex() throws Exception {
+        testSelectAllAndAliasWithIndex(true);
+    }
+
+    private void testSelectAllAndAliasWithIndex(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
         
@@ -239,7 +313,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
         
-        conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v2 DESC) INCLUDE (v1)");
+        if (localIndex) {
+            conn.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v2 DESC) INCLUDE (v1)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v2 DESC) INCLUDE (v1)");
+        }
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -257,7 +335,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        if(localIndex){
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        }
 
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -278,9 +360,13 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         
         query = "SELECT v1 as foo FROM " + DATA_TABLE_FULL_NAME + " WHERE v2 = '1' ORDER BY foo";
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER " +INDEX_TABLE_FULL_NAME + " [~'1']\n" + 
-                "    SERVER SORTED BY [V1]\n" + 
-                "CLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        if(localIndex){
+            assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER _LOCAL_IDX_" +DATA_TABLE_FULL_NAME + " [-32768,~'1']\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER " +INDEX_TABLE_FULL_NAME + " [~'1']\n" + 
+                    "    SERVER SORTED BY [V1]\n" + 
+                    "CLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        }
 
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -291,6 +377,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
     
     @Test
     public void testSelectCF() throws Exception {
+        testSelectCF(false);
+    }
+
+    @Test
+    public void testSelectCFWithLocalIndex() throws Exception {
+        testSelectCF(true);
+    }
+
+    private void testSelectCF(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
         
@@ -301,8 +396,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
-        
-        conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v2 DESC) INCLUDE (a.v1)");
+        if(localIndex) {
+            conn.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v2 DESC) INCLUDE (a.v1)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v2 DESC) INCLUDE (a.v1)");
+        }
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -326,7 +424,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
         query = "SELECT a.* FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        if(localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        }
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
         assertEquals("y",rs.getString(1));
@@ -343,6 +445,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
     
     @Test
     public void testCoveredColumns() throws Exception {
+        testCoveredColumns(false);
+    }
+
+    @Test
+    public void testCoveredColumnsWithLocalIndex() throws Exception {
+        testCoveredColumns(true);
+    }
+
+    private void testCoveredColumns(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
         
@@ -354,7 +465,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
         
-        conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1) INCLUDE (v2)");
+        if(localIndex) {
+            conn.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1) INCLUDE (v2)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1) INCLUDE (v2)");
+        }
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -390,7 +505,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        if(localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));            
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        }
 
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -407,8 +526,12 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
-
+        if(localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME + "\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));            
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        }
+        
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
         assertEquals("a",rs.getString(1));
@@ -424,8 +547,12 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
-
+        if(localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));            
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        }
+        
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
         assertEquals("a",rs.getString(1));
@@ -436,6 +563,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
     @Test
     public void testCompoundIndexKey() throws Exception {
+        testCompoundIndexKey(false);
+    }
+
+    @Test
+    public void testCompoundIndexKeyWithLocalIndex() throws Exception {
+        testCompoundIndexKey(true);
+    }
+
+    private void testCompoundIndexKey(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
         
@@ -448,8 +584,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
-        
-        conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        if(localIndex) {
+            conn.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        } else {
+            conn.createStatement().execute("CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        }
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -487,7 +626,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        if (localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+        }
         //make sure the data table looks like what we expect
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -536,6 +679,16 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
     }
  
+    @Test
+    public void testMultipleUpdatesToSingleRow() throws Exception {
+        testMultipleUpdatesToSingleRow(false);
+    }
+
+    @Test
+    public void testMultipleUpdatesToSingleRowWithLocalIndex() throws Exception {
+        testMultipleUpdatesToSingleRow(true);
+    }
+    
     /**
      * There was a case where if there were multiple updates to a single row in the same batch, the
      * index wouldn't be updated correctly as each element of the batch was evaluated with the state
@@ -544,8 +697,7 @@ public class MutableIndexIT extends BaseMutableIndexIT {
      * and current + delete, but not current + put + delete.
      * @throws Exception on failure
      */
-    @Test
-    public void testMultipleUpdatesToSingleRow() throws Exception {
+    private void testMultipleUpdatesToSingleRow(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
     
@@ -560,9 +712,14 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
-    
-        conn.createStatement().execute(
-          "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+
+        if(localIndex) {
+            conn.createStatement().execute(
+                "CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        } else {
+            conn.createStatement().execute(
+                "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        }
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -606,8 +763,13 @@ public class MutableIndexIT extends BaseMutableIndexIT {
     
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME,
-          QueryUtil.getExplainPlan(rs));
+        if(localIndex) {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT",
+                QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME,
+                QueryUtil.getExplainPlan(rs));
+        }
     
         // check that the data table matches as expected
         rs = conn.createStatement().executeQuery(query);
@@ -681,6 +843,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
     
     @Test
     public void testMultipleUpdatesAcrossRegions() throws Exception {
+        testMultipleUpdatesAcrossRegions(false);
+    }
+
+    @Test
+    public void testMultipleUpdatesAcrossRegionsWithLocalIndex() throws Exception {
+        testMultipleUpdatesAcrossRegions(true);
+    }
+
+    private void testMultipleUpdatesAcrossRegions(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
     
@@ -696,9 +867,14 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
-    
-        conn.createStatement().execute(
-          "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+
+        if(localIndex) {
+            conn.createStatement().execute(
+                "CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        } else {
+            conn.createStatement().execute(
+                "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (v1, v2)");
+        }
         query = "SELECT * FROM " + INDEX_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery(query);
         assertFalse(rs.next());
@@ -739,8 +915,13 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
         query = "SELECT * FROM " + DATA_TABLE_FULL_NAME;
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME,
-          QueryUtil.getExplainPlan(rs));
+        if (localIndex) {
+            assertEquals("CLIENT PARALLEL 2-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT",
+                QueryUtil.getExplainPlan(rs));
+        } else {
+            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME,
+                QueryUtil.getExplainPlan(rs));
+        }
     
         // check that the data table matches as expected
         rs = conn.createStatement().executeQuery(query);
@@ -761,6 +942,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
     
     @Test
     public void testIndexWithCaseSensitiveCols() throws Exception {
+        testIndexWithCaseSensitiveCols(false);
+    }
+
+    @Test
+    public void testLocalIndexWithCaseSensitiveCols() throws Exception {
+        testIndexWithCaseSensitiveCols(true);
+    }
+
+    private void testIndexWithCaseSensitiveCols(boolean localIndex) throws Exception {
         String query;
         ResultSet rs;
         
@@ -772,8 +962,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
             query = "SELECT * FROM cs";
             rs = conn.createStatement().executeQuery(query);
             assertFalse(rs.next());
-
-            conn.createStatement().execute("CREATE INDEX ics ON cs (\"v2\") INCLUDE (\"V1\")");
+            if (localIndex) {
+                conn.createStatement().execute("CREATE LOCAL INDEX ics ON cs (\"v2\") INCLUDE (\"V1\")");
+            } else {
+                conn.createStatement().execute("CREATE INDEX ics ON cs (\"v2\") INCLUDE (\"V1\")");
+            }
             query = "SELECT * FROM ics";
             rs = conn.createStatement().executeQuery(query);
             assertFalse(rs.next());
@@ -791,7 +984,11 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
             query = "SELECT * FROM cs WHERE \"v2\" = '1'";
             rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER ICS ['1']", QueryUtil.getExplainPlan(rs));
+            if(localIndex){
+                assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER _LOCAL_IDX_CS [-32768,'1']\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER ICS ['1']", QueryUtil.getExplainPlan(rs));
+            }
 
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
@@ -805,7 +1002,12 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
             query = "SELECT \"V1\", \"V1\" as foo1, \"v2\" as foo, \"v2\" as \"Foo1\", \"v2\" FROM cs ORDER BY foo";
             rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER ICS", QueryUtil.getExplainPlan(rs));
+            if(localIndex){
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_CS\nCLIENT MERGE SORT",
+                    QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER ICS", QueryUtil.getExplainPlan(rs));
+            }
 
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
@@ -838,13 +1040,26 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
     @Test
     public void testInFilterOnIndexedTable() throws Exception {
+        testInFilterOnIndexedTable(false);
+    }
+    
+    @Test
+    public void testInFilterOnLocalIndexedTable() throws Exception {
+        testInFilterOnIndexedTable(true);
+    }
+    
+    private void testInFilterOnIndexedTable(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         try {
 	        String ddl = "CREATE TABLE TEST (PK1 CHAR(2) NOT NULL PRIMARY KEY, CF1.COL1 BIGINT)";
 	        conn.createStatement().execute(ddl);
-	        ddl = "CREATE INDEX IDX1 ON TEST (COL1)";
+	        if(localIndex) {
+	            ddl = "CREATE LOCAL INDEX IDX1 ON TEST (COL1)";
+	        } else {
+	            ddl = "CREATE INDEX IDX1 ON TEST (COL1)";
+	        }
 	        conn.createStatement().execute(ddl);
 	
 	        String query = "SELECT COUNT(COL1) FROM TEST WHERE COL1 IN (1,25,50,75,100)"; 
@@ -857,6 +1072,15 @@ public class MutableIndexIT extends BaseMutableIndexIT {
 
     @Test
     public void testIndexWithDecimalCol() throws Exception {
+        testIndexWithDecimalCol(false);
+    }
+
+    @Test
+    public void testLocalIndexWithDecimalCol() throws Exception {
+        testIndexWithDecimalCol(true);
+    }
+
+    private void testIndexWithDecimalCol(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
@@ -865,13 +1089,22 @@ public class MutableIndexIT extends BaseMutableIndexIT {
             
             createTestTable();
             populateTestTable(date);
-            String ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (decimal_pk) INCLUDE (decimal_col1, decimal_col2)";
+            String ddl = null;
+            if (localIndex) {
+                ddl = "CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (decimal_pk) INCLUDE (decimal_col1, decimal_col2)";
+            } else {
+                ddl = "CREATE INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_FULL_NAME + " (decimal_pk) INCLUDE (decimal_col1, decimal_col2)";
+            }
             PreparedStatement stmt = conn.prepareStatement(ddl);
             stmt.execute();
             
             String query = "SELECT decimal_pk, decimal_col1, decimal_col2 from " + DATA_TABLE_FULL_NAME ;
             ResultSet rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            if(localIndex) {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_" + DATA_TABLE_FULL_NAME+"\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            } else {
+                assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER " + INDEX_TABLE_FULL_NAME, QueryUtil.getExplainPlan(rs));
+            }
             
             rs = conn.createStatement().executeQuery(query);
             assertTrue(rs.next());
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java
index 386d0f1..cad56d0 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ViewIndexIT.java
@@ -47,7 +47,7 @@ public class ViewIndexIT extends BaseIndexIT {
         // Drop the HBase table metadata for this test
         props.put(QueryServices.DROP_METADATA_ATTRIB, Boolean.toString(true));
         // Must update config before starting server
-        startServer(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
+        setUpTestDriver(getUrl(), new ReadOnlyProps(props.entrySet().iterator()));
     }
 
     private void createBaseTable(String tableName, Integer saltBuckets, String splits) throws SQLException {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
index 3ba5789..2a687c6 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/CreateIndexCompiler.java
@@ -56,8 +56,8 @@ public class CreateIndexCompiler {
                 throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SPLIT_LOCAL_INDEX)
                 .build().buildException();
             } 
-            if (create.getProps() != null && create.getProps().get("") != null) {
-                List<Pair<String, Object>> list = create.getProps().get("");
+            List<Pair<String, Object>> list = create.getProps() != null ? create.getProps().get("") : null;
+            if (list != null) {
                 for (Pair<String, Object> pair : list) {
                     if (pair.getFirst().equals(PhoenixDatabaseMetaData.SALT_BUCKETS)) {
                         throw new SQLExceptionInfo.Builder(SQLExceptionCode.CANNOT_SALT_LOCAL_INDEX)
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java
index 59a7ce7..2bd0c81 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java
@@ -67,6 +67,7 @@ import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.ReadOnlyTableException;
 import org.apache.phoenix.schema.SortOrder;
 import org.apache.phoenix.schema.TableRef;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.tuple.Tuple;
 import org.apache.phoenix.util.IndexUtil;
 import org.apache.phoenix.util.MetaDataUtil;
@@ -169,7 +170,7 @@ public class DeleteCompiler {
         }
         for (PTable index : tableRef.getTable().getIndexes()) {
             for (PColumn column : index.getPKColumns()) {
-                if (!IndexUtil.isDataPKColumn(column)) {
+                if (!IndexUtil.isDataPKColumn(column) && (index.getIndexType() != IndexType.LOCAL || !column.getName().toString().equals(MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME))) {
                     return true;
                 }
             }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/ExpressionCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/ExpressionCompiler.java
index 5650ed5..e7883a0 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/ExpressionCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/ExpressionCompiler.java
@@ -94,14 +94,17 @@ import org.apache.phoenix.parse.SequenceValueParseNode;
 import org.apache.phoenix.parse.StringConcatParseNode;
 import org.apache.phoenix.parse.SubtractParseNode;
 import org.apache.phoenix.parse.UnsupportedAllParseNodeVisitor;
+import org.apache.phoenix.schema.ColumnFamilyNotFoundException;
 import org.apache.phoenix.schema.ColumnNotFoundException;
 import org.apache.phoenix.schema.ColumnRef;
 import org.apache.phoenix.schema.DelegateDatum;
+import org.apache.phoenix.schema.LocalIndexDataColumnRef;
 import org.apache.phoenix.schema.PArrayDataType;
 import org.apache.phoenix.schema.PColumn;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PDatum;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.RowKeyValueAccessor;
 import org.apache.phoenix.schema.SortOrder;
@@ -322,7 +325,23 @@ public class ExpressionCompiler extends UnsupportedAllParseNodeVisitor<Expressio
      * @throws SQLException if the column expression node does not refer to a known/unambiguous column
      */
     protected ColumnRef resolveColumn(ColumnParseNode node) throws SQLException {
-        ColumnRef ref = context.getResolver().resolveColumn(node.getSchemaName(), node.getTableName(), node.getName());
+        ColumnRef ref = null;
+        try {
+            ref = context.getResolver().resolveColumn(node.getSchemaName(), node.getTableName(), node.getName());
+        } catch (ColumnNotFoundException e) {
+            // Rather than not use a local index when a column not contained by it is referenced, we
+            // join back to the data table in our coprocessor since this is a relatively cheap
+            // operation given that we know the join is local.
+            if (context.getCurrentTable().getTable().getIndexType() == IndexType.LOCAL) {
+                try {
+                    return new LocalIndexDataColumnRef(context, node.getName());
+                } catch (ColumnFamilyNotFoundException c) {
+                    throw e;
+                }
+            } else {
+                throw e;
+            }
+        }
         PTable table = ref.getTable();
         int pkPosition = ref.getPKSlotPosition();
         // Disallow explicit reference to salting column, tenant ID column, and index ID column
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/FromCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/FromCompiler.java
index b9e0949..5d3e64e 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/FromCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/FromCompiler.java
@@ -419,7 +419,7 @@ public class FromCompiler {
             PTable t = PTableImpl.makePTable(null, PName.EMPTY_NAME, PName.EMPTY_NAME, 
                     PTableType.SUBQUERY, null, MetaDataProtocol.MIN_TABLE_TIMESTAMP, PTable.INITIAL_SEQ_NUM, 
                     null, null, columns, null, Collections.<PTable>emptyList(), false, 
-                    Collections.<PName>emptyList(), null, null, false, false, null, null);
+                    Collections.<PName>emptyList(), null, null, false, false, null, null, null);
             
             String alias = subselectNode.getAlias();
             TableRef tableRef = new TableRef(alias, t, MetaDataProtocol.MIN_TABLE_TIMESTAMP, false);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
index b6acc65..c7a5424 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
@@ -96,7 +96,6 @@ public class IndexStatementRewriter extends ParseNodeRewriter {
 
         String indexColName = IndexUtil.getIndexColumnName(dataCol);
         // Same alias as before, but use the index column name instead of the data column name
-        // TODO: add dataColRef as an alternate ColumnParseNode in the case that the index
         // ColumnParseNode cannot be resolved. When this occurs, add the dataColRef to a list
         // on the StatementContext to indicate that a join back to the data table is necessary.
         // At ExpressionCompiler.resolveColumn(), test if the table is a local index, and only
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
index bba951e..233cfab 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
@@ -83,6 +83,7 @@ import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PName;
 import org.apache.phoenix.schema.PNameFactory;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableImpl;
 import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.TableRef;
@@ -771,7 +772,7 @@ public class JoinCompiler {
             PTable t = PTableImpl.makePTable(table.getTenantId(), PNameFactory.newName(PROJECTED_TABLE_SCHEMA), table.getName(), PTableType.JOIN,
                         table.getIndexState(), table.getTimeStamp(), table.getSequenceNumber(), table.getPKName(),
                         null, projectedColumns, table.getParentTableName(),
-                        table.getIndexes(), table.isImmutableRows(), Collections.<PName>emptyList(), null, null, table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId());
+                        table.getIndexes(), table.isImmutableRows(), Collections.<PName>emptyList(), null, null, table.isWALDisabled(), table.isMultiTenant(), table.getViewType(), table.getViewIndexId(), table.getIndexType());
             return new ProjectedPTableWrapper(t, columnNameMap, sourceExpressions);
         }
     }
@@ -1134,8 +1135,14 @@ public class JoinCompiler {
             List<OrderByNode> orderBy = tableRef.equals(orderByTableRef) ? select.getOrderBy() : null;
             SelectStatement stmt = getSubqueryForOptimizedPlan(select.getHint(), table.getDynamicColumns(), tableRef, join.getColumnRefs(), table.getPreFiltersCombined(), groupBy, orderBy, table.isWildCardSelect());
             QueryPlan plan = statement.getConnection().getQueryServices().getOptimizer().optimize(statement, stmt);
+            boolean localIndex = plan.getContext().getCurrentTable().getTable().getIndexType()==IndexType.LOCAL;
             if (!plan.getTableRef().equals(tableRef)) {
-                replacement.put(tableRef, plan.getTableRef());            
+                // Use local index plan only when all the columns to project are available in index.
+                // Other wise use data plan.
+                // TODO: In join queries support joining back to data table from index when columns to project are missed index. 
+                if (!localIndex || plan.getContext().getDataColumns().isEmpty()) {
+                    replacement.put(tableRef, plan.getTableRef());
+                } 
             }            
         }
         
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java
index 06d5f89..b23f038 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java
@@ -20,7 +20,10 @@ package org.apache.phoenix.compile;
 import java.sql.SQLException;
 import java.text.Format;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.List;
+import java.util.Map;
+import java.util.Set;
 
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
@@ -32,12 +35,15 @@ import org.apache.phoenix.query.KeyRange;
 import org.apache.phoenix.query.QueryConstants;
 import org.apache.phoenix.query.QueryServices;
 import org.apache.phoenix.schema.MetaDataClient;
+import org.apache.phoenix.schema.PColumn;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.TableRef;
 import org.apache.phoenix.util.DateUtil;
 import org.apache.phoenix.util.NumberUtil;
 import org.apache.phoenix.util.ScanUtil;
 
+import com.google.common.collect.Maps;
+
 
 /**
  *
@@ -59,6 +65,7 @@ public class StatementContext {
     private final String numberFormat;
     private final ImmutableBytesWritable tempPtr;
     private final PhoenixStatement statement;
+    private final Map<PColumn, Integer> dataColumns;
     
     private long currentTime = QueryConstants.UNSET_TIMESTAMP;
     private ScanRanges scanRanges = ScanRanges.EVERYTHING;
@@ -89,6 +96,7 @@ public class StatementContext {
         this.tempPtr = new ImmutableBytesWritable();
         this.currentTable = resolver != null && !resolver.getTables().isEmpty() ? resolver.getTables().get(0) : null;
         this.whereConditionColumns = new ArrayList<Pair<byte[],byte[]>>();
+        this.dataColumns = this.currentTable == null ? Collections.<PColumn, Integer>emptyMap() : Maps.<PColumn, Integer>newLinkedHashMap();
     }
 
     /**
@@ -112,6 +120,36 @@ public class StatementContext {
         this.tempPtr = new ImmutableBytesWritable();
         this.currentTable = stmtContext.currentTable;
         this.whereConditionColumns = stmtContext.whereConditionColumns;
+        this.dataColumns = stmtContext.getDataColumnsMap();
+    }
+
+    /**
+     * build map from dataColumn to what will be it's position in single KeyValue value bytes
+     * returned from the coprocessor that joins from the index row back to the data row.
+     * @param column
+     * @return
+     */
+    public int getDataColumnPosition(PColumn column) {
+        Integer pos = dataColumns.get(column);
+        if (pos == null) {
+            pos = dataColumns.size();
+            dataColumns.put(column, pos);
+        }
+        return pos;
+    }
+
+    /**
+     * @return return set of data columns.
+     */
+    public Set<PColumn> getDataColumns() {
+        return dataColumns.keySet();
+    }
+
+    /**
+     * @return map of data columns and their positions. 
+     */
+    public Map<PColumn, Integer> getDataColumnsMap() {
+        return dataColumns;
     }
 
     public String getDateFormat() {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java
index 615ee6d..44f9527 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/TrackOrderPreservingExpressionCompiler.java
@@ -70,7 +70,6 @@ public class TrackOrderPreservingExpressionCompiler extends ExpressionCompiler {
         boolean isSharedViewIndex = table.getViewIndexId() != null;
         // TODO: util for this offset, as it's computed in numerous places
         positionOffset = (isSalted ? 1 : 0) + (isMultiTenant ? 1 : 0) + (isSharedViewIndex ? 1 : 0);
-        this.isOrderPreserving &= table.getIndexType() != IndexType.LOCAL;
         entries = Lists.newArrayListWithExpectedSize(expectedEntrySize);
         this.ordering = ordering;
     }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java
index 98e4517..ccea388 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java
@@ -44,8 +44,10 @@ import org.apache.phoenix.parse.ParseNodeFactory;
 import org.apache.phoenix.schema.AmbiguousColumnException;
 import org.apache.phoenix.schema.ColumnNotFoundException;
 import org.apache.phoenix.schema.ColumnRef;
+import org.apache.phoenix.schema.LocalIndexDataColumnRef;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.TableRef;
 import org.apache.phoenix.schema.TypeMismatchException;
@@ -135,6 +137,15 @@ public class WhereCompiler {
         protected ColumnRef resolveColumn(ColumnParseNode node) throws SQLException {
             ColumnRef ref = super.resolveColumn(node);
             PTable table = ref.getTable();
+            // Current table in the context is local index and table in column reference is global means
+            // the column is not present in the local index. If where condition contains the column 
+            // not present in index then we need to go through main table for each row in index and get the
+            // missing column which is like full scan of index table and data table. Which is
+            // inefficient. So we can skip this plan.
+            if (context.getCurrentTable().getTable().getIndexType() == IndexType.LOCAL
+                    && (table.getIndexType() == null || table.getIndexType() == IndexType.GLOBAL)) {
+                throw new ColumnNotFoundException(ref.getColumn().getName().getString());
+            }
             // Track if we need to compare KeyValue during filter evaluation
             // using column family. If the column qualifier is enough, we
             // just use that.
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
index 9e42f64..3ca0ce3 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/BaseScannerRegionObserver.java
@@ -46,6 +46,9 @@ abstract public class BaseScannerRegionObserver extends BaseRegionObserver {
     public static final String GROUP_BY_LIMIT = "_GroupByLimit";
     public static final String LOCAL_INDEX = "_LocalIndex";
     public static final String LOCAL_INDEX_BUILD = "_LocalIndexBuild";
+    public static final String LOCAL_INDEX_JOIN_SCHEMA = "_LocalIndexJoinSchema";
+    public static final String DATA_TABLE_COLUMNS_TO_JOIN = "_DataTableColumnsToJoin";
+    public static final String VIEW_CONSTANTS = "_ViewConstants";
 
     /**
      * Used by logger to identify coprocessor
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java
index 84adaa9..2322eb3 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/GroupedAggregateRegionObserver.java
@@ -56,7 +56,9 @@ import org.apache.phoenix.expression.Expression;
 import org.apache.phoenix.expression.ExpressionType;
 import org.apache.phoenix.expression.aggregator.Aggregator;
 import org.apache.phoenix.expression.aggregator.ServerAggregators;
+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
 import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
+import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.join.HashJoinInfo;
 import org.apache.phoenix.join.TupleProjector;
 import org.apache.phoenix.memory.MemoryManager.MemoryChunk;
@@ -72,6 +74,7 @@ import org.apache.phoenix.util.TupleUtil;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.common.io.Closeables;
 
@@ -112,11 +115,12 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
              * For local indexes, we need to set an offset on row key expressions to skip
              * the region start key.
              */
-            offset = c.getEnvironment().getRegion().getStartKey().length;
+            HRegion region = c.getEnvironment().getRegion();
+            offset = region.getStartKey().length != 0 ? region.getStartKey().length:region.getEndKey().length;
             ScanUtil.setRowKeyOffset(scan, offset);
         }
         
-        List<Expression> expressions = deserializeGroupByExpressions(expressionBytes, offset);
+        List<Expression> expressions = deserializeGroupByExpressions(expressionBytes, 0);
         ServerAggregators aggregators =
                 ServerAggregators.deserialize(scan
                         .getAttribute(BaseScannerRegionObserver.AGGREGATORS), c
@@ -136,12 +140,30 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
                     new HashJoinRegionScanner(s, p, j, ScanUtil.getTenantId(scan),
                             c.getEnvironment());
         }
+        byte[] localIndexBytes = scan.getAttribute(LOCAL_INDEX_BUILD);
+        List<IndexMaintainer> indexMaintainers = localIndexBytes == null ? null : IndexMaintainer.deserialize(localIndexBytes);
+        boolean localIndexScan = ScanUtil.isLocalIndex(scan);
+        TupleProjector tupleProjector = null;
+        HRegion dataRegion = null;
+        byte[][] viewConstants = null;
+        ColumnReference[] dataColumns = IndexUtil.deserializeDataTableColumnsToJoin(scan);
+        if (ScanUtil.isLocalIndex(scan)) {
+            if (dataColumns != null) {
+                tupleProjector = IndexUtil.getTupleProjector(scan, dataColumns);
+                dataRegion = IndexUtil.getDataRegion(c.getEnvironment());
+                viewConstants = IndexUtil.deserializeViewConstantsFromScan(scan);
+            }
+        } 
 
         if (keyOrdered) { // Optimize by taking advantage that the rows are
                           // already in the required group by key order
-            return scanOrdered(c, scan, innerScanner, expressions, aggregators, limit);
+            return scanOrdered(c, scan, innerScanner, expressions, aggregators, limit, offset,
+                localIndexScan, dataColumns, tupleProjector, indexMaintainers, dataRegion,
+                viewConstants);
         } else { // Otherwse, collect them all up in an in memory map
-            return scanUnordered(c, scan, innerScanner, expressions, aggregators, limit);
+            return scanUnordered(c, scan, innerScanner, expressions, aggregators, limit, offset,
+                localIndexScan, dataColumns, tupleProjector, indexMaintainers, dataRegion,
+                viewConstants);
         }
     }
 
@@ -355,7 +377,10 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
      */
     private RegionScanner scanUnordered(ObserverContext<RegionCoprocessorEnvironment> c, Scan scan,
             final RegionScanner s, final List<Expression> expressions,
-            final ServerAggregators aggregators, long limit) throws IOException {
+            final ServerAggregators aggregators, long limit, int offset, boolean localIndexScan,
+            ColumnReference[] dataColumns, TupleProjector tupleProjector,
+            List<IndexMaintainer> indexMaintainers, HRegion dataRegion, byte[][] viewConstants)
+            throws IOException {
         if (logger.isDebugEnabled()) {
             logger.debug("Grouped aggregation over unordered rows with scan " + scan
                     + ", group by " + expressions + ", aggregators " + aggregators);
@@ -377,7 +402,7 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
                 GroupByCacheFactory.INSTANCE.newCache(
                         env, ScanUtil.getTenantId(scan), 
                         aggregators, estDistVals);
-
+        ImmutableBytesWritable tempPtr = new ImmutableBytesWritable();
         boolean success = false;
         try {
             boolean hasMore;
@@ -399,6 +424,11 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
                     // ones returned
                     hasMore = s.nextRaw(results);
                     if (!results.isEmpty()) {
+                        if (localIndexScan) {
+                            IndexUtil.wrapResultUsingOffset(results, offset, dataColumns, tupleProjector,
+                                dataRegion, indexMaintainers == null ? null : indexMaintainers.get(0),
+                                viewConstants, tempPtr);
+                        }
                         result.setKeyValues(results);
                         ImmutableBytesWritable key =
                                 TupleUtil.getConcatenatedValue(result, expressions);
@@ -430,15 +460,20 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
      * Used for an aggregate query in which the key order match the group by key order. In this
      * case, we can do the aggregation as we scan, by detecting when the group by key changes.
      * @param limit TODO
+     * @throws IOException 
      */
     private RegionScanner scanOrdered(final ObserverContext<RegionCoprocessorEnvironment> c,
             Scan scan, final RegionScanner s, final List<Expression> expressions,
-            final ServerAggregators aggregators, final long limit) {
+            final ServerAggregators aggregators, final long limit, final int offset,
+            final boolean localIndexScan, final ColumnReference[] dataColumns,
+            final TupleProjector tupleProjector, final List<IndexMaintainer> indexMaintainers,
+            final HRegion dataRegion, final byte[][] viewConstants) throws IOException {
 
         if (logger.isDebugEnabled()) {
             logger.debug("Grouped aggregation over ordered rows with scan " + scan + ", group by "
                     + expressions + ", aggregators " + aggregators);
         }
+        final ImmutableBytesWritable tempPtr = new ImmutableBytesWritable();
         return new BaseRegionScanner() {
             private long rowCount = 0;
             private ImmutableBytesWritable currentKey = null;
@@ -476,6 +511,11 @@ public class GroupedAggregateRegionObserver extends BaseScannerRegionObserver {
                         // ones returned
                         hasMore = s.nextRaw(kvs);
                         if (!kvs.isEmpty()) {
+                            if (localIndexScan) {
+                                IndexUtil.wrapResultUsingOffset(kvs, offset, dataColumns, tupleProjector,
+                                    dataRegion, indexMaintainers == null ? null : indexMaintainers.get(0),
+                                    viewConstants, tempPtr);
+                            }
                             result.setKeyValues(kvs);
                             key = TupleUtil.getConcatenatedValue(result, expressions);
                             aggBoundary = currentKey != null && currentKey.compareTo(key) != 0;
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
index c581f64..6fe4598 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/ScanRegionObserver.java
@@ -44,6 +44,8 @@ import org.apache.phoenix.expression.Expression;
 import org.apache.phoenix.expression.KeyValueColumnExpression;
 import org.apache.phoenix.expression.OrderByExpression;
 import org.apache.phoenix.expression.function.ArrayIndexFunction;
+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
+import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.iterate.OrderedResultIterator;
 import org.apache.phoenix.iterate.RegionScannerResultIterator;
 import org.apache.phoenix.iterate.ResultIterator;
@@ -101,7 +103,7 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
         }
     }
     
-    public static OrderedResultIterator deserializeFromScan(Scan scan, RegionScanner s, int offset) {
+    public static OrderedResultIterator deserializeFromScan(Scan scan, RegionScanner s) {
         byte[] topN = scan.getAttribute(BaseScannerRegionObserver.TOPN);
         if (topN == null) {
             return null;
@@ -117,9 +119,6 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
             for (int i = 0; i < size; i++) {
                 OrderByExpression orderByExpression = new OrderByExpression();
                 orderByExpression.readFields(input);
-                if (offset != 0) {
-                    IndexUtil.setRowKeyExpressionOffset(orderByExpression.getExpression(), offset);
-                }
                 orderByExpressions.add(orderByExpression);
             }
             ResultIterator inner = new RegionScannerResultIterator(s);
@@ -186,7 +185,8 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
              * For local indexes, we need to set an offset on row key expressions to skip
              * the region start key.
              */
-            offset = c.getEnvironment().getRegion().getStartKey().length;
+            HRegion region = c.getEnvironment().getRegion();
+            offset = region.getStartKey().length != 0 ? region.getStartKey().length:region.getEndKey().length;
             ScanUtil.setRowKeyOffset(scan, offset);
         }
         
@@ -202,15 +202,30 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
         List<KeyValueColumnExpression> arrayKVRefs = new ArrayList<KeyValueColumnExpression>();
         Expression[] arrayFuncRefs = deserializeArrayPostionalExpressionInfoFromScan(
                 scan, innerScanner, arrayKVRefs);
-        innerScanner = getWrappedScanner(c, innerScanner, arrayKVRefs, arrayFuncRefs, offset);
-        final OrderedResultIterator iterator = deserializeFromScan(scan,innerScanner, offset);
+        TupleProjector tupleProjector = null;
+        HRegion dataRegion = null;
+        IndexMaintainer indexMaintainer = null;
+        byte[][] viewConstants = null;
+        ColumnReference[] dataColumns = IndexUtil.deserializeDataTableColumnsToJoin(scan);
+        if (dataColumns != null) {
+            tupleProjector = IndexUtil.getTupleProjector(scan, dataColumns);
+            dataRegion = IndexUtil.getDataRegion(c.getEnvironment());
+            byte[] localIndexBytes = scan.getAttribute(LOCAL_INDEX_BUILD);
+            List<IndexMaintainer> indexMaintainers = localIndexBytes == null ? null : IndexMaintainer.deserialize(localIndexBytes);
+            indexMaintainer = indexMaintainers.get(0);
+            viewConstants = IndexUtil.deserializeViewConstantsFromScan(scan);
+        }
+        innerScanner =
+                getWrappedScanner(c, innerScanner, arrayKVRefs, arrayFuncRefs, offset, scan,
+                    dataColumns, tupleProjector, dataRegion, indexMaintainer, viewConstants);
+        final OrderedResultIterator iterator = deserializeFromScan(scan,innerScanner);  
         if (iterator == null) {
             return innerScanner;
         }
         // TODO:the above wrapped scanner should be used here also
         return getTopNScanner(c, innerScanner, iterator, tenantId);
     }
-    
+
     /**
      *  Return region scanner that does TopN.
      *  We only need to call startRegionOperation and closeRegionOperation when
@@ -292,9 +307,18 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
      * @param arrayFuncRefs 
      * @param arrayKVRefs 
      * @param offset starting position in the rowkey.
+     * @param scan
+     * @param tupleProjector
+     * @param dataRegion
+     * @param indexMaintainer
+     * @param viewConstants
      */
-    private RegionScanner getWrappedScanner(final ObserverContext<RegionCoprocessorEnvironment> c, final RegionScanner s, 
-           final List<KeyValueColumnExpression> arrayKVRefs, final Expression[] arrayFuncRefs, final int offset) {
+    private RegionScanner getWrappedScanner(final ObserverContext<RegionCoprocessorEnvironment> c,
+            final RegionScanner s, final List<KeyValueColumnExpression> arrayKVRefs,
+            final Expression[] arrayFuncRefs, final int offset, final Scan scan,
+            final ColumnReference[] dataColumns, final TupleProjector tupleProjector,
+            final HRegion dataRegion, final IndexMaintainer indexMaintainer,
+            final byte[][] viewConstants) {
         return new RegionScanner() {
 
             @Override
@@ -352,8 +376,8 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
                     if (arrayFuncRefs != null && arrayFuncRefs.length > 0 && arrayKVRefs.size() > 0) {
                         replaceArrayIndexElement(arrayKVRefs, arrayFuncRefs, result);
                     }
-                    if (offset > 0) {
-                        wrapResultUsingOffset(result,offset);
+                    if (ScanUtil.isLocalIndex(scan)) {
+                        IndexUtil.wrapResultUsingOffset(result, offset, dataColumns, tupleProjector, dataRegion, indexMaintainer, viewConstants, ptr);
                     }
                     // There is a scanattribute set to retrieve the specific array element
                     return next;
@@ -373,8 +397,8 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
                     if (arrayFuncRefs != null && arrayFuncRefs.length > 0 && arrayKVRefs.size() > 0) { 
                         replaceArrayIndexElement(arrayKVRefs, arrayFuncRefs, result);
                     }
-                    if (offset > 0) {
-                        wrapResultUsingOffset(result,offset);
+                    if (offset > 0 || ScanUtil.isLocalIndex(scan)) {
+                        IndexUtil.wrapResultUsingOffset(result, offset, dataColumns, tupleProjector, dataRegion, indexMaintainer, viewConstants, ptr);
                     }
                     // There is a scanattribute set to retrieve the specific array element
                     return next;
@@ -417,133 +441,10 @@ public class ScanRegionObserver extends BaseScannerRegionObserver {
                         Type.codeToType(rowKv.getTypeByte()), value, 0, value.length));
             }
 
-            private void wrapResultUsingOffset(List<Cell> result, final int offset) {
-                for (int i = 0; i < result.size(); i++) {
-                    final Cell cell = result.get(i);
-                    // TODO: Create DelegateCell class instead
-                    Cell newCell = new Cell() {
-
-                        @Override
-                        public byte[] getRowArray() {
-                            return cell.getRowArray();
-                        }
-
-                        @Override
-                        public int getRowOffset() {
-                            return cell.getRowOffset() + offset;
-                        }
-
-                        @Override
-                        public short getRowLength() {
-                            return (short)(cell.getRowLength() - offset);
-                        }
-
-                        @Override
-                        public byte[] getFamilyArray() {
-                            return cell.getFamilyArray();
-                        }
-
-                        @Override
-                        public int getFamilyOffset() {
-                            return cell.getFamilyOffset();
-                        }
-
-                        @Override
-                        public byte getFamilyLength() {
-                            return cell.getFamilyLength();
-                        }
-
-                        @Override
-                        public byte[] getQualifierArray() {
-                            return cell.getQualifierArray();
-                        }
-
-                        @Override
-                        public int getQualifierOffset() {
-                            return cell.getQualifierOffset();
-                        }
-
-                        @Override
-                        public int getQualifierLength() {
-                            return cell.getQualifierLength();
-                        }
-
-                        @Override
-                        public long getTimestamp() {
-                            return cell.getTimestamp();
-                        }
-
-                        @Override
-                        public byte getTypeByte() {
-                            return cell.getTypeByte();
-                        }
-
-                        @Override
-                        public long getMvccVersion() {
-                            return cell.getMvccVersion();
-                        }
-
-                        @Override
-                        public byte[] getValueArray() {
-                            return cell.getValueArray();
-                        }
-
-                        @Override
-                        public int getValueOffset() {
-                            return cell.getValueOffset();
-                        }
-
-                        @Override
-                        public int getValueLength() {
-                            return cell.getValueLength();
-                        }
-
-                        @Override
-                        public byte[] getTagsArray() {
-                            return cell.getTagsArray();
-                        }
-
-                        @Override
-                        public int getTagsOffset() {
-                            return cell.getTagsOffset();
-                        }
-
-                        @Override
-                        public short getTagsLength() {
-                            return cell.getTagsLength();
-                        }
-
-                        @Override
-                        public byte[] getValue() {
-                            return cell.getValue();
-                        }
-
-                        @Override
-                        public byte[] getFamily() {
-                            return cell.getFamily();
-                        }
-
-                        @Override
-                        public byte[] getQualifier() {
-                            return cell.getQualifier();
-                        }
-
-                        @Override
-                        public byte[] getRow() {
-                            return cell.getRow();
-                        }
-                    };
-                    // Wrap cell in cell that offsets row key
-                    result.set(i, newCell);
-                }
-            }
-
             @Override
             public long getMaxResultSize() {
                 return s.getMaxResultSize();
             }
         };
     }
-    
-    
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
index 1a26ab5..d8961c0 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/coprocessor/UngroupedAggregateRegionObserver.java
@@ -21,7 +21,6 @@ import static org.apache.phoenix.query.QueryConstants.AGG_TIMESTAMP;
 import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN;
 import static org.apache.phoenix.query.QueryConstants.SINGLE_COLUMN_FAMILY;
 import static org.apache.phoenix.query.QueryConstants.UNGROUPED_AGG_ROW_KEY;
-import static org.apache.phoenix.query.QueryConstants.EMPTY_COLUMN_BYTES_PTR;
 import static org.apache.phoenix.query.QueryServices.MUTATE_BATCH_SIZE_ATTRIB;
 
 import java.io.ByteArrayInputStream;
@@ -32,7 +31,6 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 
@@ -60,9 +58,8 @@ import org.apache.phoenix.expression.aggregator.Aggregator;
 import org.apache.phoenix.expression.aggregator.Aggregators;
 import org.apache.phoenix.expression.aggregator.ServerAggregators;
 import org.apache.phoenix.hbase.index.ValueGetter;
+import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
 import org.apache.phoenix.hbase.index.util.GenericKeyValueBuilder;
-import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
-import org.apache.phoenix.hbase.index.util.IndexManagementUtil;
 import org.apache.phoenix.hbase.index.util.KeyValueBuilder;
 import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.index.PhoenixIndexCodec;
@@ -79,6 +76,7 @@ import org.apache.phoenix.schema.PTableImpl;
 import org.apache.phoenix.schema.SortOrder;
 import org.apache.phoenix.schema.tuple.MultiKeyValueTuple;
 import org.apache.phoenix.util.ByteUtil;
+import org.apache.phoenix.util.IndexUtil;
 import org.apache.phoenix.util.KeyValueUtil;
 import org.apache.phoenix.util.MetaDataUtil;
 import org.apache.phoenix.util.ScanUtil;
@@ -97,7 +95,7 @@ import com.google.common.collect.Sets;
  * @since 0.1
  */
 public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver {
-
+    private ImmutableBytesWritable ptr = new ImmutableBytesWritable();
     // TODO: move all constants into a single class
     public static final String UNGROUPED_AGG = "UngroupedAgg";
     public static final String DELETE_AGG = "DeleteAgg";
@@ -138,10 +136,21 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
         if (isUngroupedAgg == null) {
             return s;
         }
+        int offset = 0;
+        if (ScanUtil.isLocalIndex(scan)) {
+            /*
+             * For local indexes, we need to set an offset on row key expressions to skip
+             * the region start key.
+             */
+            HRegion region = c.getEnvironment().getRegion();
+            offset = region.getStartKey().length != 0 ? region.getStartKey().length:region.getEndKey().length;
+            ScanUtil.setRowKeyOffset(scan, offset);
+        }
 
         byte[] localIndexBytes = scan.getAttribute(LOCAL_INDEX_BUILD);
         List<IndexMaintainer> indexMaintainers = localIndexBytes == null ? null : IndexMaintainer.deserialize(localIndexBytes);
         List<Mutation> indexMutations = localIndexBytes == null ? Collections.<Mutation>emptyList() : Lists.<Mutation>newArrayListWithExpectedSize(1024);
+        boolean localIndexScan = ScanUtil.isLocalIndex(scan);
         
         final TupleProjector p = TupleProjector.deserializeProjectorFromScan(scan);
         final HashJoinInfo j = HashJoinInfo.deserializeHashJoinFromScan(scan);
@@ -149,7 +158,6 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
         if (p != null || j != null)  {
             theScanner = new HashJoinRegionScanner(s, p, j, ScanUtil.getTenantId(scan), c.getEnvironment());
         }
-        final RegionScanner innerScanner = theScanner;
         
         byte[] indexUUID = scan.getAttribute(PhoenixIndexCodec.INDEX_UUID);
         PTable projectedTable = null;
@@ -180,6 +188,19 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
         if(localIndexBytes != null) {
             ptr = new ImmutableBytesWritable();
         }
+        TupleProjector scanProjector = null;
+        HRegion dataRegion = null;
+        byte[][] viewConstants = null;
+        ColumnReference[] dataColumns = IndexUtil.deserializeDataTableColumnsToJoin(scan);
+        final RegionScanner innerScanner;
+        if (ScanUtil.isLocalIndex(scan) && !isDelete) {
+            if (dataColumns != null) {
+                scanProjector = IndexUtil.getTupleProjector(scan, dataColumns);
+                dataRegion = IndexUtil.getDataRegion(c.getEnvironment());
+                viewConstants = IndexUtil.deserializeViewConstantsFromScan(scan);
+            }
+        } 
+        innerScanner = theScanner;
         
         int batchSize = 0;
         long ts = scan.getTimeRange().getMax();
@@ -201,6 +222,7 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
         }
         long rowCount = 0;
         region.startRegionOperation();
+        ImmutableBytesWritable tempPtr = new ImmutableBytesWritable();
         try {
             do {
                 List<Cell> results = new ArrayList<Cell>();
@@ -208,25 +230,23 @@ public class UngroupedAggregateRegionObserver extends BaseScannerRegionObserver
                 // since this is an indication of whether or not there are more values after the
                 // ones returned
                 hasMore = innerScanner.nextRaw(results);
+                
                 if (!results.isEmpty()) {
+                    if (localIndexScan && !isDelete) {
+                        IndexUtil.wrapResultUsingOffset(results, offset, dataColumns, scanProjector,
+                            dataRegion, indexMaintainers == null ? null : indexMaintainers.get(0),
+                            viewConstants, tempPtr);
+                    }
                     rowCount++;
                     result.setKeyValues(results);
                     try {
-                        if (indexMaintainers != null) {
+                        if (indexMaintainers != null && dataColumns==null && !localIndexScan) {
+                            // TODO: join back to data row here if scan attribute set
                             for (IndexMaintainer maintainer : indexMaintainers) {
-                                ImmutableBytesPtr emptyKeyValueFamily = maintainer.getEmptyKeyValueFamily();
-                                Iterator<Cell> iterator = results.iterator();
-                                while (iterator.hasNext()) {
-                                    Cell cell = iterator.next();
-                                    if (Bytes.compareTo(cell.getRowArray(), cell.getFamilyOffset(), cell.getFamilyLength(), emptyKeyValueFamily.get(), emptyKeyValueFamily.getOffset(), emptyKeyValueFamily.getLength()) == 0
-                                            && Bytes.compareTo(cell.getRowArray(), cell.getQualifierOffset(), cell.getQualifierLength(), EMPTY_COLUMN_BYTES_PTR.get(), EMPTY_COLUMN_BYTES_PTR.getOffset(), EMPTY_COLUMN_BYTES_PTR.getLength()) == 0) {
-                                        iterator.remove();
-                                    }
-                                }
                                 if (!results.isEmpty()) {
                                     result.getKey(ptr);
-                                    ValueGetter valueGetter = IndexManagementUtil.createGetterFromKeyValues(results);
-                                    Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts, c.getEnvironment().getRegion().getStartKey());
+                                    ValueGetter valueGetter = maintainer.createGetterFromKeyValues(results);
+                                    Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts, c.getEnvironment().getRegion().getStartKey(), c.getEnvironment().getRegion().getEndKey());
                                     indexMutations.add(put);
                                 }
                             }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java b/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java
index 17a1824..be96956 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/execute/BasicQueryPlan.java
@@ -17,27 +17,47 @@
  */
 package org.apache.phoenix.execute;
 
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
 import java.sql.ParameterMetaData;
 import java.sql.SQLException;
 import java.util.Collections;
 import java.util.List;
+import java.util.Set;
 
 import org.apache.hadoop.hbase.HConstants;
 import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.WritableUtils;
 import org.apache.phoenix.compile.ExplainPlan;
 import org.apache.phoenix.compile.GroupByCompiler.GroupBy;
 import org.apache.phoenix.compile.OrderByCompiler.OrderBy;
+import org.apache.phoenix.compile.FromCompiler;
 import org.apache.phoenix.compile.QueryPlan;
 import org.apache.phoenix.compile.RowProjector;
 import org.apache.phoenix.compile.ScanRanges;
 import org.apache.phoenix.compile.StatementContext;
+import org.apache.phoenix.coprocessor.BaseScannerRegionObserver;
+import org.apache.phoenix.expression.ProjectedColumnExpression;
+import org.apache.phoenix.index.IndexMaintainer;
 import org.apache.phoenix.iterate.DelegateResultIterator;
 import org.apache.phoenix.iterate.ParallelIterators.ParallelIteratorFactory;
 import org.apache.phoenix.iterate.ResultIterator;
 import org.apache.phoenix.jdbc.PhoenixConnection;
 import org.apache.phoenix.parse.FilterableStatement;
+import org.apache.phoenix.parse.ParseNodeFactory;
+import org.apache.phoenix.parse.TableName;
+import org.apache.phoenix.schema.KeyValueSchema;
+import org.apache.phoenix.schema.PColumn;
+import org.apache.phoenix.schema.PName;
+import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.TableRef;
+import org.apache.phoenix.util.ByteUtil;
+import org.apache.phoenix.util.IndexUtil;
 import org.apache.phoenix.util.SQLCloseable;
 import org.apache.phoenix.util.SQLCloseables;
 import org.apache.phoenix.util.ScanUtil;
@@ -153,8 +173,34 @@ public abstract class BasicQueryPlan implements QueryPlan {
         }
         ScanUtil.setTimeRange(scan, scn);
         ScanUtil.setTenantId(scan, connection.getTenantId() == null ? null : connection.getTenantId().getBytes());
+        // Set local index related scan attributes. 
         if (context.getCurrentTable().getTable().getIndexType() == IndexType.LOCAL) {
             ScanUtil.setLocalIndex(scan);
+            Set<PColumn> dataColumns = context.getDataColumns();
+            // If any data columns to join back from data table are present then we set following attributes
+            // 1. data columns to be projected and their key value schema.
+            // 2. index maintainer and view constants if exists to build data row key from index row key. 
+            // TODO: can have an hint to skip joining back to data table, in that case if any column to
+            // project is not present in the index then we need to skip this plan.
+            if (!dataColumns.isEmpty()) {
+                // Set data columns to be join back from data table.
+                serializeDataTableColumnsToJoin(scan, dataColumns);
+                KeyValueSchema schema = ProjectedColumnExpression.buildSchema(dataColumns);
+                // Set key value schema of the data columns.
+                serializeSchemaIntoScan(scan, schema);
+                String schemaName = context.getCurrentTable().getTable().getSchemaName().getString();
+                String parentTable = context.getCurrentTable().getTable().getParentTableName().getString();
+                final ParseNodeFactory FACTORY = new ParseNodeFactory();
+                TableRef dataTableRef =
+                        FromCompiler.getResolver(
+                            FACTORY.namedTable(null, TableName.create(schemaName, parentTable)),
+                            context.getConnection()).resolveTable(schemaName, parentTable);
+                PTable dataTable = dataTableRef.getTable();
+                // Set index maintainer of the local index.
+                serializeIndexMaintainerIntoScan(scan, dataTable);
+                // Set view constants if exists.
+                serializeViewConstantsIntoScan(scan, dataTable);
+            }
         }
         ResultIterator iterator = newIterator();
         return dependencies.isEmpty() ? 
@@ -170,6 +216,108 @@ public abstract class BasicQueryPlan implements QueryPlan {
         };
     }
 
+    private void serializeIndexMaintainerIntoScan(Scan scan, PTable dataTable) {
+        PName name = context.getCurrentTable().getTable().getName();
+        List<PTable> indexes = Lists.newArrayListWithExpectedSize(1);
+        for (PTable index : dataTable.getIndexes()) {
+            if (index.getName().equals(name) && index.getIndexType() == IndexType.LOCAL) {
+                indexes.add(index);
+                break;
+            }
+        }
+        ImmutableBytesWritable ptr = new ImmutableBytesWritable();
+        IndexMaintainer.serialize(dataTable, ptr, indexes);
+        scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX_BUILD, ByteUtil.copyKeyBytesIfNecessary(ptr));
+    }
+
+    private void serializeViewConstantsIntoScan(Scan scan, PTable dataTable) {
+        int dataPosOffset = (dataTable.getBucketNum() != null ? 1 : 0) + (dataTable.isMultiTenant() ? 1 : 0);
+        int nViewConstants = 0;
+        if (dataTable.getType() == PTableType.VIEW) {
+            ImmutableBytesWritable ptr = new ImmutableBytesWritable();
+            List<PColumn> dataPkColumns = dataTable.getPKColumns();
+            for (int i = dataPosOffset; i < dataPkColumns.size(); i++) {
+                PColumn dataPKColumn = dataPkColumns.get(i);
+                if (dataPKColumn.getViewConstant() != null) {
+                    nViewConstants++;
+                }
+            }
+            if (nViewConstants > 0) {
+                byte[][] viewConstants = new byte[nViewConstants][];
+                int j = 0;
+                for (int i = dataPosOffset; i < dataPkColumns.size(); i++) {
+                    PColumn dataPkColumn = dataPkColumns.get(i);
+                    if (dataPkColumn.getViewConstant() != null) {
+                        if (IndexUtil.getViewConstantValue(dataPkColumn, ptr)) {
+                            viewConstants[j++] = ByteUtil.copyKeyBytesIfNecessary(ptr);
+                        } else {
+                            throw new IllegalStateException();
+                        }
+                    }
+                }
+                serializeViewConstantsIntoScan(viewConstants, scan);
+            }
+        }
+    }
+
+    private void serializeViewConstantsIntoScan(byte[][] viewConstants, Scan scan) {
+        ByteArrayOutputStream stream = new ByteArrayOutputStream();
+        try {
+            DataOutputStream output = new DataOutputStream(stream);
+            WritableUtils.writeVInt(output, viewConstants.length);
+            for (byte[] viewConstant : viewConstants) {
+                Bytes.writeByteArray(output, viewConstant);
+            }
+            scan.setAttribute(BaseScannerRegionObserver.VIEW_CONSTANTS, stream.toByteArray());
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
+    private void serializeDataTableColumnsToJoin(Scan scan, Set<PColumn> dataColumns) {
+        ByteArrayOutputStream stream = new ByteArrayOutputStream();
+        try {
+            DataOutputStream output = new DataOutputStream(stream);
+            WritableUtils.writeVInt(output, dataColumns.size());
+            for (PColumn column : dataColumns) {
+                Bytes.writeByteArray(output, column.getFamilyName().getBytes());
+                Bytes.writeByteArray(output, column.getName().getBytes());
+            }
+            scan.setAttribute(BaseScannerRegionObserver.DATA_TABLE_COLUMNS_TO_JOIN, stream.toByteArray());
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
+    private void serializeSchemaIntoScan(Scan scan, KeyValueSchema schema) {
+        ByteArrayOutputStream stream = new ByteArrayOutputStream(schema.getEstimatedByteSize());
+        try {
+            DataOutputStream output = new DataOutputStream(stream);
+            schema.write(output);
+            scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX_JOIN_SCHEMA, stream.toByteArray());
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
     abstract protected ResultIterator newIterator() throws SQLException;
     
     @Override
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/execute/ScanPlan.java b/phoenix-core/src/main/java/org/apache/phoenix/execute/ScanPlan.java
index a994067..f9af543 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/execute/ScanPlan.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/execute/ScanPlan.java
@@ -42,6 +42,7 @@ import org.apache.phoenix.query.QueryServicesOptions;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.SaltingUtil;
 import org.apache.phoenix.schema.TableRef;
+import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.util.ScanUtil;
 
 import java.sql.SQLException;
@@ -115,7 +116,7 @@ public class ScanPlan extends BasicQueryPlan {
         if (isOrdered) {
             scanner = new MergeSortTopNResultIterator(iterators, limit, orderBy.getOrderByExpressions());
         } else {
-            if (isSalted &&
+            if ((isSalted || table.getIndexType() == IndexType.LOCAL) &&
                     (context.getConnection().getQueryServices().getProps().getBoolean(
                             QueryServices.ROW_KEY_ORDER_SALTED_TABLE_ATTRIB,
                             QueryServicesOptions.DEFAULT_ROW_KEY_ORDER_SALTED_TABLE) ||
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
index 102eb86..6847c9b 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
@@ -169,7 +169,7 @@ public enum ExpressionType {
     SQLViewTypeFunction(SQLViewTypeFunction.class),
     ExternalSqlTypeIdFunction(ExternalSqlTypeIdFunction.class),
     ConvertTimezoneFunction(ConvertTimezoneFunction.class),
-    SQLIndexTypeFunction(SQLIndexTypeFunction.class);
+    SQLIndexTypeFunction(SQLIndexTypeFunction.class),
     DecodeFunction(DecodeFunction.class),
     TimezoneOffsetFunction(TimezoneOffsetFunction.class),
     EncodeFunction(EncodeFunction.class),
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/KeyValueColumnExpression.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/KeyValueColumnExpression.java
index 0e35bcb..4b5fdbb 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/KeyValueColumnExpression.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/KeyValueColumnExpression.java
@@ -26,6 +26,7 @@ import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.phoenix.expression.visitor.ExpressionVisitor;
 import org.apache.phoenix.schema.PColumn;
+import org.apache.phoenix.schema.PDatum;
 import org.apache.phoenix.schema.tuple.Tuple;
 import org.apache.phoenix.util.SchemaUtil;
 
@@ -49,6 +50,12 @@ public class KeyValueColumnExpression extends ColumnExpression {
         this(column, null);
     }
 
+    public KeyValueColumnExpression(PDatum column, byte[] cf, byte[] cq) {
+        super(column);
+        this.cf = cf;
+        this.cq = cq;
+    }
+
     public KeyValueColumnExpression(PColumn column, String displayName) {
         super(column);
         this.cf = column.getFamilyName().getBytes();
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/ProjectedColumnExpression.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/ProjectedColumnExpression.java
index dcac849..1c4232a 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/ProjectedColumnExpression.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/ProjectedColumnExpression.java
@@ -20,6 +20,7 @@ package org.apache.phoenix.expression;
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
+import java.util.Collection;
 
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.phoenix.expression.visitor.ExpressionVisitor;
@@ -49,12 +50,24 @@ public class ProjectedColumnExpression extends ColumnExpression {
 		this.displayName = displayName;
 	}
     
-    private static KeyValueSchema buildSchema(PTable table) {
-    	KeyValueSchemaBuilder builder = new KeyValueSchemaBuilder(0);
-        for (PColumn column : table.getColumns()) {
-        	if (!SchemaUtil.isPKColumn(column)) {
-        		builder.addField(column);
-        	}
+    public ProjectedColumnExpression(PColumn column, Collection<PColumn> columns, int position, String displayName) {
+        super(column);
+        this.schema = buildSchema(columns);
+        this.bitSet = ValueBitSet.newInstance(schema);
+        this.position = position;
+        this.displayName = displayName;
+    }
+
+	private static KeyValueSchema buildSchema(PTable table) {
+        return buildSchema(table.getColumns());
+    }
+    
+    public static KeyValueSchema buildSchema(Collection<PColumn> columns) {
+        KeyValueSchemaBuilder builder = new KeyValueSchemaBuilder(0);
+        for (PColumn column : columns) {
+            if (!SchemaUtil.isPKColumn(column)) {
+                builder.addField(column);
+            }
         }
         return builder.build();
     }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java b/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
index b451ad6..8172956 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/filter/SkipScanFilter.java
@@ -118,7 +118,7 @@ public class SkipScanFilter extends FilterBase implements Writable {
 
     @Override
     public ReturnCode filterKeyValue(Cell kv) {
-        return navigate(kv.getRowArray(), kv.getRowOffset() + offset,kv.getRowLength(),Terminate.AFTER);
+        return navigate(kv.getRowArray(), kv.getRowOffset() + offset,kv.getRowLength()- offset,Terminate.AFTER);
     }
 
     @Override
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/util/IndexManagementUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/util/IndexManagementUtil.java
index 4cbf6dd..db38515 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/util/IndexManagementUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/hbase/index/util/IndexManagementUtil.java
@@ -120,11 +120,11 @@ public class IndexManagementUtil {
         };
     }
 
-    private static class ReferencingColumn {
+    public static class ReferencingColumn {
         ImmutableBytesPtr family;
         ImmutableBytesPtr qual;
 
-        static ReferencingColumn wrap(ColumnReference ref) {
+        public static ReferencingColumn wrap(ColumnReference ref) {
             ImmutableBytesPtr family = new ImmutableBytesPtr(ref.getFamily());
             ImmutableBytesPtr qual = new ImmutableBytesPtr(ref.getQualifier());
             return new ReferencingColumn(family, qual);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
index 3583122..ef8525c 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
@@ -31,6 +31,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.CellUtil;
 import org.apache.hadoop.hbase.KeyValue;
 import org.apache.hadoop.hbase.KeyValue.Type;
@@ -45,10 +46,12 @@ import org.apache.phoenix.hbase.index.ValueGetter;
 import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
 import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
 import org.apache.phoenix.hbase.index.util.KeyValueBuilder;
+import org.apache.phoenix.hbase.index.util.IndexManagementUtil.ReferencingColumn;
 import org.apache.phoenix.query.QueryConstants;
 import org.apache.phoenix.schema.PColumn;
 import org.apache.phoenix.schema.PColumnFamily;
 import org.apache.phoenix.schema.PDataType;
+import org.apache.phoenix.schema.PDatum;
 import org.apache.phoenix.schema.PIndexState;
 import org.apache.phoenix.schema.PTable;
 import org.apache.phoenix.schema.PTable.IndexType;
@@ -150,8 +153,20 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
      */
     public static void serialize(PTable dataTable, ImmutableBytesWritable ptr,
             List<PTable> indexes) {
+        serialize(dataTable, ptr, indexes, false);
+    }
+
+    /**
+     * For client-side to serialize all IndexMaintainers for a given table
+     * @param dataTable data table
+     * @param ptr bytes pointer to hold returned serialized value
+     * @param indexes indexes to serialize
+     * @param serializeAlways true means serialize even in case of immutable indexing.
+     */
+    public static void serialize(PTable dataTable, ImmutableBytesWritable ptr,
+            List<PTable> indexes, boolean serializeAlways) {
         Iterator<PTable> indexesItr = nonDisabledIndexIterator(indexes.iterator());
-        if (dataTable.isImmutableRows() || !indexesItr.hasNext()) {
+        if ((dataTable.isImmutableRows() && serializeAlways) || !indexesItr.hasNext()) {
             ptr.set(ByteUtil.EMPTY_BYTE_ARRAY);
             return;
         }
@@ -244,16 +259,29 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         this(dataTable.getRowKeySchema(), dataTable.getBucketNum() != null);
         this.isMultiTenant = dataTable.isMultiTenant();
         this.viewIndexId = index.getViewIndexId() == null ? null : MetaDataUtil.getViewIndexIdDataType().toBytes(index.getViewIndexId());
+        this.isLocalIndex = index.getIndexType() == IndexType.LOCAL;
 
         RowKeySchema dataRowKeySchema = dataTable.getRowKeySchema();
         boolean isDataTableSalted = dataTable.getBucketNum() != null;
         byte[] indexTableName = index.getPhysicalName().getBytes();
-        Integer nIndexSaltBuckets = index.getBucketNum();
+        // Use this for the nDataSaltBuckets as we need this for local indexes
+        // TODO: persist nDataSaltBuckets separately, but maintain b/w compat.
+        Integer nIndexSaltBuckets = isLocalIndex ? dataTable.getBucketNum() : index.getBucketNum();
         boolean indexWALDisabled = index.isWALDisabled();
         int indexPosOffset = (index.getBucketNum() == null ? 0 : 1) + (this.isMultiTenant ? 1 : 0) + (this.viewIndexId == null ? 0 : 1);
         int nIndexColumns = index.getColumns().size() - indexPosOffset;
         int nIndexPKColumns = index.getPKColumns().size() - indexPosOffset;
-        this.rowKeyMetaData = newRowKeyMetaData(nIndexPKColumns);
+        int indexedColumnsCount = 0;
+        for (int i  = indexPosOffset; i<index.getPKColumns().size();i++) {
+            PColumn indexColumn = index.getPKColumns().get(i);
+            PColumn column = IndexUtil.getDataColumn(dataTable, indexColumn.getName().getString());
+            boolean isPKColumn = SchemaUtil.isPKColumn(column);
+            if (!isPKColumn) {
+                indexedColumnsCount++;
+            } 
+        }
+        int indexPkColumnCount = this.dataRowKeySchema.getFieldCount() + indexedColumnsCount - (isDataTableSalted ? 1 : 0) - (isMultiTenant ? 1 : 0);
+        this.rowKeyMetaData = newRowKeyMetaData(indexPkColumnCount);
         BitSet bitSet = this.rowKeyMetaData.getViewConstantColumnBitSet();
 
         int dataPosOffset = (isDataTableSalted ? 1 : 0) + (this.isMultiTenant ? 1 : 0);
@@ -283,26 +311,34 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         this.emptyKeyValueCFPtr = SchemaUtil.getEmptyColumnFamilyPtr(index);
         this.nDataCFs = dataTable.getColumnFamilies().size();
         this.indexWALDisabled = indexWALDisabled;
-        this.isLocalIndex = index.getIndexType() == IndexType.LOCAL;
     }
 
-    public byte[] buildRowKey(ValueGetter valueGetter, ImmutableBytesWritable rowKeyPtr, byte[] regionStartKey)  {
+    public byte[] buildRowKey(ValueGetter valueGetter, ImmutableBytesWritable rowKeyPtr, byte[] regionStartKey, byte[] regionEndKey)  {
         ImmutableBytesWritable ptr = new ImmutableBytesWritable();
         boolean prependRegionStartKey = isLocalIndex && regionStartKey != null;
-        TrustedByteArrayOutputStream stream = new TrustedByteArrayOutputStream(estimatedIndexRowKeyBytes + (prependRegionStartKey ? regionStartKey.length : 0));
+        boolean isIndexSalted = !isLocalIndex && nIndexSaltBuckets > 0;
+        int prefixKeyLength =
+                prependRegionStartKey ? (regionStartKey.length != 0 ? regionStartKey.length
+                        : regionEndKey.length) : 0; 
+        TrustedByteArrayOutputStream stream = new TrustedByteArrayOutputStream(estimatedIndexRowKeyBytes + (prependRegionStartKey ? prefixKeyLength : 0));
         DataOutput output = new DataOutputStream(stream);
         try {
             // For local indexes, we must prepend the row key with the start region key
             if (prependRegionStartKey) {
-                output.write(regionStartKey);
+                if (regionStartKey.length == 0) {
+                    output.write(new byte[prefixKeyLength]);
+                } else {
+                    output.write(regionStartKey);
+                }
             }
-            if (nIndexSaltBuckets > 0) {
+            if (isIndexSalted) {
                 output.write(0); // will be set at end to index salt byte
             }
             // The dataRowKeySchema includes the salt byte field,
             // so we must adjust for that here.
             int dataPosOffset = isDataTableSalted ? 1 : 0 ;
-            int nIndexedColumns = getIndexPkColumnCount();
+            BitSet viewConstantColumnBitSet = this.rowKeyMetaData.getViewConstantColumnBitSet();
+            int nIndexedColumns = getIndexPkColumnCount() - getNumViewConstants();
             int[][] dataRowKeyLocator = new int[2][nIndexedColumns];
             // Skip data table salt byte
             int maxRowKeyOffset = rowKeyPtr.getOffset() + rowKeyPtr.getLength();
@@ -319,7 +355,6 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
                 output.write(viewIndexId);
             }
             
-            BitSet viewConstantColumnBitSet = this.rowKeyMetaData.getViewConstantColumnBitSet();
             // Write index row key
             for (int i = dataPosOffset; i < dataRowKeySchema.getFieldCount(); i++) {
                 Boolean hasValue=dataRowKeySchema.next(ptr, i, maxRowKeyOffset);
@@ -335,7 +370,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
                         dataRowKeyLocator[0][pos] = 0;
                         dataRowKeyLocator[1][pos] = 0;
                     }
-                }
+                } 
             }
             BitSet descIndexColumnBitSet = rowKeyMetaData.getDescIndexColumnBitSet();
             int j = 0;
@@ -387,7 +422,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
             while (length > minLength && indexRowKey[length-1] == QueryConstants.SEPARATOR_BYTE) {
                 length--;
             }
-            if (nIndexSaltBuckets > 0) {
+            if (isIndexSalted) {
                 // Set salt byte
                 byte saltByte = SaltingUtil.getSaltingByte(indexRowKey, SaltingUtil.NUM_SALTING_BYTES, length-SaltingUtil.NUM_SALTING_BYTES, nIndexSaltBuckets);
                 indexRowKey[0] = saltByte;
@@ -404,11 +439,229 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         }
     }
 
-    public Put buildUpdateMutation(KeyValueBuilder kvBuilder, ValueGetter valueGetter, ImmutableBytesWritable dataRowKeyPtr, long ts, byte[] regionStartKey) throws IOException {
+    /*
+     * Build the data row key from the index row key
+     */
+    public byte[] buildDataRowKey(ImmutableBytesWritable indexRowKeyPtr, byte[][] viewConstants)  {
+        RowKeySchema indexRowKeySchema = getIndexRowKeySchema();
+        ImmutableBytesWritable ptr = new ImmutableBytesWritable();
+        TrustedByteArrayOutputStream stream = new TrustedByteArrayOutputStream(estimatedIndexRowKeyBytes);
+        DataOutput output = new DataOutputStream(stream);
+        // Increment dataPosOffset until all have been written
+        int dataPosOffset = 0;
+        int viewConstantsIndex = 0;
+        try {
+            int indexPosOffset = !isLocalIndex && nIndexSaltBuckets > 0 ? 1 : 0;
+            int maxRowKeyOffset = indexRowKeyPtr.getOffset() + indexRowKeyPtr.getLength();
+            indexRowKeySchema.iterator(indexRowKeyPtr, ptr, indexPosOffset);
+            if (isDataTableSalted) {
+                dataPosOffset++;
+                output.write(0); // will be set at end to salt byte
+            }
+            if (isMultiTenant) {
+                indexRowKeySchema.next(ptr, indexPosOffset, maxRowKeyOffset);
+                output.write(ptr.get(), ptr.getOffset(), ptr.getLength());
+                if (!dataRowKeySchema.getField(dataPosOffset).getDataType().isFixedWidth()) {
+                    output.writeByte(QueryConstants.SEPARATOR_BYTE);
+                }
+                indexPosOffset++;
+                dataPosOffset++;
+            }
+            indexPosOffset = (!isLocalIndex && nIndexSaltBuckets > 0 ? 1 : 0) + (isMultiTenant ? 1 : 0) + (viewIndexId == null ? 0 : 1);
+            BitSet viewConstantColumnBitSet = this.rowKeyMetaData.getViewConstantColumnBitSet();
+            BitSet descIndexColumnBitSet = rowKeyMetaData.getDescIndexColumnBitSet();
+            for (int i = dataPosOffset; i < dataRowKeySchema.getFieldCount(); i++) {
+                // Write view constants from the data table, as these
+                // won't appear in the index (as they're the
+                // same for all rows in this index)
+                if (viewConstantColumnBitSet.get(i)) {
+                    output.write(viewConstants[viewConstantsIndex++]);
+                } else {
+                    int pos = rowKeyMetaData.getIndexPkPosition(i-dataPosOffset);
+                    Boolean hasValue=indexRowKeySchema.iterator(indexRowKeyPtr, ptr, pos + indexPosOffset+1);
+                    if (Boolean.TRUE.equals(hasValue)) {
+                        // Write data row key value taking into account coercion and inversion
+                        // if necessary
+                        Field dataField = dataRowKeySchema.getField(i);
+                        Field indexField = indexRowKeySchema.getField(pos + indexPosOffset);
+                        PDataType indexColumnType = indexField.getDataType();
+                        PDataType dataColumnType = dataField.getDataType();
+                        SortOrder dataSortOrder = dataField.getSortOrder();
+                        SortOrder indexSortOrder = indexField.getSortOrder();
+                        boolean isDataColumnInverted = dataSortOrder != SortOrder.ASC;
+                        boolean isBytesComparable = dataColumnType.isBytesComparableWith(indexColumnType) ;
+                        if (isBytesComparable && isDataColumnInverted == descIndexColumnBitSet.get(pos)) {
+                            output.write(ptr.get(), ptr.getOffset(), ptr.getLength());
+                        } else {
+                            if (!isBytesComparable)  {
+                                dataColumnType.coerceBytes(ptr, indexColumnType, indexSortOrder, SortOrder.getDefault());
+                            }
+                            if (descIndexColumnBitSet.get(pos) != isDataColumnInverted) {
+                                writeInverted(ptr.get(), ptr.getOffset(), ptr.getLength(), output);
+                            } else {
+                                output.write(ptr.get(), ptr.getOffset(), ptr.getLength());
+                            }
+                        }
+                    }
+                }
+                if (!dataRowKeySchema.getField(i).getDataType().isFixedWidth() && ((i+1) !=  dataRowKeySchema.getFieldCount())) {
+                    output.writeByte(QueryConstants.SEPARATOR_BYTE);
+                }
+            }
+            int length = stream.size();
+            int minLength = length - maxTrailingNulls;
+            byte[] dataRowKey = stream.getBuffer();
+            // Remove trailing nulls
+            while (length > minLength && dataRowKey[length-1] == QueryConstants.SEPARATOR_BYTE) {
+                length--;
+            }
+            // TODO: need to capture nDataSaltBuckets instead of just a boolean. For now,
+            // we store this in nIndexSaltBuckets, as we only use this function for local indexes
+            // in which case nIndexSaltBuckets would never be used. Note that when we do add this
+            // to be serialized, we have to add it at the end and allow for the value not being
+            // there to maintain compatibility between an old client and a new server.
+            assert(isLocalIndex); // Remove when we persist nDataSaltBuckets
+            if (isDataTableSalted) {
+                // Set salt byte
+                byte saltByte = SaltingUtil.getSaltingByte(dataRowKey, SaltingUtil.NUM_SALTING_BYTES, length-SaltingUtil.NUM_SALTING_BYTES, nIndexSaltBuckets);
+                dataRowKey[0] = saltByte;
+            }
+            return dataRowKey.length == length ? dataRowKey : Arrays.copyOf(dataRowKey, length);
+        } catch (IOException e) {
+            throw new RuntimeException(e); // Impossible
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e); // Impossible
+            }
+        }
+    }
+    
+    private volatile RowKeySchema indexRowKeySchema;
+    
+    // We have enough information to generate the index row key schema
+    private RowKeySchema generateIndexRowKeySchema() {
+        int nIndexedColumns = getIndexPkColumnCount() + (isMultiTenant ? 1 : 0) + (!isLocalIndex && nIndexSaltBuckets > 0 ? 1 : 0) + (viewIndexId != null ? 1 : 0) - getNumViewConstants();
+        RowKeySchema.RowKeySchemaBuilder builder = new RowKeySchema.RowKeySchemaBuilder(nIndexedColumns);
+        if (!isLocalIndex && nIndexSaltBuckets > 0) {
+            builder.addField(SaltingUtil.SALTING_COLUMN, false, SortOrder.ASC);
+            nIndexedColumns--;
+        }
+        int dataPosOffset = isDataTableSalted ? 1 : 0 ;
+        if (isMultiTenant) {
+            builder.addField(dataRowKeySchema.getField(dataPosOffset++));
+            nIndexedColumns--;
+        }
+        if (viewIndexId != null) {
+            nIndexedColumns--;
+            builder.addField(new PDatum() {
+
+                @Override
+                public boolean isNullable() {
+                    return false;
+                }
+
+                @Override
+                public PDataType getDataType() {
+                    return MetaDataUtil.getViewIndexIdDataType();
+                }
+
+                @Override
+                public Integer getMaxLength() {
+                    return null;
+                }
+
+                @Override
+                public Integer getScale() {
+                    return null;
+                }
+
+                @Override
+                public SortOrder getSortOrder() {
+                    return SortOrder.getDefault();
+                }
+                
+            }, false, SortOrder.getDefault());
+        }
+        
+        Field[] indexFields = new Field[nIndexedColumns];
+        BitSet viewConstantColumnBitSet = this.rowKeyMetaData.getViewConstantColumnBitSet();
+        // Add Field for all data row pk columns
+        for (int i = dataPosOffset; i < dataRowKeySchema.getFieldCount(); i++) {
+            // Ignore view constants from the data table, as these
+            // don't need to appear in the index (as they're the
+            // same for all rows in this index)
+            if (!viewConstantColumnBitSet.get(i)) {
+                int pos = rowKeyMetaData.getIndexPkPosition(i-dataPosOffset);
+                indexFields[pos] = dataRowKeySchema.getField(i);
+            } 
+        }
+        int indexedColumnTypesIndex = 0;
+        for (Field indexField : indexFields) {
+            if (indexField == null) { // Add field for kv column in index
+                final PDataType dataType = indexedColumnTypes.get(indexedColumnTypesIndex++);
+                builder.addField(new PDatum() {
+
+                    @Override
+                    public boolean isNullable() {
+                        return true;
+                    }
+
+                    @Override
+                    public PDataType getDataType() {
+                        return IndexUtil.getIndexColumnDataType(true, dataType);
+                    }
+
+                    @Override
+                    public Integer getMaxLength() {
+                        return null;
+                    }
+
+                    @Override
+                    public Integer getScale() {
+                        return null;
+                    }
+
+                    @Override
+                    public SortOrder getSortOrder() {
+                        return SortOrder.getDefault();
+                    }
+                    
+                }, true, SortOrder.getDefault());
+            } else { // add field from data row key
+                builder.addField(indexField);
+            }
+        }
+        return builder.build();
+    }
+    
+    private int getNumViewConstants() {
+        BitSet bitSet = this.rowKeyMetaData.getViewConstantColumnBitSet();
+        int num = 0;
+        for(int i = 0; i<dataRowKeySchema.getFieldCount();i++){
+            if(bitSet.get(i)) num++;
+        }
+        return num;
+    }
+
+    private RowKeySchema getIndexRowKeySchema() {
+        if (indexRowKeySchema != null) {
+            return indexRowKeySchema;
+        }
+        synchronized (this) {
+            if (indexRowKeySchema == null) {
+                indexRowKeySchema = generateIndexRowKeySchema();
+            }
+        }
+        return indexRowKeySchema;
+    }
+
+    public Put buildUpdateMutation(KeyValueBuilder kvBuilder, ValueGetter valueGetter, ImmutableBytesWritable dataRowKeyPtr, long ts, byte[] regionStartKey, byte[] regionEndKey) throws IOException {
         Put put = null;
         // New row being inserted: add the empty key value
         if (valueGetter.getLatestValue(dataEmptyKeyValueRef) == null) {
-            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr, regionStartKey);
+            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr, regionStartKey, regionEndKey);
             put = new Put(indexRowKey);
             // add the keyvalue for the empty row
             put.add(kvBuilder.buildPut(new ImmutableBytesPtr(indexRowKey),
@@ -420,7 +673,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         for (ColumnReference ref : this.getCoverededColumns()) {
             ImmutableBytesPtr cq = this.indexQualifiers.get(i++);
             ImmutableBytesPtr value = valueGetter.getLatestValue(ref);
-            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr, regionStartKey);
+            byte[] indexRowKey = this.buildRowKey(valueGetter, dataRowKeyPtr, regionStartKey, regionEndKey);
             ImmutableBytesPtr rowKey = new ImmutableBytesPtr(indexRowKey);
             if (value != null) {
                 if (put == null) {
@@ -487,12 +740,12 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
      * since we can build the corresponding index row key.
      */
     public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ImmutableBytesWritable dataRowKeyPtr, long ts) throws IOException {
-        return buildDeleteMutation(kvBuilder, null, dataRowKeyPtr, Collections.<KeyValue>emptyList(), ts, null);
+        return buildDeleteMutation(kvBuilder, null, dataRowKeyPtr, Collections.<KeyValue>emptyList(), ts, null, null);
     }
     
     @SuppressWarnings("deprecation")
-    public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ValueGetter oldState, ImmutableBytesWritable dataRowKeyPtr, Collection<KeyValue> pendingUpdates, long ts, byte[] regionStartKey) throws IOException {
-        byte[] indexRowKey = this.buildRowKey(oldState, dataRowKeyPtr, regionStartKey);
+    public Delete buildDeleteMutation(KeyValueBuilder kvBuilder, ValueGetter oldState, ImmutableBytesWritable dataRowKeyPtr, Collection<KeyValue> pendingUpdates, long ts, byte[] regionStartKey, byte[] regionEndKey) throws IOException {
+        byte[] indexRowKey = this.buildRowKey(oldState, dataRowKeyPtr, regionStartKey, regionEndKey);
         // Delete the entire row if any of the indexed columns changed
         if (oldState == null || isRowDeleted(pendingUpdates) || hasIndexedColumnChanged(oldState, pendingUpdates)) { // Deleting the entire row
             Delete delete = new Delete(indexRowKey, ts);
@@ -657,7 +910,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
     }
     
     private int estimateIndexRowKeyByteSize(int indexColByteSize) {
-        int estimatedIndexRowKeyBytes = indexColByteSize + dataRowKeySchema.getEstimatedValueLength() + (nIndexSaltBuckets == 0 || this.isDataTableSalted ? 0 : SaltingUtil.NUM_SALTING_BYTES);
+        int estimatedIndexRowKeyBytes = indexColByteSize + dataRowKeySchema.getEstimatedValueLength() + (nIndexSaltBuckets == 0 || isLocalIndex || this.isDataTableSalted ? 0 : SaltingUtil.NUM_SALTING_BYTES);
         return estimatedIndexRowKeyBytes;
    }
     
@@ -683,11 +936,14 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         int nIndexPkColumns = getIndexPkColumnCount();
         dataPkPosition = new int[nIndexPkColumns];
         Arrays.fill(dataPkPosition, -1);
+        int numViewConstantColumns = 0;
         BitSet viewConstantColumnBitSet = rowKeyMetaData.getViewConstantColumnBitSet();
         for (int i = dataPkOffset; i < dataRowKeySchema.getFieldCount(); i++) {
             if (!viewConstantColumnBitSet.get(i)) {
                 int dataPkPosition = rowKeyMetaData.getIndexPkPosition(i-dataPkOffset);
                 this.dataPkPosition[dataPkPosition] = i;
+            } else {
+                numViewConstantColumns++;
             }
         }
         
@@ -695,7 +951,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
         // We only get rid of nulls for variable length types, so we have to be careful to consider the type of the
         // index table, not the data type of the data table
         int indexedColumnTypesPos = indexedColumnTypes.size()-1;
-        int indexPkPos = nIndexPkColumns-1;
+        int indexPkPos = nIndexPkColumns - numViewConstantColumns - 1;
         while (indexPkPos >= 0) {
             int dataPkPos = dataPkPosition[indexPkPos];
             boolean isDataNullable;
@@ -718,7 +974,7 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
     }
 
     private int getIndexPkColumnCount() {
-        return dataRowKeySchema.getFieldCount() + indexedColumns.size() - (isDataTableSalted ? 1 : 0) - (isMultiTenant ? 1 : 0) /*+ (viewIndexId == null ? 0 : 1)*/;
+        return dataRowKeySchema.getFieldCount() + indexedColumns.size() - (isDataTableSalted ? 1 : 0) - (isMultiTenant ? 1 : 0);
     }
     
     private RowKeyMetaData newRowKeyMetaData() {
@@ -873,4 +1129,23 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
     public Iterator<ColumnReference> iterator() {
         return allColumns.iterator();
     }
+
+    public ValueGetter createGetterFromKeyValues(Collection<Cell> pendingUpdates) {
+        final Map<ReferencingColumn, ImmutableBytesPtr> valueMap = Maps.newHashMapWithExpectedSize(pendingUpdates
+                .size());
+        for (Cell kv : pendingUpdates) {
+            // create new pointers to each part of the kv
+            ImmutableBytesPtr family = new ImmutableBytesPtr(kv.getRowArray(),kv.getFamilyOffset(),kv.getFamilyLength());
+            ImmutableBytesPtr qual = new ImmutableBytesPtr(kv.getRowArray(), kv.getQualifierOffset(), kv.getQualifierLength());
+            ImmutableBytesPtr value = new ImmutableBytesPtr(kv.getValueArray(), kv.getValueOffset(), kv.getValueLength());
+            valueMap.put(new ReferencingColumn(family, qual), value);
+        }
+        return new ValueGetter() {
+            @Override
+            public ImmutableBytesPtr getLatestValue(ColumnReference ref) throws IOException {
+                if(ref.equals(dataEmptyKeyValueRef)) return null;
+                return valueMap.get(ReferencingColumn.wrap(ref));
+            }
+        };
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java b/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java
index 119c7cf..f061b8f 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/PhoenixIndexCodec.java
@@ -132,7 +132,7 @@ public class PhoenixIndexCodec extends BaseIndexCodec {
             // get the values from the scanner so we can actually use them
             ValueGetter valueGetter = IndexManagementUtil.createGetterFromScanner(scanner, dataRowKey);
             ptr.set(dataRowKey);
-            Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, state.getCurrentTimestamp(), env.getRegion().getStartKey());
+            Put put = maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, state.getCurrentTimestamp(), env.getRegion().getStartKey(), env.getRegion().getEndKey());
             indexUpdate.setTable(maintainer.getIndexTableName());
             indexUpdate.setUpdate(put);
             //make sure we close the scanner when we are done
@@ -162,7 +162,7 @@ public class PhoenixIndexCodec extends BaseIndexCodec {
             ptr.set(dataRowKey);
             Delete delete =
                 maintainer.buildDeleteMutation(kvBuilder, valueGetter, ptr,
-                  state.getPendingUpdate(), state.getCurrentTimestamp(), env.getRegion().getStartKey());
+                  state.getPendingUpdate(), state.getCurrentTimestamp(), env.getRegion().getStartKey(), env.getRegion().getEndKey());
             scanner.close();
             indexUpdate.setUpdate(delete);
             indexUpdates.add(indexUpdate);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java
index 8c26fa8..92fc54a 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIterators.java
@@ -242,7 +242,8 @@ public class ParallelIterators extends ExplainTable implements ResultIterators {
                     }
                 } else if (localIndex) {
                     if (splitScan.getStartRow().length != 0 || splitScan.getStopRow().length != 0) {
-                        SaltingUtil.addRegionStartKeyToScanStartAndStopRows(split.getLowerRange(), splitScan);
+                        SaltingUtil.addRegionStartKeyToScanStartAndStopRows(split.getLowerRange(),split.getUpperRange(),
+                            splitScan);
                     }
                 } 
                 if (ScanUtil.intersectScanRange(splitScan, split.getLowerRange(), split.getUpperRange(), this.context.getScanRanges().useSkipScanFilter())) {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java
index 8312fe7..14a3385 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java
@@ -31,6 +31,7 @@ import org.apache.phoenix.parse.HintNode;
 import org.apache.phoenix.query.KeyRange;
 import org.apache.phoenix.schema.SaltingUtil;
 import org.apache.phoenix.schema.TableRef;
+import org.apache.phoenix.schema.PTable.IndexType;
 
 
 /**
@@ -54,7 +55,8 @@ public class SkipRangeParallelIteratorRegionSplitter extends DefaultParallelIter
 
     public List<HRegionLocation> filterRegions(List<HRegionLocation> allTableRegions, final ScanRanges ranges) {
         Iterable<HRegionLocation> regions;
-        if (ranges == ScanRanges.EVERYTHING) {
+        if (ranges == ScanRanges.EVERYTHING
+                || tableRef.getTable().getIndexType() == IndexType.LOCAL) {
             return allTableRegions;
         } else if (ranges == ScanRanges.NOTHING) { // TODO: why not emptyList?
             return Lists.<HRegionLocation>newArrayList();
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/join/TupleProjector.java b/phoenix-core/src/main/java/org/apache/phoenix/join/TupleProjector.java
index c41125a..41b3906 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/join/TupleProjector.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/join/TupleProjector.java
@@ -69,7 +69,7 @@ public class TupleProjector {
         valueSet = ValueBitSet.newInstance(schema);
     }
     
-    private TupleProjector(KeyValueSchema schema, Expression[] expressions) {
+    public TupleProjector(KeyValueSchema schema, Expression[] expressions) {
     	this.schema = schema;
     	this.expressions = expressions;
     	this.valueSet = ValueBitSet.newInstance(schema);
@@ -203,7 +203,7 @@ public class TupleProjector {
     }
     
     public ProjectedValueTuple projectResults(Tuple tuple) {
-    	byte[] bytesValue = schema.toBytes(tuple, expressions, valueSet, ptr);
+    	byte[] bytesValue = schema.toBytes(tuple, getExpressions(), valueSet, ptr);
     	Cell base = tuple.getValue(0);
         return new ProjectedValueTuple(base.getRowArray(), base.getRowOffset(), base.getRowLength(), base.getTimestamp(), bytesValue, valueSet.getEstimatedLength());
     }
@@ -238,5 +238,17 @@ public class TupleProjector {
     	ImmutableBytesWritable keyPtr = dest.getKeyPtr();
         return new ProjectedValueTuple(keyPtr.get(), keyPtr.getOffset(), keyPtr.getLength(), dest.getTimestamp(), merged, destBitSetLen);
     }
+
+    public KeyValueSchema getSchema() {
+        return schema;
+    }
+
+    public Expression[] getExpressions() {
+        return expressions;
+    }
+
+    public ValueBitSet getValueBitSet() {
+        return valueSet;
+    }
 }
 
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java b/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java
index 8341813..6537485 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/optimize/QueryOptimizer.java
@@ -197,6 +197,15 @@ public class QueryOptimizer {
             if (PIndexState.ACTIVE.equals(resolver.getTables().get(0).getTable().getIndexState())) {
                 QueryCompiler compiler = new QueryCompiler(statement, indexSelect, resolver, targetColumns, parallelIteratorFactory, dataPlan.getContext().getSequenceManager());
                 QueryPlan plan = compiler.compile();
+                // If query doesn't have where clause and some of columns to project are missing
+                // in the index then we need to get missing columns from main table for each row in
+                // local index. It's like full scan of both local index and data table which is inefficient.
+                // Then we don't use the index. If all the columns to project are present in the index 
+                // then we can use the index even the query doesn't have where clause. 
+                if (index.getIndexType() == IndexType.LOCAL && indexSelect.getWhere() == null
+                        && !plan.getContext().getDataColumns().isEmpty()) {
+                    return null;
+                }
                 // Checking number of columns handles the wildcard cases correctly, as in that case the index
                 // must contain all columns from the data table to be able to be used.
                 if (plan.getTableRef().getTable().getIndexState() == PIndexState.ACTIVE && plan.getProjector().getColumnCount() == nColumns) {
@@ -284,17 +293,20 @@ public class QueryOptimizer {
                 PTable table1 = plan1.getTableRef().getTable();
                 PTable table2 = plan2.getTableRef().getTable();
                 int c = plan2.getContext().getScanRanges().getRanges().size() - plan1.getContext().getScanRanges().getRanges().size();
+                boolean bothLocalIndexes = table1.getIndexType() == IndexType.LOCAL && table2.getIndexType() == IndexType.LOCAL;
                 // Account for potential view constants which are always bound
                 if (plan1 == dataPlan) { // plan2 is index plan. Ignore the viewIndexId if present
-                    c += boundRanges - (table2.getViewIndexId() == null || table2.getIndexType() == IndexType.LOCAL ? 0 : 1);
+                    c += boundRanges - (table2.getViewIndexId() == null || bothLocalIndexes ? 0 : 1);
+                    if(table2.getIndexType()==IndexType.LOCAL && plan2.getContext().getScanRanges().getRanges().size()==0) c++;
                 } else { // plan1 is index plan. Ignore the viewIndexId if present
-                    c -= boundRanges - (table1.getViewIndexId() == null || table1.getIndexType() == IndexType.LOCAL ? 0 : 1);
+                    c -= boundRanges - (table1.getViewIndexId() == null || bothLocalIndexes ? 0 : 1);
+                    if(table1.getIndexType()==IndexType.LOCAL && plan1.getContext().getScanRanges().getRanges().size()==0) c++;
                 }
-                if (c != 0 && table1.getIndexType() != IndexType.LOCAL && table2.getIndexType() != IndexType.LOCAL) return c;
+                if (c != 0) return c;
                 if (plan1.getGroupBy()!=null && plan2.getGroupBy()!=null) {
                     if (plan1.getGroupBy().isOrderPreserving() != plan2.getGroupBy().isOrderPreserving()) {
                         return plan1.getGroupBy().isOrderPreserving() ? -1 : 1;
-                    }
+                    } 
                 }
                 // Use smaller table (table with fewest kv columns)
                 c = (table1.getColumns().size() - table1.getPKColumns().size()) - (table2.getColumns().size() - table2.getPKColumns().size());
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeRewriter.java b/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeRewriter.java
index 582ec99..db0936d 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeRewriter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/parse/ParseNodeRewriter.java
@@ -436,7 +436,12 @@ public class ParseNodeRewriter extends TraverseAllParseNodeVisitor<ParseNode> {
     
     @Override
     public ParseNode visitLeave(StringConcatParseNode node, List<ParseNode> l) throws SQLException {
-        return node;
+        return leaveCompoundNode(node, l, new CompoundNodeFactory() {
+            @Override
+            public ParseNode createNode(List<ParseNode> children) {
+                return NODE_FACTORY.concat(children);
+            }
+        });
     }
 
     @Override
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
index 5681ea3..0d9553f 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/query/ConnectionQueryServicesImpl.java
@@ -1046,7 +1046,7 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
 
     private void ensureLocalIndexTableCreated(byte[] physicalTableName, Map<String, Object> tableProps, List<Pair<byte[], Map<String, Object>>> families, byte[][] splits) throws SQLException, TableAlreadyExistsException {
         tableProps.put(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_NAME, TRUE_BYTES_AS_STRING);
-        HTableDescriptor desc = ensureTableCreated(physicalTableName, PTableType.TABLE, tableProps, families, splits, false);
+        HTableDescriptor desc = ensureTableCreated(physicalTableName, PTableType.TABLE, tableProps, families, splits, true);
         if (desc != null) {
             if (!Boolean.TRUE.equals(PDataType.BOOLEAN.toObject(desc.getValue(MetaDataUtil.IS_LOCAL_INDEX_TABLE_PROP_BYTES)))) {
                 String fullTableName = Bytes.toString(physicalTableName);
@@ -1173,7 +1173,7 @@ public class ConnectionQueryServicesImpl extends DelegateQueryServices implement
                 familiesPlusDefault.add(new Pair<byte[],Map<String,Object>>(defaultCF,Collections.<String,Object>emptyMap()));
             }
             ensureViewIndexTableCreated(tableName, tableProps, familiesPlusDefault, MetaDataUtil.isSalted(m, kvBuilder, ptr) ? splits : null, MetaDataUtil.getClientTimeStamp(m));
-            ensureLocalIndexTableCreated(MetaDataUtil.getLocalIndexPhysicalName(tableName), tableProps, families, splits);
+            ensureLocalIndexTableCreated(MetaDataUtil.getLocalIndexPhysicalName(tableName), tableProps, familiesPlusDefault, splits);
         }
         
         byte[] tableKey = SchemaUtil.getTableKey(tenantIdBytes, schemaBytes, tableBytes);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/ColumnRef.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/ColumnRef.java
index 0910712..7a00082 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/ColumnRef.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/ColumnRef.java
@@ -36,17 +36,21 @@ import org.apache.phoenix.util.SchemaUtil;
  * @since 0.1
  */
 @Immutable
-public final class ColumnRef {
+public class ColumnRef {
     private final TableRef tableRef;
     private final int columnPosition;
     private final int pkSlotPosition;
     
-    public ColumnRef(ColumnRef columnRef, long timeStamp) {
+    protected ColumnRef(ColumnRef columnRef, long timeStamp) {
         this.tableRef = new TableRef(columnRef.tableRef, timeStamp);
         this.columnPosition = columnRef.columnPosition;
         this.pkSlotPosition = columnRef.pkSlotPosition;
     }
 
+    public ColumnRef(TableRef tableRef, String familyName, String columnName) throws MetaDataEntityNotFoundException {
+        this(tableRef, tableRef.getTable().getColumnFamily(familyName).getColumn(columnName).getPosition());
+    }
+
     public ColumnRef(TableRef tableRef, int columnPosition) {
         if (tableRef == null) {
             throw new NullPointerException();
@@ -113,7 +117,12 @@ public final class ColumnRef {
             String displayName = SchemaUtil.getColumnDisplayName(defaultFamilyName.equals(dataFamilyName) ? null : dataFamilyName, dataColumnName);
         	return new KeyValueColumnExpression(column, displayName);
         }
-        
+
+        // TODO: In ExpressionCompiler create a ColumnRef for a local index that causes a
+        // different kind of ColumnExpression to be created here. You might be able to
+        // use ProjectedColumnExpression, but not sure. The column values from the data
+        // table should get returned in a single KeyValue in a similar format (using a
+        // KeyValueSchema).
         if (table.getType() == PTableType.JOIN) {
         	return new ProjectedColumnExpression(column, table, column.getName().getString());
         }
@@ -123,6 +132,10 @@ public final class ColumnRef {
         return new KeyValueColumnExpression(column, displayName);
     }
 
+    public ColumnRef cloneAtTimestamp(long timestamp) {
+        return new ColumnRef(this, timestamp);
+    }
+
     public int getColumnPosition() {
         return columnPosition;
     }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/LocalIndexDataColumnRef.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/LocalIndexDataColumnRef.java
new file mode 100644
index 0000000..16fe1a5
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/LocalIndexDataColumnRef.java
@@ -0,0 +1,55 @@
+package org.apache.phoenix.schema;
+
+import java.sql.SQLException;
+import java.util.Set;
+
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.phoenix.compile.FromCompiler;
+import org.apache.phoenix.compile.StatementContext;
+import org.apache.phoenix.expression.ColumnExpression;
+import org.apache.phoenix.expression.ProjectedColumnExpression;
+import org.apache.phoenix.parse.ParseNodeFactory;
+import org.apache.phoenix.parse.TableName;
+import org.apache.phoenix.query.QueryConstants;
+import org.apache.phoenix.util.IndexUtil;
+import org.apache.phoenix.util.SchemaUtil;
+
+public class LocalIndexDataColumnRef extends ColumnRef {
+    final private int position;
+    final private Set<PColumn> columns;
+    private static final ParseNodeFactory FACTORY = new ParseNodeFactory();
+
+    public LocalIndexDataColumnRef(StatementContext context, String indexColumnName) throws MetaDataEntityNotFoundException, SQLException {
+        super(FromCompiler.getResolver(
+            FACTORY.namedTable(null, TableName.create(context.getCurrentTable().getTable()
+                    .getSchemaName().getString(), context.getCurrentTable().getTable()
+                    .getParentTableName().getString())), context.getConnection()).resolveTable(
+            context.getCurrentTable().getTable().getSchemaName().getString(),
+            context.getCurrentTable().getTable().getParentTableName().getString()), IndexUtil
+                .getDataColumnFamilyName(indexColumnName), IndexUtil
+                .getDataColumnName(indexColumnName));
+        position = context.getDataColumnPosition(this.getColumn());
+        columns = context.getDataColumns();
+    }
+
+    protected LocalIndexDataColumnRef(LocalIndexDataColumnRef localIndexDataColumnRef, long timestamp) {
+        super(localIndexDataColumnRef, timestamp);
+        this.position = localIndexDataColumnRef.position;
+        this.columns = localIndexDataColumnRef.columns;
+    }
+
+    @Override
+    public ColumnRef cloneAtTimestamp(long timestamp) {
+        return new LocalIndexDataColumnRef(this, timestamp);
+    }
+
+    @Override
+    public ColumnExpression newColumnExpression() {
+        PTable table = this.getTable();
+        PColumn column = this.getColumn();
+        // TODO: util for this or store in member variable
+        byte[] defaultFamily = table.getDefaultFamilyName() == null ? QueryConstants.DEFAULT_COLUMN_FAMILY_BYTES : table.getDefaultFamilyName().getBytes();
+        String displayName = SchemaUtil.getColumnDisplayName(Bytes.compareTo(defaultFamily, column.getFamilyName().getBytes()) == 0  ? null : column.getFamilyName().getBytes(), column.getName().getBytes());
+        return new ProjectedColumnExpression(this.getColumn(), columns, position, displayName);
+    }
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index 449c667..200a14c 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -64,6 +64,7 @@ import java.sql.ResultSetMetaData;
 import java.sql.SQLException;
 import java.sql.SQLFeatureNotSupportedException;
 import java.sql.Types;
+import java.util.ArrayList;
 import java.util.BitSet;
 import java.util.Collection;
 import java.util.Collections;
@@ -503,8 +504,10 @@ public class MetaDataClient {
             // region observer to generate the index rows based on the data rows as we scan
             if (index.getIndexType() == IndexType.LOCAL) {
                 final PhoenixStatement statement = new PhoenixStatement(connection);
-                String query = "SELECT count(*) FROM \"" + dataTableRef.getTable().getName().getString() + "\"";
+                String tableName = getFullTableName(dataTableRef);
+                String query = "SELECT count(*) FROM " + tableName;
                 QueryPlan plan = statement.compileQuery(query);
+                TableRef tableRef = plan.getContext().getResolver().getTables().get(0);
                 // Set attribute on scan that UngroupedAggregateRegionObserver will switch on.
                 // We'll detect that this attribute was set the server-side and write the index
                 // rows per region as a result. The value of the attribute will be our persisted
@@ -512,11 +515,11 @@ public class MetaDataClient {
                 // Define the LOCAL_INDEX_BUILD as a new static in BaseScannerRegionObserver
                 Scan scan = plan.getContext().getScan();
                 ImmutableBytesWritable ptr = new ImmutableBytesWritable();
-                PTable dataTable = dataTableRef.getTable();
+                PTable dataTable = tableRef.getTable();
                 List<PTable> indexes = Lists.newArrayListWithExpectedSize(1);
                 // Only build newly created index.
                 indexes.add(index);
-                IndexMaintainer.serialize(dataTable, ptr, indexes);
+                IndexMaintainer.serialize(dataTable, ptr, indexes, false);
                 scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX_BUILD, ByteUtil.copyKeyBytesIfNecessary(ptr));
                 // By default, we'd use a FirstKeyOnly filter as nothing else needs to be projected for count(*).
                 // However, in this case, we need to project all of the data columns that contribute to the index.
@@ -547,6 +550,15 @@ public class MetaDataClient {
         }
     }
 
+    private String getFullTableName(TableRef dataTableRef) {
+        String schemaName = dataTableRef.getTable().getSchemaName().getString();
+        String tableName = dataTableRef.getTable().getTableName().getString();
+        String fullName =
+                schemaName == null ? ("\"" + tableName + "\"") : ("\"" + schemaName + "\""
+                        + QueryConstants.NAME_SEPARATOR + "\"" + tableName + "\"");
+        return fullName;
+    }
+
     /**
      * Create an index table by morphing the CreateIndexStatement into a CreateTableStatement and calling
      * MetaDataClient.createTable. In doing so, we perform the following translations:
@@ -730,10 +742,6 @@ public class MetaDataClient {
         if (connection.getSCN() != null) {
             return buildIndexAtTimeStamp(table, statement.getTable());
         }
-        if (statement.getIndexType() == IndexType.LOCAL) {
-            ColumnResolver resolver = FromCompiler.getResolverForMutation(statement, connection);
-            tableRef = resolver.getTables().get(0);
-        }
         return buildIndex(table, tableRef);
     }
 
@@ -814,6 +822,7 @@ public class MetaDataClient {
                     addSaltColumn = (saltBucketNum != null && indexType != IndexType.LOCAL);
                     defaultFamilyName = parent.getDefaultFamilyName() == null ? null : parent.getDefaultFamilyName().getString();
                     if (indexType == IndexType.LOCAL) {
+                        saltBucketNum = null;
                         // Set physical name of local index table
                         physicalNames = Collections.singletonList(PNameFactory.newName(MetaDataUtil.getLocalIndexPhysicalName(physicalName.getBytes())));
                     } else {
@@ -1410,14 +1419,30 @@ public class MetaDataClient {
                             // TODO: consider removing this, as the DROP INDEX done for each DROP VIEW command
                             // would have deleted all the rows already
                             if (!dropMetaData) {
-                                String viewIndexSchemaName = MetaDataUtil.getViewIndexSchemaName(schemaName);
-                                String viewIndexTableName = MetaDataUtil.getViewIndexTableName(tableName);
-                                PTable viewIndexTable = new PTableImpl(null, viewIndexSchemaName, viewIndexTableName, ts, table.getColumnFamilies());
-                                tableRefs.add(new TableRef(null, viewIndexTable, ts, false));
-                                String localIndexSchemaName = MetaDataUtil.getLocalIndexSchemaName(schemaName);
-                                String localIndexTableName = MetaDataUtil.getLocalIndexTableName(tableName);
-                                PTable localIndexTable = new PTableImpl(null, localIndexSchemaName, localIndexTableName, ts, table.getColumnFamilies());
-                                tableRefs.add(new TableRef(null, localIndexTable, ts, false));
+                                if (hasViewIndexTable) {
+                                    String viewIndexSchemaName = null;
+                                    String viewIndexTableName = null;
+                                    if(schemaName != null) {
+                                        viewIndexSchemaName = MetaDataUtil.getViewIndexTableName(schemaName);
+                                        viewIndexTableName = tableName;
+                                    } else {
+                                        viewIndexTableName = MetaDataUtil.getViewIndexTableName(tableName);
+                                    }
+                                    PTable viewIndexTable = new PTableImpl(null, viewIndexSchemaName, viewIndexTableName, ts, table.getColumnFamilies());
+                                    tableRefs.add(new TableRef(null, viewIndexTable, ts, false));
+                                } 
+                                if (hasLocalIndexTable) {
+                                    String localIndexSchemaName = null;
+                                    String localIndexTableName = null;
+                                    if(schemaName != null) {
+                                        localIndexSchemaName = MetaDataUtil.getLocalIndexTableName(schemaName);
+                                        localIndexTableName = tableName;
+                                    } else {
+                                        localIndexTableName = MetaDataUtil.getLocalIndexTableName(tableName);
+                                    }
+                                    PTable localIndexTable = new PTableImpl(null, localIndexSchemaName, localIndexTableName, ts, Collections.<PColumnFamily>emptyList());
+                                    tableRefs.add(new TableRef(null, localIndexTable, ts, false));
+                                }
                             }
                         }
                         if (!dropMetaData) {
@@ -1998,7 +2023,7 @@ public class MetaDataClient {
                             // TODO: consider filtering mutable indexes here, but then the issue is that
                             // we'd need to force an update of the data row empty key value if a mutable
                             // secondary index is changing its empty key value family.
-                            droppedColumnRef = new ColumnRef(droppedColumnRef, ts);
+                            droppedColumnRef = droppedColumnRef.cloneAtTimestamp(ts);
                             TableRef droppedColumnTableRef = droppedColumnRef.getTableRef();
                             PColumn droppedColumn = droppedColumnRef.getColumn();
                             MutationPlan plan = compiler.compile(
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java
index cfcad0e..34a2cc0 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/SaltingUtil.java
@@ -107,14 +107,18 @@ public class SaltingUtil {
         return KeyRange.getKeyRange(lowerRange, upperRange);
     }
 
-    public static void addRegionStartKeyToScanStartAndStopRows(byte[] startKey, Scan scan) {
-        byte[] newStartRow = new byte[scan.getStartRow().length + startKey.length];
-        System.arraycopy(startKey, 0, newStartRow, 0, startKey.length);
-        System.arraycopy(scan.getStartRow(), 0, newStartRow, startKey.length, scan.getStartRow().length);
+    public static void addRegionStartKeyToScanStartAndStopRows(byte[] startKey, byte[] endKey, Scan scan) {
+        if (startKey.length == 0 && endKey.length == 0) return;
+        byte[] prefixBytes = startKey.length != 0 ? startKey : new byte[endKey.length];
+        byte[] newStartRow = new byte[scan.getStartRow().length + prefixBytes.length];
+        System.arraycopy(prefixBytes, 0, newStartRow, 0, prefixBytes.length);
+        System.arraycopy(scan.getStartRow(), 0, newStartRow, prefixBytes.length, scan.getStartRow().length);
         scan.setStartRow(newStartRow);
-        byte[] newStopRow = new byte[scan.getStopRow().length + startKey.length];
-        System.arraycopy(startKey, 0, newStopRow, 0, startKey.length);
-        System.arraycopy(scan.getStopRow(), 0, newStopRow, startKey.length, scan.getStopRow().length);
-        scan.setStopRow(newStopRow);
+        if (scan.getStopRow().length != 0) {
+            byte[] newStopRow = new byte[scan.getStopRow().length + prefixBytes.length];
+            System.arraycopy(prefixBytes, 0, newStopRow, 0, prefixBytes.length);
+            System.arraycopy(scan.getStopRow(), 0, newStopRow, prefixBytes.length, scan.getStopRow().length);
+            scan.setStopRow(newStopRow);
+        }
     }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/ValueSchema.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/ValueSchema.java
index 661e48f..b7af8a4 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/ValueSchema.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/ValueSchema.java
@@ -138,7 +138,7 @@ public abstract class ValueSchema implements Writable {
         return minNullable;
     }
     
-    public static final class Field implements Writable {
+    public static final class Field implements Writable, PDatum {
         @Override
         public int hashCode() {
             final int prime = 31;
@@ -194,14 +194,17 @@ public abstract class ValueSchema implements Writable {
             this.sortOrder = SortOrder.getDefault();
         }
         
+        @Override
         public final SortOrder getSortOrder() {
             return sortOrder;
         }
         
+        @Override
         public final PDataType getDataType() {
             return type;
         }
         
+        @Override
         public final boolean isNullable() {
             return isNullable;
         }
@@ -215,6 +218,16 @@ public abstract class ValueSchema implements Writable {
         }
 
         @Override
+        public Integer getMaxLength() {
+            return type.isFixedWidth() ? byteSize : null;
+        }
+
+        @Override
+        public Integer getScale() {
+            return null;
+        }
+
+        @Override
         public void readFields(DataInput input) throws IOException {
             // Encode isNullable in sign bit of type ordinal (offset by 1, since ordinal could be 0)
             int typeOrdinal = WritableUtils.readVInt(input);
@@ -281,6 +294,11 @@ public abstract class ValueSchema implements Writable {
             fields.add(new Field(datum, isNullable, 1, sortOrder));
             return this;
         }
+
+        public ValueSchemaBuilder addField(Field field) {
+            fields.add(field);
+            return this;
+        }
     }
     
     public int getEstimatedByteSize() {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java
index 8b583f2..d4f8207 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/IndexUtil.java
@@ -17,19 +17,31 @@
  */
 package org.apache.phoenix.util;
 
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
 import java.io.IOException;
 import java.sql.SQLException;
 import java.util.List;
 import java.util.Map;
 
 import org.apache.hadoop.hbase.Cell;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Get;
 import org.apache.hadoop.hbase.client.Mutation;
 import org.apache.hadoop.hbase.client.Put;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.regionserver.HRegion;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.io.WritableUtils;
+import org.apache.phoenix.coprocessor.BaseScannerRegionObserver;
 import org.apache.phoenix.exception.SQLExceptionCode;
 import org.apache.phoenix.exception.SQLExceptionInfo;
 import org.apache.phoenix.expression.Expression;
+import org.apache.phoenix.expression.KeyValueColumnExpression;
 import org.apache.phoenix.expression.RowKeyColumnExpression;
 import org.apache.phoenix.expression.visitor.RowKeyExpressionVisitor;
 import org.apache.phoenix.hbase.index.ValueGetter;
@@ -37,13 +49,17 @@ import org.apache.phoenix.hbase.index.covered.update.ColumnReference;
 import org.apache.phoenix.hbase.index.util.ImmutableBytesPtr;
 import org.apache.phoenix.hbase.index.util.KeyValueBuilder;
 import org.apache.phoenix.index.IndexMaintainer;
+import org.apache.phoenix.join.TupleProjector;
 import org.apache.phoenix.query.QueryConstants;
 import org.apache.phoenix.schema.ColumnFamilyNotFoundException;
 import org.apache.phoenix.schema.ColumnNotFoundException;
+import org.apache.phoenix.schema.KeyValueSchema;
 import org.apache.phoenix.schema.PColumn;
 import org.apache.phoenix.schema.PColumnFamily;
 import org.apache.phoenix.schema.PDataType;
 import org.apache.phoenix.schema.PTable;
+import org.apache.phoenix.schema.tuple.ResultTuple;
+import org.apache.phoenix.schema.tuple.Tuple;
 
 import com.google.common.collect.Lists;
 
@@ -185,7 +201,7 @@ public class IndexUtil {
                         }
                         
                     };
-                    indexMutations.add(maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts, null));
+                    indexMutations.add(maintainer.buildUpdateMutation(kvBuilder, valueGetter, ptr, ts, null, null));
                 } else {
                     // We can only generate the correct Delete if we have no KV columns in our index.
                     // Perhaps it'd be best to ignore Delete mutations all together here, as this
@@ -233,4 +249,254 @@ public class IndexUtil {
             
         });
     }
+    
+    public static HRegion getIndexRegion(RegionCoprocessorEnvironment environment) throws IOException {
+        HRegion userRegion = environment.getRegion();
+        TableName indexTableName = TableName.valueOf(MetaDataUtil.getLocalIndexPhysicalName(userRegion.getTableDesc().getName()));
+        List<HRegion> onlineRegions = environment.getRegionServerServices().getOnlineRegions(indexTableName);
+        for(HRegion indexRegion : onlineRegions) {
+            if (Bytes.compareTo(userRegion.getStartKey(), indexRegion.getStartKey()) == 0) {
+                return indexRegion;
+            }
+        }
+        return null;
+    }
+
+    public static HRegion getDataRegion(RegionCoprocessorEnvironment env) throws IOException {
+        HRegion indexRegion = env.getRegion();
+        TableName dataTableName = TableName.valueOf(MetaDataUtil.getUserTableName(indexRegion.getTableDesc().getNameAsString()));
+        List<HRegion> onlineRegions = env.getRegionServerServices().getOnlineRegions(dataTableName);
+        for(HRegion region : onlineRegions) {
+            if (Bytes.compareTo(indexRegion.getStartKey(), region.getStartKey()) == 0) {
+                return region;
+            }
+        }
+        return null;
+    }
+
+    public static ColumnReference[] deserializeDataTableColumnsToJoin(Scan scan) {
+        byte[] columnsBytes = scan.getAttribute(BaseScannerRegionObserver.DATA_TABLE_COLUMNS_TO_JOIN);
+        if (columnsBytes == null) return null;
+        ByteArrayInputStream stream = new ByteArrayInputStream(columnsBytes); // TODO: size?
+        try {
+            DataInputStream input = new DataInputStream(stream);
+            int numColumns = WritableUtils.readVInt(input);
+            ColumnReference[] dataColumns = new ColumnReference[numColumns];
+            for (int i = 0; i < numColumns; i++) {
+                dataColumns[i] = new ColumnReference(Bytes.readByteArray(input), Bytes.readByteArray(input));
+            }
+            return dataColumns;
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
+    public static byte[][] deserializeViewConstantsFromScan(Scan scan) {
+        byte[] bytes = scan.getAttribute(BaseScannerRegionObserver.VIEW_CONSTANTS);
+        if (bytes == null) return null;
+        ByteArrayInputStream stream = new ByteArrayInputStream(bytes); // TODO: size?
+        try {
+            DataInputStream input = new DataInputStream(stream);
+            int numConstants = WritableUtils.readVInt(input);
+            byte[][] viewConstants = new byte[numConstants][];
+            for (int i = 0; i < numConstants; i++) {
+                viewConstants[i] = Bytes.readByteArray(input);
+            }
+            return viewConstants;
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+    
+    public static KeyValueSchema deserializeLocalIndexJoinSchemaFromScan(final Scan scan) {
+        byte[] schemaBytes = scan.getAttribute(BaseScannerRegionObserver.LOCAL_INDEX_JOIN_SCHEMA);
+        if (schemaBytes == null) return null;
+        ByteArrayInputStream stream = new ByteArrayInputStream(schemaBytes); // TODO: size?
+        try {
+            DataInputStream input = new DataInputStream(stream);
+            KeyValueSchema schema = new KeyValueSchema();
+            schema.readFields(input);
+            return schema;
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } finally {
+            try {
+                stream.close();
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+    
+    public static TupleProjector getTupleProjector(Scan scan, ColumnReference[] dataColumns) {
+        if (dataColumns != null && dataColumns.length != 0) {
+            KeyValueSchema keyValueSchema = deserializeLocalIndexJoinSchemaFromScan(scan); 
+            KeyValueColumnExpression[] keyValueColumns = new KeyValueColumnExpression[dataColumns.length];
+            for (int i = 0; i < dataColumns.length; i++) {
+                ColumnReference dataColumn = dataColumns[i];
+                KeyValueColumnExpression dataColumnExpr = new KeyValueColumnExpression(keyValueSchema.getField(i), dataColumn.getFamily(), dataColumn.getQualifier());
+                keyValueColumns[i] = dataColumnExpr;
+            }
+            return new TupleProjector(keyValueSchema, keyValueColumns);
+        }
+        return null;
+    }
+    
+    public static void wrapResultUsingOffset(List<Cell> result, final int offset,
+            ColumnReference[] dataColumns, TupleProjector tupleProjector, HRegion dataRegion,
+            IndexMaintainer indexMaintainer, byte[][] viewConstants, ImmutableBytesWritable ptr) throws IOException {
+        if (tupleProjector != null) {
+            // Join back to data table here by issuing a local get projecting
+            // all of the cq:cf from the KeyValueColumnExpression into the Get.
+            Cell firstCell = result.get(0);
+            byte[] indexRowKey = firstCell.getRowArray();
+            ptr.set(indexRowKey, firstCell.getRowOffset() + offset, firstCell.getRowLength() - offset);
+            byte[] dataRowKey = indexMaintainer.buildDataRowKey(ptr, viewConstants);
+            Get get = new Get(dataRowKey);
+            for (int i = 0; i < dataColumns.length; i++) {
+                get.addColumn(dataColumns[i].getFamily(), dataColumns[i].getQualifier());
+            }
+            Result joinResult = dataRegion.get(get);
+            // TODO: handle null case (but shouldn't happen)
+            Tuple joinTuple = new ResultTuple(joinResult);
+            // This will create a byte[] that captures all of the values from the data table
+            byte[] value =
+                    tupleProjector.getSchema().toBytes(joinTuple, tupleProjector.getExpressions(),
+                        tupleProjector.getValueBitSet(), ptr);
+            KeyValue keyValue =
+                    KeyValueUtil.newKeyValue(firstCell.getRowArray(),firstCell.getRowOffset(),firstCell.getRowLength(), TupleProjector.VALUE_COLUMN_FAMILY,
+                        TupleProjector.VALUE_COLUMN_QUALIFIER, firstCell.getTimestamp(), value, 0, value.length);
+            result.add(keyValue);
+        }
+        for (int i = 0; i < result.size(); i++) {
+            final Cell cell = result.get(i);
+            // TODO: Create DelegateCell class instead
+            Cell newCell = new Cell() {
+
+                @Override
+                public byte[] getRowArray() {
+                    return cell.getRowArray();
+                }
+
+                @Override
+                public int getRowOffset() {
+                    return cell.getRowOffset() + offset;
+                }
+
+                @Override
+                public short getRowLength() {
+                    return (short)(cell.getRowLength() - offset);
+                }
+
+                @Override
+                public byte[] getFamilyArray() {
+                    return cell.getFamilyArray();
+                }
+
+                @Override
+                public int getFamilyOffset() {
+                    return cell.getFamilyOffset();
+                }
+
+                @Override
+                public byte getFamilyLength() {
+                    return cell.getFamilyLength();
+                }
+
+                @Override
+                public byte[] getQualifierArray() {
+                    return cell.getQualifierArray();
+                }
+
+                @Override
+                public int getQualifierOffset() {
+                    return cell.getQualifierOffset();
+                }
+
+                @Override
+                public int getQualifierLength() {
+                    return cell.getQualifierLength();
+                }
+
+                @Override
+                public long getTimestamp() {
+                    return cell.getTimestamp();
+                }
+
+                @Override
+                public byte getTypeByte() {
+                    return cell.getTypeByte();
+                }
+
+                @Override
+                public long getMvccVersion() {
+                    return cell.getMvccVersion();
+                }
+
+                @Override
+                public byte[] getValueArray() {
+                    return cell.getValueArray();
+                }
+
+                @Override
+                public int getValueOffset() {
+                    return cell.getValueOffset();
+                }
+
+                @Override
+                public int getValueLength() {
+                    return cell.getValueLength();
+                }
+
+                @Override
+                public byte[] getTagsArray() {
+                    return cell.getTagsArray();
+                }
+
+                @Override
+                public int getTagsOffset() {
+                    return cell.getTagsOffset();
+                }
+
+                @Override
+                public short getTagsLength() {
+                    return cell.getTagsLength();
+                }
+
+                @Override
+                public byte[] getValue() {
+                    return cell.getValue();
+                }
+
+                @Override
+                public byte[] getFamily() {
+                    return cell.getFamily();
+                }
+
+                @Override
+                public byte[] getQualifier() {
+                    return cell.getQualifier();
+                }
+
+                @Override
+                public byte[] getRow() {
+                    return cell.getRow();
+                }
+            };
+            // Wrap cell in cell that offsets row key
+            result.set(i, newCell);
+        }
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
index 4842626..b98ebf0 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/MetaDataUtil.java
@@ -262,6 +262,14 @@ public class MetaDataUtil {
         return schemaName;
     }  
 
+    public static String getUserTableName(String localIndexTableName) {
+        String schemaName = SchemaUtil.getSchemaNameFromFullName(localIndexTableName);
+        if(!schemaName.isEmpty()) schemaName = schemaName.substring(LOCAL_INDEX_TABLE_PREFIX.length());
+        String tableName = localIndexTableName.substring((schemaName.isEmpty() ? 0 : (schemaName.length() + QueryConstants.NAME_SEPARATOR.length()))
+            + LOCAL_INDEX_TABLE_PREFIX.length());
+        return SchemaUtil.getTableName(schemaName, tableName);
+    }
+
     public static SequenceKey getViewIndexSequenceKey(String tenantId, PName physicalName) {
         // Create global sequence of the form: <prefixed base table name><tenant id>
         // rather than tenant-specific sequence, as it makes it much easier
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java b/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
index 2e78022..e288fd3 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/util/ScanUtil.java
@@ -111,6 +111,10 @@ public class ScanUtil {
 
     public static boolean intersectScanRange(Scan scan, byte[] startKey, byte[] stopKey, boolean useSkipScan) {
         boolean mayHaveRows = false;
+        int offset = 0;
+        if (ScanUtil.isLocalIndex(scan)) {
+            offset = startKey.length != 0 ? startKey.length : stopKey.length;
+        }
         byte[] existingStartKey = scan.getStartRow();
         byte[] existingStopKey = scan.getStopRow();
         if (existingStartKey.length > 0) {
@@ -129,7 +133,19 @@ public class ScanUtil {
         }
         scan.setStartRow(startKey);
         scan.setStopRow(stopKey);
-        
+        if (offset > 0 && useSkipScan) {
+            byte[] temp = null;
+            if (startKey.length != 0) {
+                temp =new byte[startKey.length - offset];
+                System.arraycopy(startKey, offset, temp, 0, startKey.length - offset);
+                startKey = temp;
+            }
+            if (stopKey.length != 0) {
+                temp = new byte[stopKey.length - offset];
+                System.arraycopy(stopKey, offset, temp, 0, stopKey.length - offset);
+                stopKey = temp;
+            }
+        }
         mayHaveRows = mayHaveRows || Bytes.compareTo(scan.getStartRow(), scan.getStopRow()) < 0;
         
         // If the scan is using skip scan filter, intersect and replace the filter.
diff --git a/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java b/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java
index ba78e52..f273bfd 100644
--- a/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java
+++ b/phoenix-core/src/test/java/org/apache/phoenix/index/IndexMaintainerTest.java
@@ -131,15 +131,15 @@ public class IndexMaintainerTest  extends BaseConnectionlessQueryTest {
             assertTrue(indexMutations.get(0) instanceof Put);
             Mutation indexMutation = indexMutations.get(0);
             ImmutableBytesWritable indexKeyPtr = new ImmutableBytesWritable(indexMutation.getRow());
-            
             ptr.set(rowKeyPtr.get(), rowKeyPtr.getOffset(), rowKeyPtr.getLength());
-            byte[] mutablelndexRowKey = im1.buildRowKey(valueGetter, ptr, null);
+            byte[] mutablelndexRowKey = im1.buildRowKey(valueGetter, ptr, null, null);
             byte[] immutableIndexRowKey = indexKeyPtr.copyBytes();
             assertArrayEquals(immutableIndexRowKey, mutablelndexRowKey);
-            
             for (ColumnReference ref : im1.getCoverededColumns()) {
                 valueMap.get(ref);
             }
+            byte[] dataRowKey = im1.buildDataRowKey(indexKeyPtr, null);
+            assertArrayEquals(dataRowKey, dataKeyValues.get(0).getRow());
         } finally {
             try {
                 conn.createStatement().execute("DROP TABLE " + fullTableName);
@@ -239,7 +239,7 @@ public class IndexMaintainerTest  extends BaseConnectionlessQueryTest {
  
     @Test
     public void testCompositeDescRowKeyVarFixedDescSaltedIndexSaltedTable() throws Exception {
-        testIndexRowKeyBuilding("k1 VARCHAR, k2 INTEGER NOT NULL, v VARCHAR", "k1, k2 DESC", "k2 DESC, k1", new Object [] {"a",1}, "", "SALT_BUCKETS=3", "SALT_BUCKETS=4");
+        testIndexRowKeyBuilding("k1 VARCHAR, k2 INTEGER NOT NULL, v VARCHAR", "k1, k2 DESC", "k2 DESC, k1", new Object [] {"a",1}, "", "SALT_BUCKETS=3", "SALT_BUCKETS=3");
     }
  
     @Test
-- 
1.9.4.msysgit.0


From 8f472240e966647f418bc091e3742b10e2ee5071 Mon Sep 17 00:00:00 2001
From: Rajeshbabu Chintaguntla <rajeshbabu.chintaguntla@huawei.com>
Date: Sat, 12 Jul 2014 22:44:44 +0530
Subject: [PATCH 11/14] PHOENIX-1015 Support joining back to data table row
 from local index when query condition involves leading columns in local index

---
 .../end2end/BaseTenantSpecificViewIndexIT.java     | 25 ----------
 .../org/apache/phoenix/end2end/BaseViewIT.java     |  4 +-
 .../org/apache/phoenix/end2end/HashJoinIT.java     |  4 ++
 .../java/org/apache/phoenix/end2end/QueryIT.java   | 51 --------------------
 .../phoenix/end2end/index/MutableIndexIT.java      |  4 +-
 .../org/apache/phoenix/compile/JoinCompiler.java   |  3 +-
 .../org/apache/phoenix/compile/WhereCompiler.java  |  6 +--
 .../java/org/apache/phoenix/query/BaseTest.java    | 55 ++++++++++++++++++++++
 8 files changed, 69 insertions(+), 83 deletions(-)

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java
index f9bb50a..b4ea8e5 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseTenantSpecificViewIndexIT.java
@@ -149,29 +149,4 @@ public class BaseTenantSpecificViewIndexIT extends BaseHBaseManagedTimeIT {
         assertValuesEqualsResultSet(rs,expectedResultsA);
         assertFalse(rs.next());
     }
-
-    /**
-     * Asserts that we find the expected values in the result set. We don't know the order, since we don't always
-     * have an order by and we're going through indexes, but we assert that each expected result occurs once as
-     * expected (in any order).
-     */
-    private void assertValuesEqualsResultSet(ResultSet rs, List<List<Object>> expectedResults) throws SQLException {
-        int expectedCount = expectedResults.size();
-        int count = 0;
-        List<List<Object>> actualResults = Lists.newArrayList();
-        List<Object> errorResult = null;
-        while (rs.next() && errorResult == null) {
-            List<Object> result = Lists.newArrayList();
-            for (int i = 0; i < rs.getMetaData().getColumnCount(); i++) {
-                result.add(rs.getObject(i+1));
-            }
-            if (!expectedResults.contains(result)) {
-                errorResult = result;
-            }
-            actualResults.add(result);
-            count++;
-        }
-        assertTrue("Could not find " + errorResult + " in expected results: " + expectedResults + " with actual results: " + actualResults, errorResult == null);
-        assertEquals(count, expectedCount);
-    }
 }
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java
index 78d86db..5d8df0f 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/BaseViewIT.java
@@ -122,7 +122,7 @@ public class BaseViewIT extends BaseHBaseManagedTimeIT {
         assertFalse(rs.next());
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
         if (localIndex) {
-            assertEquals("CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T [-32768,51]",
+            assertEquals("CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T [-32768,51]\nCLIENT MERGE SORT",
                 QueryUtil.getExplainPlan(rs));
         } else {
             assertEquals(saltBuckets == null
@@ -145,7 +145,7 @@ public class BaseViewIT extends BaseHBaseManagedTimeIT {
         assertFalse(rs.next());
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
         if (localIndex) {
-            assertEquals("CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T [" + (Short.MIN_VALUE+1) + ",'foo']",QueryUtil.getExplainPlan(rs));
+            assertEquals("CLIENT PARALLEL 3-WAY RANGE SCAN OVER _LOCAL_IDX_T [" + (Short.MIN_VALUE+1) + ",'foo']\nCLIENT MERGE SORT",QueryUtil.getExplainPlan(rs));
         } else {
             assertEquals(saltBuckets == null
                     ? "CLIENT PARALLEL 1-WAY RANGE SCAN OVER _IDX_T [" + (Short.MIN_VALUE+1) + ",'foo']"
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java
index ebd018e..99a3d9d 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/HashJoinIT.java
@@ -930,6 +930,7 @@ public class HashJoinIT extends BaseHBaseManagedTimeIT {
                  */
                 "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+ JOIN_ITEM_TABLE_DISPLAY_NAME +"\n"  +
                 "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER SORTED BY [I1.0:NAME, I2.0:NAME]\n" +
                 "CLIENT MERGE SORT\n" +
                 "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
                 "    BUILD HASH TABLE 0\n" +
@@ -962,6 +963,7 @@ public class HashJoinIT extends BaseHBaseManagedTimeIT {
                  */
                 "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX + "" + JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
                 "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER SORTED BY [O.order_id]\n"+
                 "CLIENT MERGE SORT\n" +
                 "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
                 "    BUILD HASH TABLE 0\n" +
@@ -1041,6 +1043,7 @@ public class HashJoinIT extends BaseHBaseManagedTimeIT {
                  */     
                 "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
                 "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER SORTED BY [O.Q DESC NULLS LAST, I.IID]\n"+
                 "CLIENT MERGE SORT\n" +
                 "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
                 "    BUILD HASH TABLE 0\n" +
@@ -1057,6 +1060,7 @@ public class HashJoinIT extends BaseHBaseManagedTimeIT {
                  */     
                 "CLIENT PARALLEL 1-WAY FULL SCAN OVER " + MetaDataUtil.LOCAL_INDEX_TABLE_PREFIX +""+JOIN_ITEM_TABLE_DISPLAY_NAME + "\n" +
                 "    SERVER FILTER BY FIRST KEY ONLY\n" +
+                "    SERVER SORTED BY [O.Q DESC, I.IID]\n"+
                 "CLIENT MERGE SORT\n" +
                 "    PARALLEL EQUI-JOIN 1 HASH TABLES:\n" +
                 "    BUILD HASH TABLE 0\n" +
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java
index 61cbd84..a1a9ba7 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/QueryIT.java
@@ -48,11 +48,9 @@ import java.sql.Time;
 import java.sql.Timestamp;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
-import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.hadoop.hbase.TableName;
@@ -81,7 +79,6 @@ import org.junit.runners.Parameterized.Parameters;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
-import com.google.common.collect.Sets;
 
 
 
@@ -159,54 +156,6 @@ public class QueryIT extends BaseClientManagedTimeIT {
         }
         assertValuesEqualsResultSet(rs, nestedExpectedResults); 
     }
-
-    /**
-     * Asserts that we find the expected values in the result set. We don't know the order, since we don't always
-     * have an order by and we're going through indexes, but we assert that each expected result occurs once as
-     * expected (in any order).
-     */
-    protected void assertValuesEqualsResultSet(ResultSet rs, List<List<Object>> expectedResults) throws SQLException {
-        int expectedCount = expectedResults.size();
-        int count = 0;
-        List<List<Object>> actualResults = Lists.newArrayList();
-        List<Object> errorResult = null;
-        while (rs.next() && errorResult == null) {
-            List<Object> result = Lists.newArrayList();
-            for (int i = 0; i < rs.getMetaData().getColumnCount(); i++) {
-                result.add(rs.getObject(i+1));
-            }
-            if (!expectedResults.contains(result)) {
-                errorResult = result;
-            }
-            actualResults.add(result);
-            count++;
-        }
-        assertTrue("Could not find " + errorResult + " in expected results: " + expectedResults + " with actual results: " + actualResults, errorResult == null);
-        assertEquals(count, expectedCount);
-    }
-    
-    protected void assertOneOfValuesEqualsResultSet(ResultSet rs, List<List<Object>>... expectedResultsArray) throws SQLException {
-        List<List<Object>> results = Lists.newArrayList();
-        while (rs.next()) {
-            List<Object> result = Lists.newArrayList();
-            for (int i = 0; i < rs.getMetaData().getColumnCount(); i++) {
-                result.add(rs.getObject(i+1));
-            }
-            results.add(result);
-        }
-        for (int j = 0; j < expectedResultsArray.length; j++) {
-            List<List<Object>> expectedResults = expectedResultsArray[j];
-            Set<List<Object>> expectedResultsSet = Sets.newHashSet(expectedResults);
-            Iterator<List<Object>> iterator = results.iterator();
-            while (iterator.hasNext()) {
-                if (expectedResultsSet.contains(iterator.next())) {
-                    iterator.remove();
-                }
-            }
-        }
-        if (results.isEmpty()) return;
-        fail("Unable to find " + results + " in " + Arrays.asList(expectedResultsArray));
-    }
     
     @Test
     public void testIntFilter() throws Exception {
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java
index 7f7d0c6..4db967a 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/MutableIndexIT.java
@@ -361,7 +361,9 @@ public class MutableIndexIT extends BaseMutableIndexIT {
         query = "SELECT v1 as foo FROM " + DATA_TABLE_FULL_NAME + " WHERE v2 = '1' ORDER BY foo";
         rs = conn.createStatement().executeQuery("EXPLAIN " + query);
         if(localIndex){
-            assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER _LOCAL_IDX_" +DATA_TABLE_FULL_NAME + " [-32768,~'1']\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
+            assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER _LOCAL_IDX_" +DATA_TABLE_FULL_NAME + " [-32768,~'1']\n" + 
+                    "    SERVER SORTED BY [V1]\n" + 
+                    "CLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
         } else {
             assertEquals("CLIENT PARALLEL 1-WAY RANGE SCAN OVER " +INDEX_TABLE_FULL_NAME + " [~'1']\n" + 
                     "    SERVER SORTED BY [V1]\n" + 
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
index 233cfab..ddf1559 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
@@ -1139,7 +1139,8 @@ public class JoinCompiler {
             if (!plan.getTableRef().equals(tableRef)) {
                 // Use local index plan only when all the columns to project are available in index.
                 // Other wise use data plan.
-                // TODO: In join queries support joining back to data table from index when columns to project are missed index. 
+                // TODO: In join queries support joining back data table row from local index when
+                // columns to project are missed in the index. refer PHOENIX-1015. 
                 if (!localIndex || plan.getContext().getDataColumns().isEmpty()) {
                     replacement.put(tableRef, plan.getTableRef());
                 } 
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java
index ccea388..b9a53f8 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/WhereCompiler.java
@@ -137,11 +137,11 @@ public class WhereCompiler {
         protected ColumnRef resolveColumn(ColumnParseNode node) throws SQLException {
             ColumnRef ref = super.resolveColumn(node);
             PTable table = ref.getTable();
-            // Current table in the context is local index and table in column reference is global means
+            // if current table in the context is local index and table in column reference is global means
             // the column is not present in the local index. If where condition contains the column 
-            // not present in index then we need to go through main table for each row in index and get the
+            // not present in the index then we need to go through main table for each row in index and get the
             // missing column which is like full scan of index table and data table. Which is
-            // inefficient. So we can skip this plan.
+            // inefficient. Then we can skip this plan.
             if (context.getCurrentTable().getTable().getIndexType() == IndexType.LOCAL
                     && (table.getIndexType() == null || table.getIndexType() == IndexType.GLOBAL)) {
                 throw new ColumnNotFoundException(ref.getColumn().getName().getString());
diff --git a/phoenix-core/src/test/java/org/apache/phoenix/query/BaseTest.java b/phoenix-core/src/test/java/org/apache/phoenix/query/BaseTest.java
index c9935b3..bb09eb7 100644
--- a/phoenix-core/src/test/java/org/apache/phoenix/query/BaseTest.java
+++ b/phoenix-core/src/test/java/org/apache/phoenix/query/BaseTest.java
@@ -81,7 +81,10 @@ import static org.apache.phoenix.util.TestUtil.ROW9;
 import static org.apache.phoenix.util.TestUtil.STABLE_NAME;
 import static org.apache.phoenix.util.TestUtil.TABLE_WITH_ARRAY;
 import static org.apache.phoenix.util.TestUtil.TABLE_WITH_SALTING;
+import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
 import java.math.BigDecimal;
 import java.sql.Array;
@@ -94,8 +97,10 @@ import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
 import java.util.Arrays;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.Map.Entry;
 import java.util.Properties;
 import java.util.logging.Level;
@@ -130,6 +135,8 @@ import org.apache.phoenix.util.TestUtil;
 import org.junit.Assert;
 
 import com.google.common.collect.ImmutableMap;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Sets;
 
 /**
  * 
@@ -1311,4 +1318,52 @@ public abstract class BaseTest {
             admin.close();
         }
     }
+
+    public static void assertOneOfValuesEqualsResultSet(ResultSet rs, List<List<Object>>... expectedResultsArray) throws SQLException {
+        List<List<Object>> results = Lists.newArrayList();
+        while (rs.next()) {
+            List<Object> result = Lists.newArrayList();
+            for (int i = 0; i < rs.getMetaData().getColumnCount(); i++) {
+                result.add(rs.getObject(i+1));
+            }
+            results.add(result);
+        }
+        for (int j = 0; j < expectedResultsArray.length; j++) {
+            List<List<Object>> expectedResults = expectedResultsArray[j];
+            Set<List<Object>> expectedResultsSet = Sets.newHashSet(expectedResults);
+            Iterator<List<Object>> iterator = results.iterator();
+            while (iterator.hasNext()) {
+                if (expectedResultsSet.contains(iterator.next())) {
+                    iterator.remove();
+                }
+            }
+        }
+        if (results.isEmpty()) return;
+        fail("Unable to find " + results + " in " + Arrays.asList(expectedResultsArray));
+    }
+
+    /**
+     * Asserts that we find the expected values in the result set. We don't know the order, since we don't always
+     * have an order by and we're going through indexes, but we assert that each expected result occurs once as
+     * expected (in any order).
+     */
+    public static void assertValuesEqualsResultSet(ResultSet rs, List<List<Object>> expectedResults) throws SQLException {
+        int expectedCount = expectedResults.size();
+        int count = 0;
+        List<List<Object>> actualResults = Lists.newArrayList();
+        List<Object> errorResult = null;
+        while (rs.next() && errorResult == null) {
+            List<Object> result = Lists.newArrayList();
+            for (int i = 0; i < rs.getMetaData().getColumnCount(); i++) {
+                result.add(rs.getObject(i+1));
+            }
+            if (!expectedResults.contains(result)) {
+                errorResult = result;
+            }
+            actualResults.add(result);
+            count++;
+        }
+        assertTrue("Could not find " + errorResult + " in expected results: " + expectedResults + " with actual results: " + actualResults, errorResult == null);
+        assertEquals(count, expectedCount);
+    }
 }
-- 
1.9.4.msysgit.0


From 84da58c967a4c93df2ae4d67dc4d8c90a327cafb Mon Sep 17 00:00:00 2001
From: Rajeshbabu Chintaguntla <rajeshbabu.chintaguntla@huawei.com>
Date: Sun, 13 Jul 2014 22:56:13 +0530
Subject: [PATCH 12/14] PHOENIX-1015 Support joining back to data table row
 from local index when query condition involves leading columns in local index

---
 .../phoenix/end2end/index/ImmutableIndexIT.java    |  5 ---
 .../DefaultParallelIteratorRegionSplitter.java     |  4 ---
 .../LocalIndexParallelIteratorRegionSplitter.java  | 42 ++++++++++++++++++++++
 .../ParallelIteratorRegionSplitterFactory.java     |  4 +++
 .../SkipRangeParallelIteratorRegionSplitter.java   |  4 +--
 5 files changed, 47 insertions(+), 12 deletions(-)
 create mode 100644 phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
index 91cdfe9..35148ec 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
@@ -213,11 +213,6 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
         testDeleteFromAllPKColumnIndex(false);
     }
 
-    @Test
-    public void testDeleteFromAllPKColumnLocalIndex() throws Exception {
-        testDeleteFromAllPKColumnIndex(true);
-    }
-
     private void testDeleteFromAllPKColumnIndex(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
index 2a22107..e9524ad 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
@@ -86,10 +86,6 @@ public class DefaultParallelIteratorRegionSplitter implements ParallelIteratorRe
         Scan scan = context.getScan();
         PTable table = tableRef.getTable();
         List<HRegionLocation> allTableRegions = context.getConnection().getQueryServices().getAllTableRegions(table.getPhysicalName().getBytes());
-
-        if (table.getType().equals(PTableType.INDEX) && table.getIndexType().equals(IndexType.LOCAL)) {
-            return filterRegions(allTableRegions, HConstants.EMPTY_START_ROW, HConstants.EMPTY_END_ROW);
-        }
         // If we're not salting, then we've already intersected the minMaxRange with the scan range
         // so there's nothing to do here.
         return filterRegions(allTableRegions, scan.getStartRow(), scan.getStopRow());
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java
new file mode 100644
index 0000000..f5b3d81
--- /dev/null
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java
@@ -0,0 +1,42 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.phoenix.iterate;
+
+import java.sql.SQLException;
+import java.util.List;
+
+import org.apache.hadoop.hbase.HRegionLocation;
+import org.apache.phoenix.compile.StatementContext;
+import org.apache.phoenix.parse.HintNode;
+import org.apache.phoenix.schema.TableRef;
+
+public class LocalIndexParallelIteratorRegionSplitter extends DefaultParallelIteratorRegionSplitter {
+    
+    public static DefaultParallelIteratorRegionSplitter getInstance(StatementContext context, TableRef table, HintNode hintNode) {
+        return new LocalIndexParallelIteratorRegionSplitter(context, table, hintNode);
+    }
+    
+    protected LocalIndexParallelIteratorRegionSplitter(StatementContext context, TableRef table, HintNode hintNode) {
+        super(context,table,hintNode);
+    }
+
+    @Override
+    protected List<HRegionLocation> getAllRegions() throws SQLException {
+        return context.getConnection().getQueryServices().getAllTableRegions(tableRef.getTable().getPhysicalName().getBytes());
+    }
+}
\ No newline at end of file
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitterFactory.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitterFactory.java
index 82f56f1..d3c7d46 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitterFactory.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitterFactory.java
@@ -22,6 +22,7 @@ import java.sql.SQLException;
 import org.apache.phoenix.compile.StatementContext;
 import org.apache.phoenix.parse.HintNode;
 import org.apache.phoenix.schema.TableRef;
+import org.apache.phoenix.schema.PTable.IndexType;
 
 
 /**
@@ -30,6 +31,9 @@ import org.apache.phoenix.schema.TableRef;
 public class ParallelIteratorRegionSplitterFactory {
 
     public static ParallelIteratorRegionSplitter getSplitter(StatementContext context, TableRef table, HintNode hintNode) throws SQLException {
+        if(table.getTable().getIndexType() == IndexType.LOCAL) {
+            return LocalIndexParallelIteratorRegionSplitter.getInstance(context, table, hintNode);
+        }
         if (context.getScanRanges().useSkipScanFilter()) {
             return SkipRangeParallelIteratorRegionSplitter.getInstance(context, table, hintNode);
         }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java
index 14a3385..8312fe7 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/SkipRangeParallelIteratorRegionSplitter.java
@@ -31,7 +31,6 @@ import org.apache.phoenix.parse.HintNode;
 import org.apache.phoenix.query.KeyRange;
 import org.apache.phoenix.schema.SaltingUtil;
 import org.apache.phoenix.schema.TableRef;
-import org.apache.phoenix.schema.PTable.IndexType;
 
 
 /**
@@ -55,8 +54,7 @@ public class SkipRangeParallelIteratorRegionSplitter extends DefaultParallelIter
 
     public List<HRegionLocation> filterRegions(List<HRegionLocation> allTableRegions, final ScanRanges ranges) {
         Iterable<HRegionLocation> regions;
-        if (ranges == ScanRanges.EVERYTHING
-                || tableRef.getTable().getIndexType() == IndexType.LOCAL) {
+        if (ranges == ScanRanges.EVERYTHING) {
             return allTableRegions;
         } else if (ranges == ScanRanges.NOTHING) { // TODO: why not emptyList?
             return Lists.<HRegionLocation>newArrayList();
-- 
1.9.4.msysgit.0


From 28733849ec96af706216c84c1e9187b99c04347e Mon Sep 17 00:00:00 2001
From: Rajeshbabu Chintaguntla <rajeshbabu.chintaguntla@huawei.com>
Date: Tue, 15 Jul 2014 00:18:37 +0530
Subject: [PATCH 13/14] PHOENIX-1015 Support joining back to data table row
 from local index when query condition involves leading columns in local index

---
 .../phoenix/end2end/index/ImmutableIndexIT.java    | 55 ++--------------------
 .../apache/phoenix/end2end/index/LocalIndexIT.java | 17 +++++++
 .../org/apache/phoenix/compile/DeleteCompiler.java |  4 +-
 .../phoenix/compile/IndexStatementRewriter.java    |  6 ---
 .../org/apache/phoenix/compile/JoinCompiler.java   |  9 +---
 .../apache/phoenix/compile/StatementContext.java   |  2 +-
 .../apache/phoenix/exception/SQLExceptionCode.java |  1 +
 .../apache/phoenix/expression/ExpressionType.java  |  3 +-
 .../org/apache/phoenix/index/IndexMaintainer.java  | 14 +-----
 .../DefaultParallelIteratorRegionSplitter.java     | 19 ++++----
 .../LocalIndexParallelIteratorRegionSplitter.java  |  7 ++-
 .../iterate/ParallelIteratorRegionSplitter.java    |  3 ++
 .../org/apache/phoenix/schema/MetaDataClient.java  |  6 ++-
 13 files changed, 50 insertions(+), 96 deletions(-)

diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
index 35148ec..698ade0 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/ImmutableIndexIT.java
@@ -112,40 +112,20 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testIndexWithNullableFixedWithCols() throws Exception {
-        testIndexWithNullableFixedWithCols(false);
-    }
-
-    @Test
-    public void testIndexWithNullableFixedWithColsWithLocalIndex() throws Exception {
-        testIndexWithNullableFixedWithCols(true);
-    }
-
-    private void testIndexWithNullableFixedWithCols(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         ensureTableCreated(getUrl(), INDEX_DATA_TABLE);
         populateTestTable();
-        String ddl = null;
-        if(localIndex){
-            ddl = "CREATE LOCAL INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
-                    + " (char_col1 ASC, int_col1 ASC)"
-                    + " INCLUDE (long_col1, long_col2)";
-        } else {
-            ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+        String ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
                     + " (char_col1 ASC, int_col1 ASC)"
                     + " INCLUDE (long_col1, long_col2)";
-        }
         PreparedStatement stmt = conn.prepareStatement(ddl);
         stmt.execute();
         
         String query = "SELECT char_col1, int_col1 from " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE;
         ResultSet rs = conn.createStatement().executeQuery("EXPLAIN " + query);
-        if (localIndex) {
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER _LOCAL_IDX_INDEX_TEST.INDEX_DATA_TABLE\nCLIENT MERGE SORT", QueryUtil.getExplainPlan(rs));
-        } else {
-            assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER INDEX_TEST.IDX", QueryUtil.getExplainPlan(rs));
-        }
+        assertEquals("CLIENT PARALLEL 1-WAY FULL SCAN OVER INDEX_TEST.IDX", QueryUtil.getExplainPlan(rs));
         
         rs = conn.createStatement().executeQuery(query);
         assertTrue(rs.next());
@@ -210,26 +190,14 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testDeleteFromAllPKColumnIndex() throws Exception {
-        testDeleteFromAllPKColumnIndex(false);
-    }
-
-    private void testDeleteFromAllPKColumnIndex(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         ensureTableCreated(getUrl(), INDEX_DATA_TABLE);
         populateTestTable();
-        String ddl = null;
-        if (localIndex) {
-            ddl = "CREATE LOCAL INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
-                    + " (long_pk, varchar_pk)"
-                    + " INCLUDE (long_col1, long_col2)";
-            
-        } else {
-            ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+        String ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
                     + " (long_pk, varchar_pk)"
                     + " INCLUDE (long_col1, long_col2)";
-        }
         PreparedStatement stmt = conn.prepareStatement(ddl);
         stmt.execute();
         
@@ -276,28 +244,13 @@ public class ImmutableIndexIT extends BaseHBaseManagedTimeIT {
     
     @Test
     public void testDropIfImmutableKeyValueColumn() throws Exception {
-        testDropIfImmutableKeyValueColumn(false);
-    }
-    
-    @Test
-    public void testDropIfImmutableKeyValueColumnWithLocalIndex() throws Exception {
-        testDropIfImmutableKeyValueColumn(true);
-    }
-
-    private void testDropIfImmutableKeyValueColumn(boolean localIndex) throws Exception {
         Properties props = new Properties(TEST_PROPERTIES);
         Connection conn = DriverManager.getConnection(getUrl(), props);
         conn.setAutoCommit(false);
         ensureTableCreated(getUrl(), INDEX_DATA_TABLE);
         populateTestTable();
-        String ddl = null;
-        if(localIndex) {
-            ddl = "CREATE LOCAL INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
-                    + " (long_col1)";
-        } else {
-            ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
+        String ddl = "CREATE INDEX IDX ON " + INDEX_DATA_SCHEMA + QueryConstants.NAME_SEPARATOR + INDEX_DATA_TABLE
                     + " (long_col1)";
-        }
         PreparedStatement stmt = conn.prepareStatement(ddl);
         stmt.execute();
         
diff --git a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
index 1842e5d..5f7691b 100644
--- a/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
+++ b/phoenix-core/src/it/java/org/apache/phoenix/end2end/index/LocalIndexIT.java
@@ -127,6 +127,23 @@ public class LocalIndexIT extends BaseIndexIT {
     }
 
     @Test
+    public void testLocalIndexOnTableWithImmutableRows() throws Exception {
+        createBaseTable(DATA_TABLE_NAME, null, null);
+        Connection conn1 = DriverManager.getConnection(getUrl());
+        Connection conn2 = DriverManager.getConnection(getUrl());
+        try {
+            conn1.createStatement().execute("ALTER TABLE " + DATA_TABLE_NAME + " SET IMMUTABLE_ROWS=true");
+            conn1.createStatement().execute("CREATE LOCAL INDEX " + INDEX_TABLE_NAME + " ON " + DATA_TABLE_NAME + "(v1)");
+            fail("Local index aren't allowed on table with immutable rows");
+        } catch (SQLException e) { }
+        try {
+            conn2.createStatement().executeQuery("SELECT * FROM " + DATA_TABLE_FULL_NAME).next();
+            conn2.unwrap(PhoenixConnection.class).getMetaDataCache().getTable(new PTableKey(null,INDEX_TABLE_NAME));
+            fail("Local index should not be created.");
+        } catch (TableNotFoundException e) { }
+    }
+
+    @Test
     public void testLocalIndexTableRegionSplitPolicyAndSplitKeys() throws Exception {
         createBaseTable(DATA_TABLE_NAME, null,"('e','i','o')");
         Connection conn1 = DriverManager.getConnection(getUrl());
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java
index 2bd0c81..81438ff 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/DeleteCompiler.java
@@ -28,7 +28,6 @@ import java.util.Map;
 
 import org.apache.hadoop.hbase.Cell;
 import org.apache.hadoop.hbase.client.Scan;
-import org.apache.hadoop.hbase.filter.FilterList;
 import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
 import org.apache.phoenix.cache.ServerCacheClient.ServerCache;
 import org.apache.phoenix.compile.GroupByCompiler.GroupBy;
@@ -67,7 +66,6 @@ import org.apache.phoenix.schema.PTableType;
 import org.apache.phoenix.schema.ReadOnlyTableException;
 import org.apache.phoenix.schema.SortOrder;
 import org.apache.phoenix.schema.TableRef;
-import org.apache.phoenix.schema.PTable.IndexType;
 import org.apache.phoenix.schema.tuple.Tuple;
 import org.apache.phoenix.util.IndexUtil;
 import org.apache.phoenix.util.MetaDataUtil;
@@ -170,7 +168,7 @@ public class DeleteCompiler {
         }
         for (PTable index : tableRef.getTable().getIndexes()) {
             for (PColumn column : index.getPKColumns()) {
-                if (!IndexUtil.isDataPKColumn(column) && (index.getIndexType() != IndexType.LOCAL || !column.getName().toString().equals(MetaDataUtil.VIEW_INDEX_ID_COLUMN_NAME))) {
+                if (!IndexUtil.isDataPKColumn(column)) {
                     return true;
                 }
             }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
index c7a5424..c645799 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/IndexStatementRewriter.java
@@ -95,12 +95,6 @@ public class IndexStatementRewriter extends ParseNodeRewriter {
             return node;
 
         String indexColName = IndexUtil.getIndexColumnName(dataCol);
-        // Same alias as before, but use the index column name instead of the data column name
-        // ColumnParseNode cannot be resolved. When this occurs, add the dataColRef to a list
-        // on the StatementContext to indicate that a join back to the data table is necessary.
-        // At ExpressionCompiler.resolveColumn(), test if the table is a local index, and only
-        // then use the alternate. How will the values be resolved on the client? Need to have
-        // a special cf? Or append the values to the PK?
         ParseNode indexColNode = new ColumnParseNode(tName, node.isCaseSensitive() ? '"' + indexColName + '"' : indexColName, node.getAlias());
         PDataType indexColType = IndexUtil.getIndexColumnDataType(dataCol);
         PDataType dataColType = dataColRef.getColumn().getDataType();
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
index ddf1559..6a7f24c 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/JoinCompiler.java
@@ -1135,15 +1135,8 @@ public class JoinCompiler {
             List<OrderByNode> orderBy = tableRef.equals(orderByTableRef) ? select.getOrderBy() : null;
             SelectStatement stmt = getSubqueryForOptimizedPlan(select.getHint(), table.getDynamicColumns(), tableRef, join.getColumnRefs(), table.getPreFiltersCombined(), groupBy, orderBy, table.isWildCardSelect());
             QueryPlan plan = statement.getConnection().getQueryServices().getOptimizer().optimize(statement, stmt);
-            boolean localIndex = plan.getContext().getCurrentTable().getTable().getIndexType()==IndexType.LOCAL;
             if (!plan.getTableRef().equals(tableRef)) {
-                // Use local index plan only when all the columns to project are available in index.
-                // Other wise use data plan.
-                // TODO: In join queries support joining back data table row from local index when
-                // columns to project are missed in the index. refer PHOENIX-1015. 
-                if (!localIndex || plan.getContext().getDataColumns().isEmpty()) {
-                    replacement.put(tableRef, plan.getTableRef());
-                } 
+                replacement.put(tableRef, plan.getTableRef());
             }            
         }
         
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java b/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java
index b23f038..b27447c 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/compile/StatementContext.java
@@ -124,7 +124,7 @@ public class StatementContext {
     }
 
     /**
-     * build map from dataColumn to what will be it's position in single KeyValue value bytes
+     * build map from dataColumn to what will be its position in single KeyValue value bytes
      * returned from the coprocessor that joins from the index row back to the data row.
      * @param column
      * @return
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java b/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
index 817d259..4667051 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/exception/SQLExceptionCode.java
@@ -212,6 +212,7 @@ public enum SQLExceptionCode {
     VIEW_WHERE_IS_CONSTANT(1045, "43A02", "WHERE clause in VIEW should not evaluate to a constant."),
     CANNOT_UPDATE_VIEW_COLUMN(1046, "43A03", "Column updated in VIEW may not differ from value specified in WHERE clause."),
     TOO_MANY_INDEXES(1047, "43A04", "Too many indexes have already been created on the physical table."),
+    NO_LOCAL_INDEX_ON_TABLE_WITH_IMMUTABLE_ROWS(1048,"43A04","Local indexes aren't allowed on tables with immutable rows."),
         
     /** Sequence related */
     SEQUENCE_ALREADY_EXIST(1200, "42Z00", "Sequence already exists.", new Factory() {
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
index 6847c9b..2dd1617 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
@@ -169,7 +169,6 @@ public enum ExpressionType {
     SQLViewTypeFunction(SQLViewTypeFunction.class),
     ExternalSqlTypeIdFunction(ExternalSqlTypeIdFunction.class),
     ConvertTimezoneFunction(ConvertTimezoneFunction.class),
-    SQLIndexTypeFunction(SQLIndexTypeFunction.class),
     DecodeFunction(DecodeFunction.class),
     TimezoneOffsetFunction(TimezoneOffsetFunction.class),
     EncodeFunction(EncodeFunction.class),
@@ -180,8 +179,8 @@ public enum ExpressionType {
     ArrayAnyComparisonExpression(ArrayAnyComparisonExpression.class),
     ArrayAllComparisonExpression(ArrayAllComparisonExpression.class),
     InlineArrayElemRefExpression(InlineArrayElemRefExpression.class),
+    SQLIndexTypeFunction(SQLIndexTypeFunction.class);
     ModulusExpression(ModulusExpression.class);
-    
     ExpressionType(Class<? extends Expression> clazz) {
         this.clazz = clazz;
     }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
index ef8525c..2002137 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/index/IndexMaintainer.java
@@ -153,20 +153,8 @@ public class IndexMaintainer implements Writable, Iterable<ColumnReference> {
      */
     public static void serialize(PTable dataTable, ImmutableBytesWritable ptr,
             List<PTable> indexes) {
-        serialize(dataTable, ptr, indexes, false);
-    }
-
-    /**
-     * For client-side to serialize all IndexMaintainers for a given table
-     * @param dataTable data table
-     * @param ptr bytes pointer to hold returned serialized value
-     * @param indexes indexes to serialize
-     * @param serializeAlways true means serialize even in case of immutable indexing.
-     */
-    public static void serialize(PTable dataTable, ImmutableBytesWritable ptr,
-            List<PTable> indexes, boolean serializeAlways) {
         Iterator<PTable> indexesItr = nonDisabledIndexIterator(indexes.iterator());
-        if ((dataTable.isImmutableRows() && serializeAlways) || !indexesItr.hasNext()) {
+        if ((dataTable.isImmutableRows()) || !indexesItr.hasNext()) {
             ptr.set(ByteUtil.EMPTY_BYTE_ARRAY);
             return;
         }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
index e9524ad..1498480 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/DefaultParallelIteratorRegionSplitter.java
@@ -142,15 +142,7 @@ public class DefaultParallelIteratorRegionSplitter implements ParallelIteratorRe
         // distributed across regions, using this scheme compensates for regions that
         // have more rows than others, by applying tighter splits and therefore spawning
         // off more scans over the overloaded regions.
-        PTable table = tableRef.getTable();
-        boolean localIndex = table.getType().equals(PTableType.INDEX) && table.getIndexType().equals(IndexType.LOCAL);
-        int splitsPerRegion;
-        if (localIndex) {
-            splitsPerRegion = 1;
-        } else {
-            splitsPerRegion = regions.size() >= targetConcurrency ? 1 : (regions.size() > targetConcurrency / 2 ? maxConcurrency : targetConcurrency) / regions.size();
-        }
-        splitsPerRegion = Math.min(splitsPerRegion, maxIntraRegionParallelization);
+        int splitsPerRegion = getSplitsPerRegion(regions.size());
         // Create a multi-map of ServerName to List<KeyRange> which we'll use to round robin from to ensure
         // that we keep each region server busy for each query.
         ListMultimap<HRegionLocation,KeyRange> keyRangesPerRegion = ArrayListMultimap.create(regions.size(),regions.size() * splitsPerRegion);;
@@ -233,4 +225,13 @@ public class DefaultParallelIteratorRegionSplitter implements ParallelIteratorRe
     public List<KeyRange> getSplits() throws SQLException {
         return genKeyRanges(getAllRegions());
     }
+
+    @Override
+    public int getSplitsPerRegion(int numRegions) {
+        int splitsPerRegion =
+                numRegions >= targetConcurrency ? 1
+                        : (numRegions > targetConcurrency / 2 ? maxConcurrency : targetConcurrency)
+                                / numRegions;
+        return Math.min(splitsPerRegion, maxIntraRegionParallelization);
+    }
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java
index f5b3d81..c3a38d5 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/LocalIndexParallelIteratorRegionSplitter.java
@@ -39,4 +39,9 @@ public class LocalIndexParallelIteratorRegionSplitter extends DefaultParallelIte
     protected List<HRegionLocation> getAllRegions() throws SQLException {
         return context.getConnection().getQueryServices().getAllTableRegions(tableRef.getTable().getPhysicalName().getBytes());
     }
-}
\ No newline at end of file
+
+    @Override
+    public int getSplitsPerRegion(int numRegions) {
+        return 1;
+    }
+}
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitter.java b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitter.java
index 1384824..efd9eec 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitter.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/iterate/ParallelIteratorRegionSplitter.java
@@ -31,4 +31,7 @@ import org.apache.phoenix.query.KeyRange;
 public interface ParallelIteratorRegionSplitter {
 
     public List<KeyRange> getSplits() throws SQLException;
+
+    public int getSplitsPerRegion(int numRegions);
+
 }
diff --git a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
index 200a14c..b246857 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/schema/MetaDataClient.java
@@ -64,7 +64,6 @@ import java.sql.ResultSetMetaData;
 import java.sql.SQLException;
 import java.sql.SQLFeatureNotSupportedException;
 import java.sql.Types;
-import java.util.ArrayList;
 import java.util.BitSet;
 import java.util.Collection;
 import java.util.Collections;
@@ -519,7 +518,7 @@ public class MetaDataClient {
                 List<PTable> indexes = Lists.newArrayListWithExpectedSize(1);
                 // Only build newly created index.
                 indexes.add(index);
-                IndexMaintainer.serialize(dataTable, ptr, indexes, false);
+                IndexMaintainer.serialize(dataTable, ptr, indexes);
                 scan.setAttribute(BaseScannerRegionObserver.LOCAL_INDEX_BUILD, ByteUtil.copyKeyBytesIfNecessary(ptr));
                 // By default, we'd use a FirstKeyOnly filter as nothing else needs to be projected for count(*).
                 // However, in this case, we need to project all of the data columns that contribute to the index.
@@ -635,6 +634,9 @@ public class MetaDataClient {
                  * 2) for a view on an index.
                  */
                 if (statement.getIndexType() == IndexType.LOCAL || (dataTable.getType() == PTableType.VIEW && dataTable.getViewType() != ViewType.MAPPED)) {
+                    if (dataTable.isImmutableRows() && statement.getIndexType() == IndexType.LOCAL) {
+                        throw new SQLExceptionInfo.Builder(SQLExceptionCode.NO_LOCAL_INDEX_ON_TABLE_WITH_IMMUTABLE_ROWS).setTableName(indexTableName.getTableName()).build().buildException();
+                    }
                     allocateIndexId = true;
                     // Next add index ID column
                     PDataType dataType = MetaDataUtil.getViewIndexIdDataType();
-- 
1.9.4.msysgit.0


From 62b56e3792364ee6f922d98b2b23a505283b1db6 Mon Sep 17 00:00:00 2001
From: Rajeshbabu Chintaguntla <rajeshbabu.chintaguntla@huawei.com>
Date: Tue, 15 Jul 2014 01:31:04 +0530
Subject: [PATCH 14/14] PHOENOX-1015 Support joining back to data table row
 from local index when query condition involves leading columns in local index
 Small compilation issue.

---
 .../src/main/java/org/apache/phoenix/expression/ExpressionType.java     | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
index 2dd1617..27330d6 100644
--- a/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
+++ b/phoenix-core/src/main/java/org/apache/phoenix/expression/ExpressionType.java
@@ -179,7 +179,7 @@ public enum ExpressionType {
     ArrayAnyComparisonExpression(ArrayAnyComparisonExpression.class),
     ArrayAllComparisonExpression(ArrayAllComparisonExpression.class),
     InlineArrayElemRefExpression(InlineArrayElemRefExpression.class),
-    SQLIndexTypeFunction(SQLIndexTypeFunction.class);
+    SQLIndexTypeFunction(SQLIndexTypeFunction.class),
     ModulusExpression(ModulusExpression.class);
     ExpressionType(Class<? extends Expression> clazz) {
         this.clazz = clazz;
-- 
1.9.4.msysgit.0

